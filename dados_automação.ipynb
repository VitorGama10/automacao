{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e61ab96b",
   "metadata": {},
   "source": [
    "## VD+ Pedidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ddfe39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 14:10:38,137 - INFO - Iniciando processo de atualização de 'consultapedidos.xlsx'...\n",
      "2025-06-07 14:10:38,150 - INFO - Arquivos encontrados em Downloads: 6\n",
      "2025-06-07 14:10:38,151 - INFO -   -> ConsultaPedidos_5f2730d3-8095-4b0b-835e-3c77f208ae25.xlsx\n",
      "2025-06-07 14:10:38,151 - INFO -   -> ConsultaPedidos_80c264b7-7880-4cfe-b64c-dbdb608ea4ac.xlsx\n",
      "2025-06-07 14:10:38,152 - INFO -   -> ConsultaPedidos_815e7bf7-0373-412b-af30-10688b27f50a.xlsx\n",
      "2025-06-07 14:10:38,152 - INFO -   -> ConsultaPedidos_b99d31ed-1019-4d60-a636-da2e90872919.xlsx\n",
      "2025-06-07 14:10:38,154 - INFO -   -> ConsultaPedidos_d3ca485d-5fcc-4dc5-b8e8-d273d6b26a79.xlsx\n",
      "2025-06-07 14:10:38,155 - INFO -   -> ConsultaPedidos_db7434da-5513-4582-b6f0-9fbf84306aa1.xlsx\n",
      "2025-06-07 14:10:38,162 - INFO - Lendo arquivo: 'ConsultaPedidos_5f2730d3-8095-4b0b-835e-3c77f208ae25.xlsx'\n",
      "2025-06-07 14:11:09,685 - INFO - Arquivo 'ConsultaPedidos_5f2730d3-8095-4b0b-835e-3c77f208ae25.xlsx' processado.\n",
      "2025-06-07 14:11:09,685 - INFO - Lendo arquivo: 'ConsultaPedidos_80c264b7-7880-4cfe-b64c-dbdb608ea4ac.xlsx'\n",
      "2025-06-07 14:11:38,467 - INFO - Arquivo 'ConsultaPedidos_80c264b7-7880-4cfe-b64c-dbdb608ea4ac.xlsx' processado.\n",
      "2025-06-07 14:11:38,467 - INFO - Lendo arquivo: 'ConsultaPedidos_815e7bf7-0373-412b-af30-10688b27f50a.xlsx'\n",
      "2025-06-07 14:11:43,551 - INFO - Arquivo 'ConsultaPedidos_815e7bf7-0373-412b-af30-10688b27f50a.xlsx' processado.\n",
      "2025-06-07 14:11:43,551 - INFO - Lendo arquivo: 'ConsultaPedidos_b99d31ed-1019-4d60-a636-da2e90872919.xlsx'\n",
      "2025-06-07 14:11:49,418 - INFO - Arquivo 'ConsultaPedidos_b99d31ed-1019-4d60-a636-da2e90872919.xlsx' processado.\n",
      "2025-06-07 14:11:49,418 - INFO - Lendo arquivo: 'ConsultaPedidos_d3ca485d-5fcc-4dc5-b8e8-d273d6b26a79.xlsx'\n",
      "2025-06-07 14:12:23,506 - INFO - Arquivo 'ConsultaPedidos_d3ca485d-5fcc-4dc5-b8e8-d273d6b26a79.xlsx' processado.\n",
      "2025-06-07 14:12:23,507 - INFO - Lendo arquivo: 'ConsultaPedidos_db7434da-5513-4582-b6f0-9fbf84306aa1.xlsx'\n",
      "2025-06-07 14:12:49,435 - INFO - Arquivo 'ConsultaPedidos_db7434da-5513-4582-b6f0-9fbf84306aa1.xlsx' processado.\n",
      "2025-06-07 14:12:49,451 - INFO - Total de 111453 linhas lidas dos arquivos novos.\n",
      "2025-06-07 14:12:49,504 - INFO - Arquivo de destino 'consultapedidos.xlsx' existente encontrado. Lendo dados antigos...\n",
      "2025-06-07 14:13:28,971 - INFO - Combinando dados existentes com os novos...\n",
      "2025-06-07 14:13:29,036 - INFO - Atualizando/Adicionando registros baseado no 'CodigoPedido'...\n",
      "2025-06-07 14:13:29,087 - INFO - Total de linhas após combinação e atualização: 128558\n",
      "2025-06-07 14:13:29,087 - INFO - Salvando dados finais em 'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Automação\\Dados\\consultapedidos.xlsx'...\n",
      "2025-06-07 14:14:04,086 - INFO - ✅ Dados salvos com sucesso em 'consultapedidos.xlsx'.\n",
      "2025-06-07 14:14:04,086 - INFO - Aplicando formatação monetária R$ em 'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Automação\\Dados\\consultapedidos.xlsx'...\n",
      "2025-06-07 14:15:13,404 - INFO - Formatação monetária aplicada com sucesso.\n",
      "2025-06-07 14:15:13,404 - INFO - Processo concluído.\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_1308\\2730232004.py:176: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  dados = dados.applymap(lambda x: str(x).replace('\\n', ' ').strip() if pd.notna(x) else '')\n",
      "2025-06-07 14:16:06,922 - INFO - --- INÍCIO DO SCRIPT DE ENVIO PARA BIGQUERY (consultapedidos) ---\n",
      "2025-06-07 14:16:06,972 - INFO - [OK] Credenciais carregadas: C:\\Users\\Florestas\\Desktop\\grupo-florestas-429613-080e980a456d.json\n",
      "2025-06-07 14:16:07,009 - INFO - Lendo arquivo CSV: consultapedidosvd.csv...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV limpo salvo com sucesso em: C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Automação\\Dados\\consultapedidosvd.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 14:16:07,507 - INFO - Leitura do CSV OK: 128558 linhas.\n",
      "2025-06-07 14:16:07,507 - INFO - Limpando nomes das colunas para o BigQuery...\n",
      "2025-06-07 14:16:07,507 - INFO - Nomes das colunas limpos (ex: ['codigopedido', 'situacaofiscal', 'notafiscal', 'qtdemateriais', 'qtdeitens']...).\n",
      "2025-06-07 14:16:07,507 - INFO - Convertendo colunas de data para datetime no Pandas...\n",
      "2025-06-07 14:16:07,507 - INFO -  -> Convertendo coluna de data: datacaptacao\n",
      "2025-06-07 14:16:07,523 - INFO -  -> Convertendo coluna de data: dataaprovacao\n",
      "2025-06-07 14:16:07,543 - INFO -  -> Convertendo coluna de data: datafaturamento\n",
      "2025-06-07 14:16:07,556 - WARNING -      Coluna 'datafaturamento': 5 valores não puderam ser convertidos para data (formato esperado AAAA-MM-DD) e serão NULOS.\n",
      "2025-06-07 14:16:07,557 - INFO -  -> Convertendo coluna de data: dataentrega\n",
      "2025-06-07 14:16:07,569 - WARNING -      Coluna 'dataentrega': 17261 valores não puderam ser convertidos para data (formato esperado AAAA-MM-DD) e serão NULOS.\n",
      "2025-06-07 14:16:07,569 - INFO -  -> Convertendo coluna de data: previsaoentrega\n",
      "2025-06-07 14:16:07,576 - WARNING -      Coluna 'previsaoentrega': 128558 valores não puderam ser convertidos para data (formato esperado AAAA-MM-DD) e serão NULOS.\n",
      "2025-06-07 14:16:07,576 - INFO - Convertendo colunas de valor para numérico (float) no Pandas...\n",
      "2025-06-07 14:16:07,576 - INFO -  -> Convertendo coluna de valor: valorliquido\n",
      "2025-06-07 14:16:07,707 - INFO -  -> Convertendo coluna de valor: valorpedido\n",
      "2025-06-07 14:16:07,826 - INFO - Schema definido para o BigQuery: [{'name': 'codigopedido', 'type': 'STRING'}, {'name': 'situacaofiscal', 'type': 'STRING'}, {'name': 'notafiscal', 'type': 'STRING'}, {'name': 'qtdemateriais', 'type': 'STRING'}, {'name': 'qtdeitens', 'type': 'STRING'}, {'name': 'valorliquido', 'type': 'FLOAT64'}, {'name': 'valorpedido', 'type': 'FLOAT64'}, {'name': 'meiocaptacao', 'type': 'STRING'}, {'name': 'tipodeentrega', 'type': 'STRING'}, {'name': 'situacaocomercial', 'type': 'STRING'}, {'name': 'detalhesituacaocomercial', 'type': 'STRING'}, {'name': 'datacaptacao', 'type': 'DATE'}, {'name': 'dataaprovacao', 'type': 'DATE'}, {'name': 'datafaturamento', 'type': 'DATE'}, {'name': 'dataentrega', 'type': 'DATE'}, {'name': 'previsaoentrega', 'type': 'DATE'}, {'name': 'cidade', 'type': 'STRING'}, {'name': 'cep', 'type': 'STRING'}, {'name': 'cidadeentregaretirada', 'type': 'STRING'}, {'name': 'cepentregaretirada', 'type': 'STRING'}, {'name': 'codexternopedido', 'type': 'STRING'}]\n",
      "2025-06-07 14:16:07,826 - INFO - Preparação do DataFrame e schema para envio ao BigQuery concluída.\n",
      "2025-06-07 14:16:07,834 - INFO - Enviando 128558 linhas para BigQuery: 'grupo-florestas-429613:VDHUB.consultapedidos' (Substituindo)...\n",
      "128558 out of 128558 rows loaded.t/s]2025-06-07 14:16:20,690 - INFO - \n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "2025-06-07 14:16:20,690 - INFO - ✅ SUCESSO! Dados enviados e tabela 'VDHUB.consultapedidos' foi substituída no BigQuery.\n",
      "2025-06-07 14:16:20,690 - INFO - --- FIM DO SCRIPT DE ENVIO PARA BIGQUERY (VDHUB.consultapedidos) ---\n"
     ]
    }
   ],
   "source": [
    "# Script para processar múltiplos arquivos e atualizar dados com formatação\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import numbers\n",
    "import logging\n",
    "import shutil # Apenas se for usar a opção de mover arquivos\n",
    "# Configura mensagens de log\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Configurações ---\n",
    "caminho_downloads = r'C:\\Users\\Florestas\\Downloads'\n",
    "caminho_destino_dados = r'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Automação\\Dados'\n",
    "# Pasta para mover arquivos processados (opcional)\n",
    "# caminho_arquivados = os.path.join(caminho_destino_dados, 'Arquivados')\n",
    "nome_arquivo_destino = 'consultapedidos.xlsx'\n",
    "caminho_completo_destino = os.path.join(caminho_destino_dados, nome_arquivo_destino)\n",
    "padrao_arquivo_origem = '*consultapedidos*.xlsx'\n",
    "\n",
    "colunas_desejadas = [\n",
    "    'CodigoPedido', 'SituaçãoFiscal', 'NotaFiscal', 'QtdeMateriais', 'QtdeItens', 'ValorLiquido', 'ValorPedido',\n",
    "    'MeioCaptacao', 'Tipo de Entrega', 'SituaçãoComercial', 'DetalheSituaçãoComercial',\n",
    "    'Data Captação', 'Data Aprovação', 'DataFaturamento', 'DataEntrega', 'PrevisãoEntrega',\n",
    "    'Cidade', 'CEP', 'CidadeEntregaRetirada', 'CEPEntregaRetirada', 'Cód Externo Pedido'\n",
    "]\n",
    "colunas_valores = ['ValorLiquido', 'ValorPedido']\n",
    "\n",
    "# --- Funções Auxiliares ---\n",
    "def formatar_valores_excel(caminho, colunas_formatar):\n",
    "    logging.info(f\"Aplicando formatação monetária R$ em '{caminho}'...\")\n",
    "    try:\n",
    "        wb = load_workbook(caminho)\n",
    "        ws = wb.active\n",
    "        header = {cell.value: idx for idx, cell in enumerate(ws[1], start=1)}\n",
    "        cols_para_formatar_idx = []\n",
    "        for col_nome in colunas_formatar:\n",
    "             if col_nome in header:\n",
    "                  cols_para_formatar_idx.append(header[col_nome])\n",
    "             else:\n",
    "                  logging.warning(f\"Coluna '{col_nome}' para formatação não encontrada no cabeçalho de '{caminho}'.\")\n",
    "        if not cols_para_formatar_idx:\n",
    "             logging.warning(\"Nenhuma coluna válida encontrada para aplicar formatação monetária.\")\n",
    "             return\n",
    "        for col_idx in cols_para_formatar_idx:\n",
    "            for linha in ws.iter_rows(min_row=2, min_col=col_idx, max_col=col_idx):\n",
    "                for celula in linha:\n",
    "                    if isinstance(celula.value, (int, float)):\n",
    "                         celula.number_format = 'R$ #,##0.00'\n",
    "        wb.save(caminho)\n",
    "        logging.info(f\"Formatação monetária aplicada com sucesso.\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Erro ao formatar: Arquivo '{caminho}' não encontrado.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro inesperado ao formatar Excel '{caminho}': {e}\")\n",
    "\n",
    "def safe_to_numeric(series):\n",
    "    series_str = series.astype(str).str.replace(',', '.', regex=False)\n",
    "    series_str = series_str.str.replace(r'[^\\d.]', '', regex=True)\n",
    "    return pd.to_numeric(series_str, errors='coerce')\n",
    "\n",
    "# --- Processo Principal ---\n",
    "logging.info(f\"Iniciando processo de atualização de '{nome_arquivo_destino}'...\")\n",
    "if not os.path.exists(caminho_destino_dados):\n",
    "    logging.error(f\"A pasta de destino '{caminho_destino_dados}' não existe. Processo interrompido.\")\n",
    "else:\n",
    "    caminho_busca = os.path.join(caminho_downloads, padrao_arquivo_origem)\n",
    "    arquivos_encontrados = glob.glob(caminho_busca)\n",
    "    if not arquivos_encontrados:\n",
    "        logging.warning(f\"Nenhum arquivo correspondente a '{padrao_arquivo_origem}' foi encontrado em '{caminho_downloads}'.\")\n",
    "    else:\n",
    "        logging.info(f\"Arquivos encontrados em Downloads: {len(arquivos_encontrados)}\")\n",
    "        for f in arquivos_encontrados: logging.info(f\"  -> {os.path.basename(f)}\")\n",
    "        lista_novos_dados = []\n",
    "        arquivos_processados_com_sucesso = []\n",
    "        for caminho_arquivo_origem in arquivos_encontrados:\n",
    "            try:\n",
    "                nome_base_arquivo = os.path.basename(caminho_arquivo_origem)\n",
    "                logging.info(f\"Lendo arquivo: '{nome_base_arquivo}'\")\n",
    "                df_novo = pd.read_excel(caminho_arquivo_origem, dtype=str)\n",
    "                colunas_existentes_novo = [col for col in colunas_desejadas if col in df_novo.columns]\n",
    "                df_novo = df_novo[colunas_existentes_novo]\n",
    "                for col in colunas_valores:\n",
    "                    if col in df_novo.columns: df_novo[col] = safe_to_numeric(df_novo[col])\n",
    "                    else: logging.warning(f\"Coluna de valor '{col}' não encontrada em '{nome_base_arquivo}'.\")\n",
    "                if 'CodigoPedido' not in df_novo.columns:\n",
    "                     logging.error(f\"Coluna 'CodigoPedido' essencial não encontrada em '{nome_base_arquivo}'. Arquivo será ignorado.\")\n",
    "                     continue\n",
    "                lista_novos_dados.append(df_novo)\n",
    "                arquivos_processados_com_sucesso.append(caminho_arquivo_origem)\n",
    "                logging.info(f\"Arquivo '{nome_base_arquivo}' processado.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Erro ao processar o arquivo '{nome_base_arquivo}': {e}\")\n",
    "        if not lista_novos_dados:\n",
    "            logging.warning(\"Nenhum arquivo novo pôde ser lido e processado com sucesso.\")\n",
    "        else:\n",
    "            novos_dados_combinados = pd.concat(lista_novos_dados, ignore_index=True)\n",
    "            logging.info(f\"Total de {len(novos_dados_combinados)} linhas lidas dos arquivos novos.\")\n",
    "            df_final = pd.DataFrame()\n",
    "            if os.path.exists(caminho_completo_destino):\n",
    "                logging.info(f\"Arquivo de destino '{nome_arquivo_destino}' existente encontrado. Lendo dados antigos...\")\n",
    "                try:\n",
    "                    dados_existentes = pd.read_excel(caminho_completo_destino, dtype=str)\n",
    "                    colunas_existentes_antigo = [col for col in colunas_desejadas if col in dados_existentes.columns]\n",
    "                    dados_existentes = dados_existentes[colunas_existentes_antigo]\n",
    "                    for col in colunas_valores:\n",
    "                        if col in dados_existentes.columns: dados_existentes[col] = safe_to_numeric(dados_existentes[col])\n",
    "                        else: logging.warning(f\"Coluna de valor '{col}' não encontrada no arquivo existente.\")\n",
    "                    if 'CodigoPedido' not in dados_existentes.columns:\n",
    "                         logging.error(f\"Coluna 'CodigoPedido' não encontrada no arquivo existente '{nome_arquivo_destino}'. Usando apenas os dados novos.\")\n",
    "                         df_final = novos_dados_combinados\n",
    "                    else:\n",
    "                         logging.info(\"Combinando dados existentes com os novos...\")\n",
    "                         df_combinado_temp = pd.concat([dados_existentes, novos_dados_combinados], ignore_index=True)\n",
    "                         logging.info(f\"Atualizando/Adicionando registros baseado no 'CodigoPedido'...\")\n",
    "                         df_final = df_combinado_temp.drop_duplicates(subset='CodigoPedido', keep='last')\n",
    "                         logging.info(f\"Total de linhas após combinação e atualização: {len(df_final)}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Erro ao ler ou processar o arquivo existente '{nome_arquivo_destino}': {e}. Considerando apenas dados novos.\")\n",
    "                    df_final = novos_dados_combinados\n",
    "            else:\n",
    "                logging.info(f\"Arquivo de destino '{nome_arquivo_destino}' não encontrado. Usando apenas dados novos.\")\n",
    "                df_final = novos_dados_combinados\n",
    "            if not df_final.empty:\n",
    "                 if 'CodigoPedido' not in df_final.columns:\n",
    "                      logging.error(\"Coluna 'CodigoPedido' está faltando no DataFrame final. Não foi possível salvar.\")\n",
    "                 else:\n",
    "                    try:\n",
    "                        logging.info(f\"Salvando dados finais em '{caminho_completo_destino}'...\")\n",
    "                        for col in colunas_desejadas:\n",
    "                            if col not in df_final.columns: df_final[col] = pd.NA\n",
    "                        df_final = df_final[colunas_desejadas]\n",
    "                        df_final.to_excel(caminho_completo_destino, index=False)\n",
    "                        logging.info(f\"✅ Dados salvos com sucesso em '{nome_arquivo_destino}'.\")\n",
    "                        formatar_valores_excel(caminho_completo_destino, colunas_valores)\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Erro ao salvar o arquivo final '{nome_arquivo_destino}': {e}\")\n",
    "            else:\n",
    "                 logging.warning(\"Nenhum dado final para salvar (verifique os arquivos de origem).\")\n",
    "logging.info(\"Processo concluído.\")\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import unicodedata\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Caminhos\n",
    "caminho_destino = r'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Automação\\Dados'\n",
    "nome_padrao_destino = 'consultapedidos.xlsx'\n",
    "caminho_arquivo_origem = os.path.join(caminho_destino, nome_padrao_destino)\n",
    "caminho_csv_teste = os.path.join(caminho_destino, 'consultapedidosvd.csv')\n",
    "\n",
    "# Função para limpar os nomes das colunas\n",
    "def limpar_nome_coluna(col):\n",
    "    if not isinstance(col, str):\n",
    "        return col\n",
    "    col = unicodedata.normalize('NFKD', col).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    col = re.sub(r'[^\\w]', '', col)\n",
    "    return col\n",
    "\n",
    "# Função para converter datas para o formato yyyy-mm-dd\n",
    "def converter_data_para_iso(valor):\n",
    "    if isinstance(valor, str):\n",
    "        # Tenta detectar datas no formato dd/mm/yyyy ou dd-mm-yyyy\n",
    "        match = re.match(r'^(\\d{2})[/-](\\d{2})[/-](\\d{4})$', valor.strip())\n",
    "        if match:\n",
    "            dia, mes, ano = match.groups()\n",
    "            try:\n",
    "                return f\"{ano}-{mes}-{dia}\"\n",
    "            except:\n",
    "                return valor\n",
    "    return valor\n",
    "\n",
    "if os.path.exists(caminho_arquivo_origem):\n",
    "    dados = pd.read_excel(caminho_arquivo_origem, dtype=str)\n",
    "\n",
    "    # Limpa os nomes das colunas\n",
    "    dados.columns = [limpar_nome_coluna(col) for col in dados.columns]\n",
    "\n",
    "    # Remove quebras de linha e espaços extras\n",
    "    dados = dados.applymap(lambda x: str(x).replace('\\n', ' ').strip() if pd.notna(x) else '')\n",
    "\n",
    "    # Remove horas de colunas com datas\n",
    "    for coluna in dados.columns:\n",
    "        if any(keyword in coluna.lower() for keyword in ['data', 'dt']):\n",
    "            # Remove horas do tipo \"dd/mm/yyyy hh:mm\"\n",
    "            dados[coluna] = dados[coluna].apply(\n",
    "                lambda x: re.sub(r'\\s+\\d{2}:\\d{2}(:\\d{2})?$', '', x) if isinstance(x, str) else x\n",
    "            )\n",
    "            # Converte para o formato yyyy-mm-dd\n",
    "            dados[coluna] = dados[coluna].apply(converter_data_para_iso)\n",
    "\n",
    "    # Substitui 'nan' por string vazia em colunas conhecidas\n",
    "    colunas_para_validar = ['DataFaturamento', 'DataEntrega']\n",
    "    for coluna in colunas_para_validar:\n",
    "        if coluna in dados.columns:\n",
    "            dados[coluna] = dados[coluna].astype(str).replace('nan', '', regex=False)\n",
    "\n",
    "    # Preenche qualquer valor NaN restante com string vazia\n",
    "    dados.fillna('', inplace=True)\n",
    "\n",
    "    # Exporta CSV limpo\n",
    "    dados.to_csv(\n",
    "        caminho_csv_teste,\n",
    "        sep=',',\n",
    "        index=False,\n",
    "        encoding='utf-8',\n",
    "        quoting=csv.QUOTE_ALL,\n",
    "        lineterminator='\\n'\n",
    "    )\n",
    "\n",
    "    print(f\"✅ CSV limpo salvo com sucesso em: {caminho_csv_teste}\")\n",
    "else:\n",
    "    print(f\"⚠️ Arquivo não encontrado: {caminho_arquivo_origem}\")\n",
    "\n",
    "    import os\n",
    "import pandas as pd\n",
    "import unicodedata # Para limpar nomes de colunas\n",
    "import re          # Para limpar nomes de colunas\n",
    "import logging\n",
    "from google.oauth2 import service_account # Para carregar credenciais do BigQuery\n",
    "import pandas_gbq # Para enviar dados ao BigQuery\n",
    "\n",
    "# --- Configuração do Logging ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Caminhos e Nomes ---\n",
    "caminho_pasta_dados = r'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Automação\\Dados'\n",
    "nome_arquivo_csv_limpo = 'consultapedidosvd.csv'\n",
    "caminho_completo_csv_limpo = os.path.join(caminho_pasta_dados, nome_arquivo_csv_limpo)\n",
    "\n",
    "# --- CONFIGURAÇÕES DO BIGQUERY ---\n",
    "PROJECT_ID = 'grupo-florestas-429613'\n",
    "TABLE_ID = 'VDHUB.consultapedidos'\n",
    "ARQUIVO_CREDENCIAL_JSON = r'C:\\Users\\Florestas\\Desktop\\grupo-florestas-429613-080e980a456d.json'\n",
    "\n",
    "# --- Nomes das Colunas para Conversão de Tipo Específica ---\n",
    "COLUNAS_DATAS = [\n",
    "    'datacaptacao', 'dataaprovacao', 'datafaturamento',\n",
    "    'dataentrega', 'previsaoentrega'\n",
    "    # Adicione outras colunas de data aqui, se necessário\n",
    "]\n",
    "COLUNAS_VALORES_FLOAT = ['valorliquido', 'valorpedido']\n",
    "\n",
    "# --- Função para limpar nomes de colunas ---\n",
    "def limpar_nome_coluna_bq(col_name):\n",
    "    if not isinstance(col_name, str): col_name = str(col_name)\n",
    "    nfkd_form = unicodedata.normalize('NFKD', col_name)\n",
    "    ascii_name = nfkd_form.encode('ASCII', 'ignore').decode('ASCII')\n",
    "    lower_name = ascii_name.lower()\n",
    "    cleaned_name = re.sub(r'\\s+', '_', lower_name)\n",
    "    cleaned_name = re.sub(r'[^\\w_]+', '', cleaned_name)\n",
    "    if cleaned_name and cleaned_name[0].isdigit():\n",
    "        cleaned_name = '_' + cleaned_name\n",
    "    if not cleaned_name:\n",
    "        cleaned_name = 'coluna_sem_nome'\n",
    "    return cleaned_name\n",
    "\n",
    "# --- Execução Principal ---\n",
    "logging.info(\"--- INÍCIO DO SCRIPT DE ENVIO PARA BIGQUERY (consultapedidos) ---\")\n",
    "# ... (mensagens de log iniciais) ...\n",
    "\n",
    "# 1. Autenticação (igual ao script anterior)\n",
    "if not os.path.exists(ARQUIVO_CREDENCIAL_JSON):\n",
    "    logging.error(f\"!!! ERRO FATAL: Arquivo de Credenciais JSON não encontrado: '{ARQUIVO_CREDENCIAL_JSON}'.\")\n",
    "    exit()\n",
    "try:\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        ARQUIVO_CREDENCIAL_JSON,\n",
    "        scopes=[\"https://www.googleapis.com/auth/bigquery\"]\n",
    "    )\n",
    "    logging.info(f\"[OK] Credenciais carregadas: {ARQUIVO_CREDENCIAL_JSON}\")\n",
    "except Exception as auth_err:\n",
    "    logging.error(f\"!!! ERRO FATAL ao carregar credenciais: {auth_err}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Verifica se o arquivo CSV existe (igual ao script anterior)\n",
    "if not os.path.exists(caminho_completo_csv_limpo):\n",
    "    logging.error(f\"!!! ERRO FATAL: Arquivo CSV de origem não encontrado: '{caminho_completo_csv_limpo}'.\")\n",
    "    exit()\n",
    "\n",
    "# 3. Lê o CSV e Prepara o DataFrame\n",
    "df_para_bq = None\n",
    "try:\n",
    "    logging.info(f\"Lendo arquivo CSV: {nome_arquivo_csv_limpo}...\")\n",
    "    df_para_bq = pd.read_csv(caminho_completo_csv_limpo, dtype=str, keep_default_na=False, na_values=[''])\n",
    "\n",
    "    if df_para_bq.empty:\n",
    "        raise ValueError(\"O arquivo CSV está vazio.\")\n",
    "    logging.info(f\"Leitura do CSV OK: {len(df_para_bq)} linhas.\")\n",
    "\n",
    "    logging.info(\"Limpando nomes das colunas para o BigQuery...\")\n",
    "    df_para_bq.columns = [limpar_nome_coluna_bq(col) for col in df_para_bq.columns]\n",
    "    logging.info(f\"Nomes das colunas limpos (ex: {list(df_para_bq.columns[:5])}...).\")\n",
    "\n",
    "    # Converte colunas de DATAS\n",
    "    logging.info(\"Convertendo colunas de data para datetime no Pandas...\")\n",
    "    for col_data in COLUNAS_DATAS:\n",
    "        if col_data in df_para_bq.columns:\n",
    "            logging.info(f\" -> Convertendo coluna de data: {col_data}\")\n",
    "            # ***** AJUSTE PRINCIPAL AQUI *****\n",
    "            # Se suas datas no CSV estão como AAAA-MM-DD, use format='%Y-%m-%d'\n",
    "            # Se estiverem como DD/MM/AAAA, use format='%d/%m/%Y' (e remova dayfirst=True ou mantenha-o)\n",
    "            # Se estiverem como AAAAMMDD (sem separadores), use format='%Y%m%d'\n",
    "            # Se o formato for variado, você pode remover o parâmetro 'format' e deixar o Pandas tentar inferir,\n",
    "            # possivelmente com dayfirst=True se DD/MM/AAAA for comum:\n",
    "            # df_para_bq[col_data] = pd.to_datetime(df_para_bq[col_data], errors='coerce', dayfirst=True)\n",
    "            df_para_bq[col_data] = pd.to_datetime(df_para_bq[col_data], format='%Y-%m-%d', errors='coerce') # ASSUMINDO AAAA-MM-DD\n",
    "\n",
    "            num_nulos = df_para_bq[col_data].isna().sum()\n",
    "            if num_nulos > 0:\n",
    "                logging.warning(f\"     Coluna '{col_data}': {num_nulos} valores não puderam ser convertidos para data (formato esperado AAAA-MM-DD) e serão NULOS.\")\n",
    "        else:\n",
    "            logging.warning(f\"Coluna de data '{col_data}' não encontrada. Verifique a lista 'COLUNAS_DATAS'.\")\n",
    "\n",
    "    # Converte colunas de VALORES para FLOAT (igual ao script anterior)\n",
    "    logging.info(\"Convertendo colunas de valor para numérico (float) no Pandas...\")\n",
    "    for col_valor in COLUNAS_VALORES_FLOAT:\n",
    "        if col_valor in df_para_bq.columns:\n",
    "            logging.info(f\" -> Convertendo coluna de valor: {col_valor}\")\n",
    "            if df_para_bq[col_valor].dtype == 'object':\n",
    "                df_para_bq[col_valor] = df_para_bq[col_valor].str.replace(r'[^\\d.,-]', '', regex=True).str.replace(',', '.', regex=False)\n",
    "            df_para_bq[col_valor] = pd.to_numeric(df_para_bq[col_valor], errors='coerce')\n",
    "            num_nulos = df_para_bq[col_valor].isna().sum()\n",
    "            if num_nulos > 0:\n",
    "                logging.warning(f\"     Coluna '{col_valor}': {num_nulos} valores não puderam ser convertidos para número (serão NULOS).\")\n",
    "        else:\n",
    "            logging.warning(f\"Coluna de valor '{col_valor}' não encontrada. Verifique a lista 'COLUNAS_VALORES_FLOAT'.\")\n",
    "\n",
    "    # Define o esquema da tabela para o BigQuery (igual ao script anterior)\n",
    "    table_schema = []\n",
    "    for col_name in df_para_bq.columns:\n",
    "        if col_name in COLUNAS_DATAS:\n",
    "            table_schema.append({'name': col_name, 'type': 'DATE'})\n",
    "        elif col_name in COLUNAS_VALORES_FLOAT:\n",
    "            table_schema.append({'name': col_name, 'type': 'FLOAT64'})\n",
    "        else:\n",
    "            table_schema.append({'name': col_name, 'type': 'STRING'})\n",
    "    logging.info(f\"Schema definido para o BigQuery: {table_schema}\")\n",
    "\n",
    "    logging.info(\"Preparação do DataFrame e schema para envio ao BigQuery concluída.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"!!! ERRO FATAL durante leitura ou preparação do CSV: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    exit()\n",
    "\n",
    "# 4. Envia o DataFrame para o BigQuery (igual ao script anterior)\n",
    "if df_para_bq is not None and not df_para_bq.empty:\n",
    "    logging.info(f\"Enviando {len(df_para_bq)} linhas para BigQuery: '{PROJECT_ID}:{TABLE_ID}' (Substituindo)...\")\n",
    "    try:\n",
    "        pandas_gbq.to_gbq(\n",
    "            df_para_bq,\n",
    "            destination_table=TABLE_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            credentials=credentials,\n",
    "            if_exists='replace',\n",
    "            table_schema=table_schema,\n",
    "            progress_bar=True\n",
    "        )\n",
    "        logging.info(f\"✅ SUCESSO! Dados enviados e tabela '{TABLE_ID}' foi substituída no BigQuery.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"!!! ERRO AO ENVIAR PARA O BIGQUERY: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    logging.error(\"DataFrame está vazio ou não foi definido. Nenhum dado enviado.\")\n",
    "\n",
    "logging.info(f\"--- FIM DO SCRIPT DE ENVIO PARA BIGQUERY ({TABLE_ID}) ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cd9e0a",
   "metadata": {},
   "source": [
    "### Dados Transformados \n",
    "data, salvados para puxar para o bigquery, tivemos que deixar \n",
    "\n",
    "No painel de criação da tabela:\n",
    "\n",
    "Formato: CSV\n",
    "\n",
    "Delimitador: não precisa alterar (deixe como vírgula)\n",
    "\n",
    "Esquema: pode manter “Detectar automaticamente”\n",
    "\n",
    "Certifique-se de que a primeira linha do CSV contém os nomes das colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563deb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\3834806206.py:42: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  dados = dados.applymap(lambda x: str(x).replace('\\n', ' ').strip() if pd.notna(x) else '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV limpo salvo com sucesso em: C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Automação\\Dados\\consultapedidosvd.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import unicodedata\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Caminhos\n",
    "caminho_destino = r'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Automação\\Dados'\n",
    "nome_padrao_destino = 'consultapedidos.xlsx'\n",
    "caminho_arquivo_origem = os.path.join(caminho_destino, nome_padrao_destino)\n",
    "caminho_csv_teste = os.path.join(caminho_destino, 'consultapedidosvd.csv')\n",
    "\n",
    "# Função para limpar os nomes das colunas\n",
    "def limpar_nome_coluna(col):\n",
    "    if not isinstance(col, str):\n",
    "        return col\n",
    "    col = unicodedata.normalize('NFKD', col).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    col = re.sub(r'[^\\w]', '', col)\n",
    "    return col\n",
    "\n",
    "# Função para converter datas para o formato yyyy-mm-dd\n",
    "def converter_data_para_iso(valor):\n",
    "    if isinstance(valor, str):\n",
    "        # Tenta detectar datas no formato dd/mm/yyyy ou dd-mm-yyyy\n",
    "        match = re.match(r'^(\\d{2})[/-](\\d{2})[/-](\\d{4})$', valor.strip())\n",
    "        if match:\n",
    "            dia, mes, ano = match.groups()\n",
    "            try:\n",
    "                return f\"{ano}-{mes}-{dia}\"\n",
    "            except:\n",
    "                return valor\n",
    "    return valor\n",
    "\n",
    "if os.path.exists(caminho_arquivo_origem):\n",
    "    dados = pd.read_excel(caminho_arquivo_origem, dtype=str)\n",
    "\n",
    "    # Limpa os nomes das colunas\n",
    "    dados.columns = [limpar_nome_coluna(col) for col in dados.columns]\n",
    "\n",
    "    # Remove quebras de linha e espaços extras\n",
    "    dados = dados.applymap(lambda x: str(x).replace('\\n', ' ').strip() if pd.notna(x) else '')\n",
    "\n",
    "    # Remove horas de colunas com datas\n",
    "    for coluna in dados.columns:\n",
    "        if any(keyword in coluna.lower() for keyword in ['data', 'dt']):\n",
    "            # Remove horas do tipo \"dd/mm/yyyy hh:mm\"\n",
    "            dados[coluna] = dados[coluna].apply(\n",
    "                lambda x: re.sub(r'\\s+\\d{2}:\\d{2}(:\\d{2})?$', '', x) if isinstance(x, str) else x\n",
    "            )\n",
    "            # Converte para o formato yyyy-mm-dd\n",
    "            dados[coluna] = dados[coluna].apply(converter_data_para_iso)\n",
    "\n",
    "    # Substitui 'nan' por string vazia em colunas conhecidas\n",
    "    colunas_para_validar = ['DataFaturamento', 'DataEntrega']\n",
    "    for coluna in colunas_para_validar:\n",
    "        if coluna in dados.columns:\n",
    "            dados[coluna] = dados[coluna].astype(str).replace('nan', '', regex=False)\n",
    "\n",
    "    # Preenche qualquer valor NaN restante com string vazia\n",
    "    dados.fillna('', inplace=True)\n",
    "\n",
    "    # Exporta CSV limpo\n",
    "    dados.to_csv(\n",
    "        caminho_csv_teste,\n",
    "        sep=',',\n",
    "        index=False,\n",
    "        encoding='utf-8',\n",
    "        quoting=csv.QUOTE_ALL,\n",
    "        lineterminator='\\n'\n",
    "    )\n",
    "\n",
    "    print(f\"✅ CSV limpo salvo com sucesso em: {caminho_csv_teste}\")\n",
    "else:\n",
    "    print(f\"⚠️ Arquivo não encontrado: {caminho_arquivo_origem}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bcab1a",
   "metadata": {},
   "source": [
    "### Enviar Bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca903b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-bigquery in c:\\users\\florestas\\anaconda3\\lib\\site-packages (3.31.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=2.11.1 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (2.24.2)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (2.39.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (2.7.2)\n",
      "Requirement already satisfied: packaging>=24.2.0 in c:\\users\\florestas\\appdata\\roaming\\python\\python312\\site-packages (from google-cloud-bigquery) (24.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in c:\\users\\florestas\\appdata\\roaming\\python\\python312\\site-packages (from google-cloud-bigquery) (2.9.0.post0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.21.0 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-cloud-bigquery) (2.32.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.70.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (5.29.4)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.26.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.71.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (4.9.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-resumable-media<3.0.0,>=2.0.0->google-cloud-bigquery) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\florestas\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (2025.1.31)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d16e6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\florestas\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: pandas_gbq in c:\\users\\florestas\\anaconda3\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\florestas\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from pandas_gbq) (75.1.0)\n",
      "Requirement already satisfied: db-dtypes<2.0.0,>=1.0.4 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from pandas_gbq) (1.4.2)\n",
      "Requirement already satisfied: pyarrow>=4.0.0 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from pandas_gbq) (16.1.0)\n",
      "Requirement already satisfied: pydata-google-auth>=1.5.0 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from pandas_gbq) (1.9.1)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.10.2 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from pandas_gbq) (2.24.2)\n",
      "Requirement already satisfied: google-auth>=2.13.0 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from pandas_gbq) (2.39.0)\n",
      "Requirement already satisfied: google-auth-oauthlib>=0.7.0 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from pandas_gbq) (1.2.2)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=3.4.2 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from pandas_gbq) (3.31.0)\n",
      "Requirement already satisfied: packaging>=22.0.0 in c:\\users\\florestas\\appdata\\roaming\\python\\python312\\site-packages (from pandas_gbq) (24.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-api-core<3.0.0dev,>=2.10.2->pandas_gbq) (1.70.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-api-core<3.0.0dev,>=2.10.2->pandas_gbq) (5.29.4)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-api-core<3.0.0dev,>=2.10.2->pandas_gbq) (1.26.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-api-core<3.0.0dev,>=2.10.2->pandas_gbq) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-auth>=2.13.0->pandas_gbq) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-auth>=2.13.0->pandas_gbq) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-auth>=2.13.0->pandas_gbq) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-auth-oauthlib>=0.7.0->pandas_gbq) (2.0.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-cloud-bigquery<4.0.0dev,>=3.4.2->pandas_gbq) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-cloud-bigquery<4.0.0dev,>=3.4.2->pandas_gbq) (2.7.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\florestas\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery<4.0.0dev,>=3.4.2->pandas_gbq) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery<4.0.0dev,>=3.4.2->pandas_gbq) (1.71.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from google-resumable-media<3.0.0,>=2.0.0->google-cloud-bigquery<4.0.0dev,>=3.4.2->pandas_gbq) (1.7.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.13.0->pandas_gbq) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core<3.0.0dev,>=2.10.2->pandas_gbq) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core<3.0.0dev,>=2.10.2->pandas_gbq) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core<3.0.0dev,>=2.10.2->pandas_gbq) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core<3.0.0dev,>=2.10.2->pandas_gbq) (2025.1.31)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\florestas\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.7.0->pandas_gbq) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas pandas_gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82103d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 10:16:00,045 - INFO - --- INÍCIO DO SCRIPT DE ENVIO PARA BIGQUERY (consultapedidos) ---\n",
      "2025-06-06 10:16:00,228 - INFO - [OK] Credenciais carregadas: C:\\Users\\Florestas\\Desktop\\grupo-florestas-429613-080e980a456d.json\n",
      "2025-06-06 10:16:00,228 - INFO - Lendo arquivo CSV: consultapedidosvd.csv...\n",
      "2025-06-06 10:16:01,836 - INFO - Leitura do CSV OK: 127464 linhas.\n",
      "2025-06-06 10:16:01,843 - INFO - Limpando nomes das colunas para o BigQuery...\n",
      "2025-06-06 10:16:01,843 - INFO - Nomes das colunas limpos (ex: ['codigopedido', 'situacaofiscal', 'notafiscal', 'qtdemateriais', 'qtdeitens']...).\n",
      "2025-06-06 10:16:01,843 - INFO - Convertendo colunas de data para datetime no Pandas...\n",
      "2025-06-06 10:16:01,851 - INFO -  -> Convertendo coluna de data: datacaptacao\n",
      "2025-06-06 10:16:01,916 - INFO -  -> Convertendo coluna de data: dataaprovacao\n",
      "2025-06-06 10:16:01,958 - INFO -  -> Convertendo coluna de data: datafaturamento\n",
      "2025-06-06 10:16:01,991 - WARNING -      Coluna 'datafaturamento': 6 valores não puderam ser convertidos para data (formato esperado AAAA-MM-DD) e serão NULOS.\n",
      "2025-06-06 10:16:01,999 - INFO -  -> Convertendo coluna de data: dataentrega\n",
      "2025-06-06 10:16:02,038 - WARNING -      Coluna 'dataentrega': 30031 valores não puderam ser convertidos para data (formato esperado AAAA-MM-DD) e serão NULOS.\n",
      "2025-06-06 10:16:02,041 - INFO -  -> Convertendo coluna de data: previsaoentrega\n",
      "2025-06-06 10:16:02,085 - WARNING -      Coluna 'previsaoentrega': 127464 valores não puderam ser convertidos para data (formato esperado AAAA-MM-DD) e serão NULOS.\n",
      "2025-06-06 10:16:02,087 - INFO - Convertendo colunas de valor para numérico (float) no Pandas...\n",
      "2025-06-06 10:16:02,091 - INFO -  -> Convertendo coluna de valor: valorliquido\n",
      "2025-06-06 10:16:02,515 - INFO -  -> Convertendo coluna de valor: valorpedido\n",
      "2025-06-06 10:16:02,937 - INFO - Schema definido para o BigQuery: [{'name': 'codigopedido', 'type': 'STRING'}, {'name': 'situacaofiscal', 'type': 'STRING'}, {'name': 'notafiscal', 'type': 'STRING'}, {'name': 'qtdemateriais', 'type': 'STRING'}, {'name': 'qtdeitens', 'type': 'STRING'}, {'name': 'valorliquido', 'type': 'FLOAT64'}, {'name': 'valorpedido', 'type': 'FLOAT64'}, {'name': 'meiocaptacao', 'type': 'STRING'}, {'name': 'tipodeentrega', 'type': 'STRING'}, {'name': 'situacaocomercial', 'type': 'STRING'}, {'name': 'detalhesituacaocomercial', 'type': 'STRING'}, {'name': 'datacaptacao', 'type': 'DATE'}, {'name': 'dataaprovacao', 'type': 'DATE'}, {'name': 'datafaturamento', 'type': 'DATE'}, {'name': 'dataentrega', 'type': 'DATE'}, {'name': 'previsaoentrega', 'type': 'DATE'}, {'name': 'cidade', 'type': 'STRING'}, {'name': 'cep', 'type': 'STRING'}, {'name': 'cidadeentregaretirada', 'type': 'STRING'}, {'name': 'cepentregaretirada', 'type': 'STRING'}, {'name': 'codexternopedido', 'type': 'STRING'}]\n",
      "2025-06-06 10:16:02,938 - INFO - Preparação do DataFrame e schema para envio ao BigQuery concluída.\n",
      "2025-06-06 10:16:02,938 - INFO - Enviando 127464 linhas para BigQuery: 'grupo-florestas-429613:VDHUB.consultapedidos' (Substituindo)...\n",
      "127464 out of 127464 rows loaded.t/s]2025-06-06 10:16:36,743 - INFO - \n",
      "100%|██████████| 1/1 [00:00<00:00, 190.68it/s]\n",
      "2025-06-06 10:16:36,748 - INFO - ✅ SUCESSO! Dados enviados e tabela 'VDHUB.consultapedidos' foi substituída no BigQuery.\n",
      "2025-06-06 10:16:36,756 - INFO - --- FIM DO SCRIPT DE ENVIO PARA BIGQUERY (VDHUB.consultapedidos) ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata # Para limpar nomes de colunas\n",
    "import re          # Para limpar nomes de colunas\n",
    "import logging\n",
    "from google.oauth2 import service_account # Para carregar credenciais do BigQuery\n",
    "import pandas_gbq # Para enviar dados ao BigQuery\n",
    "\n",
    "# --- Configuração do Logging ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Caminhos e Nomes ---\n",
    "caminho_pasta_dados = r'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Automação\\Dados'\n",
    "nome_arquivo_csv_limpo = 'consultapedidosvd.csv'\n",
    "caminho_completo_csv_limpo = os.path.join(caminho_pasta_dados, nome_arquivo_csv_limpo)\n",
    "\n",
    "# --- CONFIGURAÇÕES DO BIGQUERY ---\n",
    "PROJECT_ID = 'grupo-florestas-429613'\n",
    "TABLE_ID = 'VDHUB.consultapedidos'\n",
    "ARQUIVO_CREDENCIAL_JSON = r'C:\\Users\\Florestas\\Desktop\\grupo-florestas-429613-080e980a456d.json'\n",
    "\n",
    "# --- Nomes das Colunas para Conversão de Tipo Específica ---\n",
    "COLUNAS_DATAS = [\n",
    "    'datacaptacao', 'dataaprovacao', 'datafaturamento',\n",
    "    'dataentrega', 'previsaoentrega'\n",
    "    # Adicione outras colunas de data aqui, se necessário\n",
    "]\n",
    "COLUNAS_VALORES_FLOAT = ['valorliquido', 'valorpedido']\n",
    "\n",
    "# --- Função para limpar nomes de colunas ---\n",
    "def limpar_nome_coluna_bq(col_name):\n",
    "    if not isinstance(col_name, str): col_name = str(col_name)\n",
    "    nfkd_form = unicodedata.normalize('NFKD', col_name)\n",
    "    ascii_name = nfkd_form.encode('ASCII', 'ignore').decode('ASCII')\n",
    "    lower_name = ascii_name.lower()\n",
    "    cleaned_name = re.sub(r'\\s+', '_', lower_name)\n",
    "    cleaned_name = re.sub(r'[^\\w_]+', '', cleaned_name)\n",
    "    if cleaned_name and cleaned_name[0].isdigit():\n",
    "        cleaned_name = '_' + cleaned_name\n",
    "    if not cleaned_name:\n",
    "        cleaned_name = 'coluna_sem_nome'\n",
    "    return cleaned_name\n",
    "\n",
    "# --- Execução Principal ---\n",
    "logging.info(\"--- INÍCIO DO SCRIPT DE ENVIO PARA BIGQUERY (consultapedidos) ---\")\n",
    "# ... (mensagens de log iniciais) ...\n",
    "\n",
    "# 1. Autenticação (igual ao script anterior)\n",
    "if not os.path.exists(ARQUIVO_CREDENCIAL_JSON):\n",
    "    logging.error(f\"!!! ERRO FATAL: Arquivo de Credenciais JSON não encontrado: '{ARQUIVO_CREDENCIAL_JSON}'.\")\n",
    "    exit()\n",
    "try:\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        ARQUIVO_CREDENCIAL_JSON,\n",
    "        scopes=[\"https://www.googleapis.com/auth/bigquery\"]\n",
    "    )\n",
    "    logging.info(f\"[OK] Credenciais carregadas: {ARQUIVO_CREDENCIAL_JSON}\")\n",
    "except Exception as auth_err:\n",
    "    logging.error(f\"!!! ERRO FATAL ao carregar credenciais: {auth_err}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Verifica se o arquivo CSV existe (igual ao script anterior)\n",
    "if not os.path.exists(caminho_completo_csv_limpo):\n",
    "    logging.error(f\"!!! ERRO FATAL: Arquivo CSV de origem não encontrado: '{caminho_completo_csv_limpo}'.\")\n",
    "    exit()\n",
    "\n",
    "# 3. Lê o CSV e Prepara o DataFrame\n",
    "df_para_bq = None\n",
    "try:\n",
    "    logging.info(f\"Lendo arquivo CSV: {nome_arquivo_csv_limpo}...\")\n",
    "    df_para_bq = pd.read_csv(caminho_completo_csv_limpo, dtype=str, keep_default_na=False, na_values=[''])\n",
    "\n",
    "    if df_para_bq.empty:\n",
    "        raise ValueError(\"O arquivo CSV está vazio.\")\n",
    "    logging.info(f\"Leitura do CSV OK: {len(df_para_bq)} linhas.\")\n",
    "\n",
    "    logging.info(\"Limpando nomes das colunas para o BigQuery...\")\n",
    "    df_para_bq.columns = [limpar_nome_coluna_bq(col) for col in df_para_bq.columns]\n",
    "    logging.info(f\"Nomes das colunas limpos (ex: {list(df_para_bq.columns[:5])}...).\")\n",
    "\n",
    "    # Converte colunas de DATAS\n",
    "    logging.info(\"Convertendo colunas de data para datetime no Pandas...\")\n",
    "    for col_data in COLUNAS_DATAS:\n",
    "        if col_data in df_para_bq.columns:\n",
    "            logging.info(f\" -> Convertendo coluna de data: {col_data}\")\n",
    "            # ***** AJUSTE PRINCIPAL AQUI *****\n",
    "            # Se suas datas no CSV estão como AAAA-MM-DD, use format='%Y-%m-%d'\n",
    "            # Se estiverem como DD/MM/AAAA, use format='%d/%m/%Y' (e remova dayfirst=True ou mantenha-o)\n",
    "            # Se estiverem como AAAAMMDD (sem separadores), use format='%Y%m%d'\n",
    "            # Se o formato for variado, você pode remover o parâmetro 'format' e deixar o Pandas tentar inferir,\n",
    "            # possivelmente com dayfirst=True se DD/MM/AAAA for comum:\n",
    "            # df_para_bq[col_data] = pd.to_datetime(df_para_bq[col_data], errors='coerce', dayfirst=True)\n",
    "            df_para_bq[col_data] = pd.to_datetime(df_para_bq[col_data], format='%Y-%m-%d', errors='coerce') # ASSUMINDO AAAA-MM-DD\n",
    "\n",
    "            num_nulos = df_para_bq[col_data].isna().sum()\n",
    "            if num_nulos > 0:\n",
    "                logging.warning(f\"     Coluna '{col_data}': {num_nulos} valores não puderam ser convertidos para data (formato esperado AAAA-MM-DD) e serão NULOS.\")\n",
    "        else:\n",
    "            logging.warning(f\"Coluna de data '{col_data}' não encontrada. Verifique a lista 'COLUNAS_DATAS'.\")\n",
    "\n",
    "    # Converte colunas de VALORES para FLOAT (igual ao script anterior)\n",
    "    logging.info(\"Convertendo colunas de valor para numérico (float) no Pandas...\")\n",
    "    for col_valor in COLUNAS_VALORES_FLOAT:\n",
    "        if col_valor in df_para_bq.columns:\n",
    "            logging.info(f\" -> Convertendo coluna de valor: {col_valor}\")\n",
    "            if df_para_bq[col_valor].dtype == 'object':\n",
    "                df_para_bq[col_valor] = df_para_bq[col_valor].str.replace(r'[^\\d.,-]', '', regex=True).str.replace(',', '.', regex=False)\n",
    "            df_para_bq[col_valor] = pd.to_numeric(df_para_bq[col_valor], errors='coerce')\n",
    "            num_nulos = df_para_bq[col_valor].isna().sum()\n",
    "            if num_nulos > 0:\n",
    "                logging.warning(f\"     Coluna '{col_valor}': {num_nulos} valores não puderam ser convertidos para número (serão NULOS).\")\n",
    "        else:\n",
    "            logging.warning(f\"Coluna de valor '{col_valor}' não encontrada. Verifique a lista 'COLUNAS_VALORES_FLOAT'.\")\n",
    "\n",
    "    # Define o esquema da tabela para o BigQuery (igual ao script anterior)\n",
    "    table_schema = []\n",
    "    for col_name in df_para_bq.columns:\n",
    "        if col_name in COLUNAS_DATAS:\n",
    "            table_schema.append({'name': col_name, 'type': 'DATE'})\n",
    "        elif col_name in COLUNAS_VALORES_FLOAT:\n",
    "            table_schema.append({'name': col_name, 'type': 'FLOAT64'})\n",
    "        else:\n",
    "            table_schema.append({'name': col_name, 'type': 'STRING'})\n",
    "    logging.info(f\"Schema definido para o BigQuery: {table_schema}\")\n",
    "\n",
    "    logging.info(\"Preparação do DataFrame e schema para envio ao BigQuery concluída.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"!!! ERRO FATAL durante leitura ou preparação do CSV: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    exit()\n",
    "\n",
    "# 4. Envia o DataFrame para o BigQuery (igual ao script anterior)\n",
    "if df_para_bq is not None and not df_para_bq.empty:\n",
    "    logging.info(f\"Enviando {len(df_para_bq)} linhas para BigQuery: '{PROJECT_ID}:{TABLE_ID}' (Substituindo)...\")\n",
    "    try:\n",
    "        pandas_gbq.to_gbq(\n",
    "            df_para_bq,\n",
    "            destination_table=TABLE_ID,\n",
    "            project_id=PROJECT_ID,\n",
    "            credentials=credentials,\n",
    "            if_exists='replace',\n",
    "            table_schema=table_schema,\n",
    "            progress_bar=True\n",
    "        )\n",
    "        logging.info(f\"✅ SUCESSO! Dados enviados e tabela '{TABLE_ID}' foi substituída no BigQuery.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"!!! ERRO AO ENVIAR PARA O BIGQUERY: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    logging.error(\"DataFrame está vazio ou não foi definido. Nenhum dado enviado.\")\n",
    "\n",
    "logging.info(f\"--- FIM DO SCRIPT DE ENVIO PARA BIGQUERY ({TABLE_ID}) ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27449a17",
   "metadata": {},
   "source": [
    "## PL\n",
    "em atualização, verificar se vai funcionar a automação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ac5717f",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Arquivo mais recente encontrado: C:\\Users\\Florestas\\Downloads\\3633d422a83fa307eb3e31fb07b40d96ebf1cb8d.csv\n",
      "✅ Arquivo atualizado: PedidosPl.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Caminhos\n",
    "caminho_downloads = r'C:\\Users\\Florestas\\Downloads'\n",
    "caminho_destino = r'G:\\Meu Drive\\Relatorio Oficial 2025\\Drive\\Plataforma Logistica'\n",
    "nome_arquivo_destino = 'PedidosPl.xlsx'\n",
    "\n",
    "if not os.path.exists(caminho_destino):\n",
    "    os.makedirs(caminho_destino)\n",
    "    print(f\"📁 Pasta criada: {caminho_destino}\")\n",
    "\n",
    "# Lista arquivos .csv\n",
    "arquivos_csv = [\n",
    "    os.path.join(caminho_downloads, f)\n",
    "    for f in os.listdir(caminho_downloads)\n",
    "    if f.lower().endswith('.csv')\n",
    "]\n",
    "\n",
    "if arquivos_csv:\n",
    "    arquivo_mais_recente = max(arquivos_csv, key=os.path.getmtime)\n",
    "    print(f\"📥 Arquivo mais recente encontrado: {arquivo_mais_recente}\")\n",
    "\n",
    "    # Tentativa de leitura com auto-separador e encoding\n",
    "    try:\n",
    "        dados = pd.read_csv(\n",
    "            arquivo_mais_recente,\n",
    "            sep=None,\n",
    "            engine='python',\n",
    "            encoding='utf-8',\n",
    "            dtype=str\n",
    "        )\n",
    "    except UnicodeDecodeError:\n",
    "        dados = pd.read_csv(\n",
    "            arquivo_mais_recente,\n",
    "            sep=None,\n",
    "            engine='python',\n",
    "            encoding='latin1',\n",
    "            dtype=str\n",
    "        )\n",
    "\n",
    "    # Remove linhas que estiverem com mais colunas ou valores bagunçados\n",
    "    num_colunas = len(dados.columns)\n",
    "    dados = dados[dados.apply(lambda row: row.count() > num_colunas // 2, axis=1)]  # mantém linhas com pelo menos metade dos dados\n",
    "\n",
    "    caminho_arquivo_destino = os.path.join(caminho_destino, nome_arquivo_destino)\n",
    "\n",
    "    if os.path.exists(caminho_arquivo_destino):\n",
    "        dados_existentes = pd.read_excel(caminho_arquivo_destino, dtype=str)\n",
    "\n",
    "        # Concatena, remove duplicados com base na coluna \"Pedido\" (se existir)\n",
    "        if 'Pedido' in dados.columns and 'Pedido' in dados_existentes.columns:\n",
    "            dados_combinados = pd.concat([dados_existentes, dados], ignore_index=True)\n",
    "            dados_combinados = dados_combinados.drop_duplicates(subset='Pedido', keep='last')\n",
    "        else:\n",
    "            dados_combinados = pd.concat([dados_existentes, dados], ignore_index=True)\n",
    "\n",
    "        dados_combinados.to_excel(caminho_arquivo_destino, index=False)\n",
    "        print(f\"✅ Arquivo atualizado: {nome_arquivo_destino}\")\n",
    "    else:\n",
    "        dados.to_excel(caminho_arquivo_destino, index=False)\n",
    "        print(f\"✅ Arquivo criado: {nome_arquivo_destino}\")\n",
    "\n",
    "    # # Remove o CSV original\n",
    "    # os.remove(arquivo_mais_recente)\n",
    "    # print(f\"🗑️ Arquivo original removido: {arquivo_mais_recente}\")\n",
    "else:\n",
    "    print(\"⚠️ Nenhum arquivo .csv encontrado na pasta de downloads.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de4299",
   "metadata": {},
   "source": [
    "### Dados Transformados para enviar para o banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fe6f041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 10:26:58,393 - INFO - -----------------------------------------------------\n",
      "2025-06-06 10:26:58,399 - INFO - Iniciando processamento de 'PedidosPl.xlsx'\n",
      "2025-06-06 10:26:58,399 - INFO - Gerando CSV 'PedidosPl_para_BQ.csv' para BigQuery\n",
      "2025-06-06 10:26:58,405 - INFO - -----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 10:32:23,776 - INFO - Leitura concluída com 86117 linhas.\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\1280868867.py:64: UserWarning: Parsing dates in %Y-%m-%dT%H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  convertidas = pd.to_datetime(df_copy[col], errors='coerce', dayfirst=True)\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\1280868867.py:66: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_copy[col].fillna('', inplace=True)\n",
      "2025-06-06 10:32:24,390 - WARNING - Coluna 'Data de criação': 1 datas não puderam ser convertidas.\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\1280868867.py:64: UserWarning: Parsing dates in %Y-%m-%dT%H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  convertidas = pd.to_datetime(df_copy[col], errors='coerce', dayfirst=True)\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\1280868867.py:66: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_copy[col].fillna('', inplace=True)\n",
      "2025-06-06 10:32:24,765 - WARNING - Coluna 'Data de Coleta': 997 datas não puderam ser convertidas.\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\1280868867.py:64: UserWarning: Parsing dates in %Y-%m-%dT%H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  convertidas = pd.to_datetime(df_copy[col], errors='coerce', dayfirst=True)\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\1280868867.py:66: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_copy[col].fillna('', inplace=True)\n",
      "2025-06-06 10:32:25,071 - WARNING - Coluna 'Nota 1 Data': 1 datas não puderam ser convertidas.\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\1280868867.py:64: UserWarning: Parsing dates in %Y-%m-%dT%H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  convertidas = pd.to_datetime(df_copy[col], errors='coerce', dayfirst=True)\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\1280868867.py:66: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_copy[col].fillna('', inplace=True)\n",
      "2025-06-06 10:32:25,446 - WARNING - Coluna 'Data aprovação': 1 datas não puderam ser convertidas.\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\1280868867.py:64: UserWarning: Parsing dates in %Y-%m-%dT%H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  convertidas = pd.to_datetime(df_copy[col], errors='coerce', dayfirst=True)\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\1280868867.py:66: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_copy[col].fillna('', inplace=True)\n",
      "2025-06-06 10:32:25,799 - WARNING - Coluna 'Data de contratação': 2 datas não puderam ser convertidas.\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\1280868867.py:77: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_copy[col].fillna('', inplace=True)\n",
      "2025-06-06 10:32:29,119 - INFO - 86116 linhas após limpeza.\n",
      "2025-06-06 10:32:33,216 - INFO - ✅ CSV salvo com sucesso em: 'G:\\Meu Drive\\Relatorio Oficial 2025\\Drive\\Plataforma Logistica\\PedidosPl_para_BQ.csv'\n",
      "2025-06-06 10:32:33,219 - INFO - Processamento concluído.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "# --- Configurações de Logging ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Caminhos ---\n",
    "caminho_pasta = r'G:\\Meu Drive\\Relatorio Oficial 2025\\Drive\\Plataforma Logistica'\n",
    "nome_arquivo_excel_origem = 'PedidosPl.xlsx'\n",
    "nome_arquivo_csv_destino = 'PedidosPl_para_BQ.csv'\n",
    "caminho_completo_excel_origem = os.path.join(caminho_pasta, nome_arquivo_excel_origem)\n",
    "caminho_completo_csv_destino = os.path.join(caminho_pasta, nome_arquivo_csv_destino)\n",
    "\n",
    "# --- Mapeamento de colunas: Excel -> BigQuery ---\n",
    "colunas_mapeadas = {\n",
    "    'Pedido': 'pedido',\n",
    "    'Data de criação': 'data_criacao',\n",
    "    'Data de Coleta': 'data_coleta',\n",
    "    'Nota 1 Data': 'nota1_data',\n",
    "    'Data aprovação': 'data_aprovacao',\n",
    "    'Data de contratação': 'data_contratacao',\n",
    "    'Prazo Cliente': 'prazo_cliente',\n",
    "    'Prazo': 'prazo',\n",
    "    'Valor do frete': 'valor_frete',\n",
    "    'Preço do frete': 'preco_frete',\n",
    "    'Valor Total': 'valor_total',\n",
    "    'UF': 'uf',\n",
    "    'Município': 'municipio',\n",
    "    'CEP': 'cep',\n",
    "    'Bairro': 'bairro',\n",
    "    'Endereço': 'endereco',\n",
    "    'Número': 'numero',\n",
    "    'Complemento': 'complemento',\n",
    "    'CPF/CNPJ': 'cpf_cnpj',\n",
    "    'Tipo de Pessoa': 'tipo_pessoa',\n",
    "    'Nome': 'nome',\n",
    "    'Rota': 'rota',\n",
    "    'Itens': 'itens',\n",
    "    'Peso (Kg)': 'peso_kg',\n",
    "    'Motorista': 'motorista',\n",
    "    'Transportadora': 'transportadora',\n",
    "    'Status': 'status',\n",
    "    'Status (hora efetuada)': 'status_hora_efetuada',\n",
    "    'Status (detalhe)': 'status_detalhe'\n",
    "}\n",
    "\n",
    "coluna_chave = 'Pedido'\n",
    "\n",
    "# Define colunas de data e datetime\n",
    "colunas_datas_simples = [\n",
    "    'Data de criação', 'Data de Coleta', 'Nota 1 Data',\n",
    "    'Data aprovação', 'Data de contratação'\n",
    "]\n",
    "colunas_datetime = ['Status (hora efetuada)']\n",
    "\n",
    "def format_date_columns(df, colunas_data, colunas_datahora):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    for col in colunas_data:\n",
    "        if col in df_copy.columns:\n",
    "            try:\n",
    "                convertidas = pd.to_datetime(df_copy[col], errors='coerce', dayfirst=True)\n",
    "                df_copy[col] = convertidas.dt.strftime('%Y-%m-%d')\n",
    "                df_copy[col].fillna('', inplace=True)\n",
    "                if convertidas.isna().sum() > 0:\n",
    "                    logging.warning(f\"Coluna '{col}': {convertidas.isna().sum()} datas não puderam ser convertidas.\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Erro ao converter coluna '{col}': {e}\")\n",
    "\n",
    "    for col in colunas_datahora:\n",
    "        if col in df_copy.columns:\n",
    "            try:\n",
    "                df_copy[col] = df_copy[col].astype(str).str.split(\"T\").str[0]\n",
    "                df_copy[col] = pd.to_datetime(df_copy[col], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "                df_copy[col].fillna('', inplace=True)\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Erro ao dividir/formatar coluna '{col}': {e}\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# --- Execução Principal ---\n",
    "logging.info(\"-----------------------------------------------------\")\n",
    "logging.info(f\"Iniciando processamento de '{nome_arquivo_excel_origem}'\")\n",
    "logging.info(f\"Gerando CSV '{nome_arquivo_csv_destino}' para BigQuery\")\n",
    "logging.info(\"-----------------------------------------------------\")\n",
    "\n",
    "if not os.path.exists(caminho_completo_excel_origem):\n",
    "    logging.error(f\"Arquivo não encontrado: '{caminho_completo_excel_origem}'\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    df_source = pd.read_excel(caminho_completo_excel_origem, dtype=str)\n",
    "    if df_source.empty:\n",
    "        logging.error(\"Arquivo Excel está vazio.\")\n",
    "        exit()\n",
    "    logging.info(f\"Leitura concluída com {len(df_source)} linhas.\")\n",
    "\n",
    "    colunas_existentes = [col for col in colunas_mapeadas.keys() if col in df_source.columns]\n",
    "    colunas_faltando = [col for col in colunas_mapeadas.keys() if col not in df_source.columns]\n",
    "\n",
    "    if not colunas_existentes:\n",
    "        logging.error(\"Nenhuma das colunas especificadas foi encontrada.\")\n",
    "        exit()\n",
    "    if colunas_faltando:\n",
    "        logging.warning(f\"Colunas não encontradas no Excel: {colunas_faltando}\")\n",
    "    if coluna_chave not in colunas_existentes:\n",
    "        logging.error(f\"Coluna chave '{coluna_chave}' não encontrada.\")\n",
    "        exit()\n",
    "\n",
    "    # Filtra colunas e renomeia\n",
    "    df = df_source[colunas_existentes].copy()\n",
    "    df = format_date_columns(df, colunas_datas_simples, colunas_datetime)\n",
    "    df.rename(columns={k: v for k, v in colunas_mapeadas.items() if k in df.columns}, inplace=True)\n",
    "\n",
    "    # Limpeza\n",
    "    df.dropna(how='all', inplace=True)\n",
    "    for col in df.select_dtypes(include='object'):\n",
    "        df[col] = df[col].str.strip()\n",
    "    df.dropna(subset=[colunas_mapeadas[coluna_chave]], inplace=True)\n",
    "    df = df[df[colunas_mapeadas[coluna_chave]].astype(str).str.strip() != '']\n",
    "    logging.info(f\"{len(df)} linhas após limpeza.\")\n",
    "\n",
    "    # Remove duplicatas\n",
    "    df.drop_duplicates(subset=[colunas_mapeadas[coluna_chave]], keep='last', inplace=True)\n",
    "\n",
    "    # Exporta CSV\n",
    "    df.fillna('', inplace=True)\n",
    "    df.to_csv(caminho_completo_csv_destino, sep=',', encoding='utf-8-sig', index=False,\n",
    "              quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    logging.info(f\"✅ CSV salvo com sucesso em: '{caminho_completo_csv_destino}'\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erro durante o processamento: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "logging.info(\"Processamento concluído.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b19c4fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 10:32:33,299 - INFO - -----------------------------------------------------\n",
      "2025-06-06 10:32:33,299 - INFO - Iniciando envio do arquivo CSV 'PedidosPl_para_BQ.csv' para BigQuery.\n",
      "2025-06-06 10:32:33,306 - INFO - Origem do CSV: G:\\Meu Drive\\Relatorio Oficial 2025\\Drive\\Plataforma Logistica\\PedidosPl_para_BQ.csv\n",
      "2025-06-06 10:32:33,309 - INFO - Destino no BigQuery: grupo-florestas-429613:VDHUB.pedidos_pl\n",
      "2025-06-06 10:32:33,311 - INFO - A tabela no BigQuery será SUBSTITUÍDA.\n",
      "2025-06-06 10:32:33,314 - INFO - -----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 10:32:33,445 - INFO - [OK] Credenciais carregadas do arquivo JSON: C:\\Users\\Florestas\\Desktop\\grupo-florestas-429613-080e980a456d.json\n",
      "2025-06-06 10:32:33,512 - INFO - Lendo arquivo CSV processado: PedidosPl_para_BQ.csv...\n",
      "2025-06-06 10:32:34,992 - INFO - Leitura do CSV processado OK: 86116 linhas.\n",
      "2025-06-06 10:32:34,994 - INFO - Limpando nomes das colunas para o BigQuery...\n",
      "2025-06-06 10:32:34,999 - INFO - Nomes das colunas limpos (ex: ['pedido', 'data_criacao', 'data_coleta', 'nota1_data', 'data_aprovacao']...).\n",
      "2025-06-06 10:32:34,999 - INFO - Convertendo colunas de data para o tipo datetime (esperando formato YYYY-MM-DD)...\n",
      "2025-06-06 10:32:35,002 - INFO -  -> Convertendo coluna de data: data_criacao\n",
      "2025-06-06 10:32:35,039 - INFO -  -> Convertendo coluna de data: data_coleta\n",
      "2025-06-06 10:32:35,069 - INFO -       Coluna 'data_coleta': 996 valores são nulos (NaT) após conversão.\n",
      "2025-06-06 10:32:35,069 - INFO -  -> Convertendo coluna de data: nota1_data\n",
      "2025-06-06 10:32:35,101 - INFO -  -> Convertendo coluna de data: data_aprovacao\n",
      "2025-06-06 10:32:35,133 - INFO -  -> Convertendo coluna de data: data_contratacao\n",
      "2025-06-06 10:32:35,164 - INFO -       Coluna 'data_contratacao': 1 valores são nulos (NaT) após conversão.\n",
      "2025-06-06 10:32:35,167 - INFO -  -> Convertendo coluna de data: prazo_cliente\n",
      "2025-06-06 10:32:35,200 - INFO -  -> Convertendo coluna de data: prazo\n",
      "2025-06-06 10:32:35,222 - INFO -       Coluna 'prazo': 86116 valores são nulos (NaT) após conversão.\n",
      "2025-06-06 10:32:35,224 - INFO -  -> Convertendo coluna de data: status_hora_efetuada\n",
      "2025-06-06 10:32:35,251 - INFO -       Coluna 'status_hora_efetuada': 1 valores são nulos (NaT) após conversão.\n",
      "2025-06-06 10:32:35,257 - INFO - Convertendo colunas numéricas para o tipo float/numeric...\n",
      "2025-06-06 10:32:35,259 - INFO -  -> Convertendo coluna numérica: valor_frete\n",
      "2025-06-06 10:32:35,409 - INFO -  -> Convertendo coluna numérica: preco_frete\n",
      "2025-06-06 10:32:35,453 - INFO -       Coluna 'preco_frete': 86116 valores são nulos (NaN) após conversão.\n",
      "2025-06-06 10:32:35,454 - INFO -  -> Convertendo coluna numérica: valor_total\n",
      "2025-06-06 10:32:35,634 - INFO -  -> Convertendo coluna numérica: peso_kg\n",
      "2025-06-06 10:32:35,776 - INFO - Preparação do DataFrame para envio ao BigQuery concluída.\n",
      "2025-06-06 10:32:35,782 - INFO - Enviando 86116 linhas para BigQuery: 'grupo-florestas-429613:VDHUB.pedidos_pl' (Substituindo tabela)...\n",
      "86116 out of 86116 rows loaded.?it/s]2025-06-06 10:32:45,980 - INFO - \n",
      "100%|██████████| 1/1 [00:00<00:00, 366.19it/s]\n",
      "2025-06-06 10:32:45,984 - INFO - ✅ SUCESSO! Dados enviados e tabela 'VDHUB.pedidos_pl' foi substituída no BigQuery.\n",
      "2025-06-06 10:32:45,986 - INFO - --- FIM DO SCRIPT DE ENVIO PARA BIGQUERY (VDHUB.pedidos_pl) ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata # Para a função limpar_nome_coluna_bq\n",
    "import re          # Para a função limpar_nome_coluna_bq\n",
    "import logging\n",
    "from google.oauth2 import service_account # Para carregar as credenciais do BigQuery\n",
    "import pandas_gbq # Para usar pandas_gbq.to_gbq()\n",
    "\n",
    "# --- Configuração do Logging ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Caminhos e Nomes (AJUSTE CONFORME SEU AMBIENTE) ---\n",
    "# Caminho onde o CSV 'PedidosPl_para_BQ.csv' foi salvo pelo seu script anterior\n",
    "caminho_pasta_dados = r'G:\\Meu Drive\\Relatorio Oficial 2025\\Drive\\Plataforma Logistica'\n",
    "nome_arquivo_csv_processado = 'PedidosPl_para_BQ.csv'\n",
    "caminho_completo_csv_processado = os.path.join(caminho_pasta_dados, nome_arquivo_csv_processado)\n",
    "\n",
    "# --- CONFIGURAÇÕES DO BIGQUERY (AJUSTE SE NECESSÁRIO) ---\n",
    "PROJECT_ID = 'grupo-florestas-429613' # SEU ID do Projeto Google Cloud\n",
    "DATASET_ID = 'VDHUB'\n",
    "TABLE_NAME = 'pedidos_pl'\n",
    "DESTINATION_TABLE = f'{DATASET_ID}.{TABLE_NAME}' # Tabela a ser SUBSTITUÍDA\n",
    "\n",
    "# Caminho para o seu arquivo JSON de credenciais do Google Cloud\n",
    "ARQUIVO_CREDENCIAL_JSON = r'C:\\Users\\Florestas\\Desktop\\grupo-florestas-429613-080e980a456d.json' # CONFIRME ESTE CAMINHO\n",
    "\n",
    "# --- Colunas para Conversão de Tipo ---\n",
    "# Nomes das colunas como estarão no CSV (após o mapeamento do seu script de preparação)\n",
    "# Seu script de preparação já formata estas para YYYY-MM-DD\n",
    "colunas_para_converter_data = [\n",
    "    'data_criacao', 'data_coleta', 'nota1_data',\n",
    "    'data_aprovacao', 'data_contratacao', 'prazo_cliente', 'prazo',\n",
    "    'status_hora_efetuada' # Seu script anterior formata esta para YYYY-MM-DD também\n",
    "]\n",
    "\n",
    "# Colunas que devem ser numéricas no BigQuery\n",
    "colunas_para_converter_numerico = [\n",
    "    'valor_frete', 'preco_frete', 'valor_total', 'peso_kg'\n",
    "    # Adicione outras colunas se forem numéricas, ex: 'numero' se for sempre numérico e não um identificador\n",
    "]\n",
    "\n",
    "\n",
    "# --- Funções Auxiliares ---\n",
    "def limpar_nome_coluna_bq(col_name):\n",
    "    \"\"\"Limpa o nome da coluna para conformidade com o BigQuery.\"\"\"\n",
    "    if not isinstance(col_name, str): col_name = str(col_name)\n",
    "    nfkd_form = unicodedata.normalize('NFKD', col_name)\n",
    "    ascii_name = nfkd_form.encode('ASCII', 'ignore').decode('ASCII')\n",
    "    lower_name = ascii_name.lower()\n",
    "    cleaned_name = re.sub(r'\\s+', '_', lower_name)\n",
    "    cleaned_name = re.sub(r'[^\\w_]+', '', cleaned_name)\n",
    "    if cleaned_name and cleaned_name[0].isdigit():\n",
    "        cleaned_name = '_' + cleaned_name\n",
    "    if not cleaned_name:\n",
    "        cleaned_name = 'coluna_sem_nome'\n",
    "    return cleaned_name\n",
    "\n",
    "# --- Execução Principal ---\n",
    "logging.info(\"-----------------------------------------------------\")\n",
    "logging.info(f\"Iniciando envio do arquivo CSV '{nome_arquivo_csv_processado}' para BigQuery.\")\n",
    "logging.info(f\"Origem do CSV: {caminho_completo_csv_processado}\")\n",
    "logging.info(f\"Destino no BigQuery: {PROJECT_ID}:{DESTINATION_TABLE}\")\n",
    "logging.info(\"A tabela no BigQuery será SUBSTITUÍDA.\")\n",
    "logging.info(\"-----------------------------------------------------\")\n",
    "\n",
    "# 1. Autenticar (Verifica arquivo de credenciais)\n",
    "if not os.path.exists(ARQUIVO_CREDENCIAL_JSON):\n",
    "    logging.error(f\"!!! ERRO FATAL: Arquivo de Credenciais JSON não encontrado: '{ARQUIVO_CREDENCIAL_JSON}'. Verifique o caminho.\")\n",
    "    exit()\n",
    "try:\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        ARQUIVO_CREDENCIAL_JSON,\n",
    "        scopes=[\"https://www.googleapis.com/auth/bigquery\"]\n",
    "    )\n",
    "    logging.info(f\"[OK] Credenciais carregadas do arquivo JSON: {ARQUIVO_CREDENCIAL_JSON}\")\n",
    "except Exception as auth_err:\n",
    "    logging.error(f\"!!! ERRO FATAL: Falha ao carregar credenciais do arquivo JSON: {auth_err}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Verifica se o arquivo CSV processado de origem existe\n",
    "if not os.path.exists(caminho_completo_csv_processado):\n",
    "    logging.error(f\"!!! ERRO FATAL: Arquivo CSV processado de origem não encontrado: '{caminho_completo_csv_processado}'.\")\n",
    "    logging.error(\"Certifique-se de que o script de preparação (que gera este CSV) foi executado com sucesso.\")\n",
    "    exit()\n",
    "\n",
    "# 3. Lê o CSV Processado e Prepara o DataFrame\n",
    "df_para_bq = None\n",
    "try:\n",
    "    logging.info(f\"Lendo arquivo CSV processado: {nome_arquivo_csv_processado}...\")\n",
    "    # Seu script de preparação já preenche NaNs com '' e formata datas como string.\n",
    "    # Ler tudo como string inicialmente para controle, depois converter tipos.\n",
    "    df_para_bq = pd.read_csv(caminho_completo_csv_processado, dtype=str, keep_default_na=False, na_values=[''])\n",
    "\n",
    "    if df_para_bq.empty:\n",
    "        raise ValueError(\"O arquivo CSV processado está vazio ou não pôde ser lido corretamente.\")\n",
    "    logging.info(f\"Leitura do CSV processado OK: {len(df_para_bq)} linhas.\")\n",
    "\n",
    "    # Limpa nomes das colunas para conformidade com BigQuery (seu script já faz um bom trabalho, mas isso é uma garantia)\n",
    "    logging.info(\"Limpando nomes das colunas para o BigQuery...\")\n",
    "    df_para_bq.columns = [limpar_nome_coluna_bq(col) for col in df_para_bq.columns]\n",
    "    # Garante que os nomes das colunas nas listas de conversão também estejam no formato limpo\n",
    "    colunas_para_converter_data = [limpar_nome_coluna_bq(col) for col in colunas_para_converter_data]\n",
    "    colunas_para_converter_numerico = [limpar_nome_coluna_bq(col) for col in colunas_para_converter_numerico]\n",
    "    logging.info(f\"Nomes das colunas limpos (ex: {list(df_para_bq.columns[:5])}...).\")\n",
    "\n",
    "\n",
    "    # Converte colunas de data (que devem estar como string 'YYYY-MM-DD' ou vazias)\n",
    "    logging.info(\"Convertendo colunas de data para o tipo datetime (esperando formato YYYY-MM-DD)...\")\n",
    "    for coluna_data in colunas_para_converter_data:\n",
    "        if coluna_data in df_para_bq.columns:\n",
    "            logging.info(f\" -> Convertendo coluna de data: {coluna_data}\")\n",
    "            # errors='coerce' transformará strings vazias ou formatos inválidos em NaT (Not a Time).\n",
    "            df_para_bq[coluna_data] = pd.to_datetime(df_para_bq[coluna_data], errors='coerce', format='%Y-%m-%d')\n",
    "            # Log de nulos (opcional, mas útil para depuração)\n",
    "            num_nulos_dt = df_para_bq[coluna_data].isna().sum()\n",
    "            if num_nulos_dt > 0:\n",
    "                logging.info(f\"      Coluna '{coluna_data}': {num_nulos_dt} valores são nulos (NaT) após conversão.\")\n",
    "        else:\n",
    "            logging.warning(f\"Coluna de data '{coluna_data}' não encontrada no DataFrame após limpeza dos nomes. Verifique a lista.\")\n",
    "\n",
    "    # Converte colunas numéricas\n",
    "    logging.info(f\"Convertendo colunas numéricas para o tipo float/numeric...\")\n",
    "    for coluna_num in colunas_para_converter_numerico:\n",
    "        if coluna_num in df_para_bq.columns:\n",
    "            logging.info(f\" -> Convertendo coluna numérica: {coluna_num}\")\n",
    "            # Tratar possíveis vírgulas como separador decimal antes de converter para numérico\n",
    "            # Seu script de limpeza não parece tratar isso, então é bom garantir aqui.\n",
    "            if df_para_bq[coluna_num].dtype == 'object': # Se for string\n",
    "                 df_para_bq[coluna_num] = df_para_bq[coluna_num].str.replace(',', '.', regex=False)\n",
    "            # errors='coerce' transformará strings vazias ou não numéricas em NaN.\n",
    "            df_para_bq[coluna_num] = pd.to_numeric(df_para_bq[coluna_num], errors='coerce')\n",
    "            # Log de nulos (opcional)\n",
    "            num_nulos_num = df_para_bq[coluna_num].isna().sum()\n",
    "            if num_nulos_num > 0:\n",
    "                logging.info(f\"      Coluna '{coluna_num}': {num_nulos_num} valores são nulos (NaN) após conversão.\")\n",
    "        else:\n",
    "            logging.warning(f\"Coluna numérica '{coluna_num}' não encontrada no DataFrame após limpeza. Verifique a lista.\")\n",
    "\n",
    "    logging.info(\"Preparação do DataFrame para envio ao BigQuery concluída.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"!!! ERRO FATAL durante leitura ou preparação do CSV processado: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    exit()\n",
    "\n",
    "# 4. Envia o DataFrame Final para o BigQuery\n",
    "if df_para_bq is not None and not df_para_bq.empty:\n",
    "    try:\n",
    "        logging.info(f\"Enviando {len(df_para_bq)} linhas para BigQuery: '{PROJECT_ID}:{DESTINATION_TABLE}' (Substituindo tabela)...\")\n",
    "        \n",
    "        pandas_gbq.to_gbq(\n",
    "            df_para_bq,\n",
    "            destination_table=DESTINATION_TABLE,\n",
    "            project_id=PROJECT_ID,\n",
    "            credentials=credentials,\n",
    "            if_exists='replace', # SUBSTITUI A TABELA!\n",
    "            progress_bar=True\n",
    "        )\n",
    "\n",
    "        logging.info(f\"✅ SUCESSO! Dados enviados e tabela '{DESTINATION_TABLE}' foi substituída no BigQuery.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"!!! ERRO AO ENVIAR PARA O BIGQUERY: {e}\")\n",
    "        logging.error(\"Verifique: PROJECT_ID, DESTINATION_TABLE, API do BigQuery Ativa, Permissões da Conta de Serviço (precisa de 'Editor de Dados do BigQuery' e 'Usuário de Tarefas do BigQuery'), Tipos de Dados no DataFrame.\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    logging.error(\"DataFrame 'df_para_bq' está vazio ou não foi definido. Nenhum dado enviado para o BigQuery.\")\n",
    "\n",
    "logging.info(f\"--- FIM DO SCRIPT DE ENVIO PARA BIGQUERY ({DESTINATION_TABLE}) ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf81b7f",
   "metadata": {},
   "source": [
    "# Transportadora \n",
    "\n",
    "médias de caixas e km, para enviar para bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c61c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 15:44:43,884 - INFO - -----------------------------------------------------\n",
      "2025-05-14 15:44:43,885 - INFO - Processando 'Transportadora (Qtd de caixas, Km).xlsx' para gerar 'Transportadora.csv'.\n",
      "2025-05-14 15:44:43,887 - INFO - Selecionando colunas, limpando N/A, DIVIDINDO Quantidade, LIMPANDO Distância, formatando datas, etc.\n",
      "2025-05-14 15:44:43,887 - INFO - -----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 15:44:43,894 - INFO - [OK] Pasta de destino CSV verificada/criada: 'G:\\Meu Drive\\Relatorio Oficial 2025\\Drive\\Custos Transportadora'\n",
      "2025-05-14 15:44:43,967 - INFO - Lendo arquivo Excel: Transportadora (Qtd de caixas, Km).xlsx...\n",
      "2025-05-14 15:44:44,136 - INFO - Leitura inicial completa: 1417 linhas.\n",
      "2025-05-14 15:44:44,136 - INFO - Selecionando colunas: ['Número da Rota', 'Status', 'Quantidade de Pacotes', 'Data de Atualização', 'Distância', 'Transportadora']...\n",
      "2025-05-14 15:44:44,136 - INFO - Colunas mantidas inicialmente: ['Número da Rota', 'Status', 'Quantidade de Pacotes', 'Data de Atualização', 'Distância', 'Transportadora']\n",
      "2025-05-14 15:44:44,136 - INFO - Realizando limpeza básica (espaços, linhas vazias, N/A)...\n",
      "2025-05-14 15:44:44,150 - INFO - Substituindo 'N/A' por vazio...\n",
      "2025-05-14 15:44:44,156 - INFO - Linhas após limpeza: 1417\n",
      "2025-05-14 15:44:44,156 - INFO - Dividindo coluna 'Quantidade de Pacotes'...\n",
      "2025-05-14 15:44:44,156 - INFO -  -> Colunas 'Qtd Prevista' e 'Qtd Realizada' criadas.\n",
      "2025-05-14 15:44:44,156 - INFO - Limpando e convertendo coluna 'Distância'...\n",
      "2025-05-14 15:44:44,156 - WARNING -  -> 29 valores em 'Distância' não convertidos para número.\n",
      "2025-05-14 15:44:44,172 - INFO -  -> Coluna 'Distância' processada.\n",
      "2025-05-14 15:44:44,172 - INFO - Removendo rotas duplicadas baseado em 'Número da Rota' (mantendo a última)...\n",
      "2025-05-14 15:44:44,174 - INFO - Linhas após remoção de duplicatas: 1417\n",
      "2025-05-14 15:44:44,174 - INFO - Formatando colunas de data...\n",
      "2025-05-14 15:44:44,174 - INFO - Formatando datas para AAAA-MM-DD nas colunas: ['Data de Atualização']...\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_7948\\3718997851.py:63: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  converted_dates = pd.to_datetime(coluna_str, errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_7948\\3718997851.py:70: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_copy[col].fillna('', inplace=True)\n",
      "2025-05-14 15:44:44,179 - WARNING - Coluna 'Data de Atualização': 1129 valores não formatados (ficarão vazios).\n",
      "2025-05-14 15:44:44,179 - INFO - Limpando nomes das colunas para o CSV final...\n",
      "2025-05-14 15:44:44,179 - INFO - Nomes das colunas limpos (ex: 'Número da Rota' virou 'numero_da_rota').\n",
      "2025-05-14 15:44:44,179 - INFO - Salvando dados processados em CSV: 'G:\\Meu Drive\\Relatorio Oficial 2025\\Drive\\Custos Transportadora\\Transportadora.csv'...\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_7948\\3718997851.py:199: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_processed.fillna('', inplace=True) # Garante que nulos/NaN virem strings vazias\n",
      "2025-05-14 15:44:44,261 - INFO - ✅ Arquivo CSV 'Transportadora.csv' criado/atualizado com sucesso em 'G:\\Meu Drive\\Relatorio Oficial 2025\\Drive\\Custos Transportadora'.\n",
      "2025-05-14 15:44:44,262 - INFO - --- FIM DO SCRIPT ---\n"
     ]
    }
   ],
   "source": [
    "# Script ATUALIZADO para ler Excel, SELECIONAR COLUNAS, limpar N/A,\n",
    "# DIVIDIR Quantidade, PROCESSAR Distancia, formatar datas, remover duplicatas,\n",
    "# limpar nomes colunas e salvar como Transportadora.csv.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import csv\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# --- Configurações ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Caminhos\n",
    "caminho_pasta_origem_excel = r'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Excel - Pedidos'\n",
    "nome_arquivo_excel_origem = 'Transportadora (Qtd de caixas, Km).xlsx'\n",
    "caminho_completo_excel_origem = os.path.join(caminho_pasta_origem_excel, nome_arquivo_excel_origem)\n",
    "\n",
    "caminho_pasta_destino_csv = r'G:\\Meu Drive\\Relatorio Oficial 2025\\Drive\\Custos Transportadora'\n",
    "nome_arquivo_csv_destino = 'Transportadora.csv'\n",
    "caminho_completo_csv_destino = os.path.join(caminho_pasta_destino_csv, nome_arquivo_csv_destino)\n",
    "\n",
    "# --- Definições de Colunas ---\n",
    "\n",
    "# ====> Colunas que você quer MANTER/PROCESSAR do Excel <====\n",
    "# !!! VERIFIQUE OS NOMES EXATOS NO SEU EXCEL !!!\n",
    "# Inclua a coluna original que será dividida (ex: 'Quantidade de Pacotes')\n",
    "colunas_selecionadas = [\n",
    "    'Número da Rota',        # Coluna Chave\n",
    "    'Status',\n",
    "    'Quantidade de Pacotes', # <<< COLUNA A SER DIVIDIDA (VERIFIQUE NOME!)\n",
    "    'Data de Atualização',   # <<< Coluna de Data\n",
    "    'Distância',             # <<< Coluna a ser processada (VERIFIQUE NOME!)\n",
    "    'Transportadora'\n",
    "]\n",
    "\n",
    "# Coluna a ser dividida e os nomes das novas colunas resultantes\n",
    "coluna_para_dividir = 'Quantidade de Pacotes' # <<< NOME EXATO NO EXCEL\n",
    "nova_coluna_prevista = 'Qtd Prevista'         # Nome da 1ª nova coluna\n",
    "nova_coluna_realizada = 'Qtd Realizada'       # Nome da 2ª nova coluna\n",
    "\n",
    "# Coluna de distância a ser limpa\n",
    "coluna_distancia = 'Distância' # <<< NOME EXATO NO EXCEL\n",
    "\n",
    "# Coluna chave para identificar rotas únicas (DEVE estar em colunas_selecionadas)\n",
    "coluna_chave = 'Número da Rota'\n",
    "\n",
    "# Colunas de DATA (que estão em colunas_selecionadas) para formatar AAAA-MM-DD\n",
    "colunas_datas_simples = [ 'Data de Atualização' ]\n",
    "\n",
    "# --- Funções Auxiliares ---\n",
    "def format_date_columns(df, colunas_data):\n",
    "    # (Mesma função das respostas anteriores)\n",
    "    if df is None or df.empty: return df\n",
    "    df_copy = df.copy()\n",
    "    logging.info(f\"Formatando datas para AAAA-MM-DD nas colunas: {colunas_data}...\")\n",
    "    for col in colunas_data:\n",
    "        if col in df_copy.columns:\n",
    "            coluna_original_na_count = df_copy[col].isna().sum()\n",
    "            coluna_str = df_copy[col].astype(str)\n",
    "            try:\n",
    "                converted_dates = pd.to_datetime(coluna_str, errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
    "                falhou_inicial = converted_dates.isna()\n",
    "                if falhou_inicial.any():\n",
    "                    try:\n",
    "                        converted_dates.loc[falhou_inicial] = pd.to_datetime(coluna_str[falhou_inicial], errors='coerce', format='%Y-%m-%d')\n",
    "                    except ValueError: pass\n",
    "                df_copy[col] = converted_dates.dt.strftime('%Y-%m-%d')\n",
    "                df_copy[col].fillna('', inplace=True)\n",
    "                num_falhas_final = converted_dates.isna().sum()\n",
    "                novas_falhas = num_falhas_final - coluna_original_na_count\n",
    "                if novas_falhas > 0: logging.warning(f\"Coluna '{col}': {novas_falhas} valores não formatados (ficarão vazios).\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Erro ao converter coluna de data '{col}': {e}\")\n",
    "                df_copy[col] = ''\n",
    "        else:\n",
    "             logging.warning(f\"Coluna de data '{col}' definida para formatação não encontrada.\")\n",
    "    return df_copy\n",
    "\n",
    "def limpar_nome_coluna(col_name):\n",
    "    # (Mesma função das respostas anteriores)\n",
    "    if not isinstance(col_name, str): col_name = str(col_name)\n",
    "    nfkd_form = unicodedata.normalize('NFKD', col_name)\n",
    "    ascii_name = nfkd_form.encode('ASCII', 'ignore').decode('ASCII')\n",
    "    lower_name = ascii_name.lower()\n",
    "    cleaned_name = re.sub(r'\\s+', '_', lower_name)\n",
    "    cleaned_name = re.sub(r'[^\\w_]+', '', cleaned_name)\n",
    "    cleaned_name = re.sub(r'_+', '_', cleaned_name)\n",
    "    cleaned_name = cleaned_name.strip('_')\n",
    "    if cleaned_name and cleaned_name[0].isdigit(): cleaned_name = '_' + cleaned_name\n",
    "    if not cleaned_name: cleaned_name = 'coluna_vazia'\n",
    "    return cleaned_name\n",
    "\n",
    "# --- Execução Principal ---\n",
    "logging.info(\"-----------------------------------------------------\")\n",
    "logging.info(f\"Processando '{nome_arquivo_excel_origem}' para gerar '{nome_arquivo_csv_destino}'.\")\n",
    "logging.info(\"Selecionando colunas, limpando N/A, DIVIDINDO Quantidade, LIMPANDO Distância, formatando datas, etc.\")\n",
    "logging.info(\"-----------------------------------------------------\")\n",
    "\n",
    "# 1. Garante que a pasta de destino do CSV exista\n",
    "try:\n",
    "    os.makedirs(caminho_pasta_destino_csv, exist_ok=True)\n",
    "    logging.info(f\"[OK] Pasta de destino CSV verificada/criada: '{caminho_pasta_destino_csv}'\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"!!! ERRO FATAL: Falha ao verificar/criar pasta de destino: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Verifica se o arquivo Excel de origem existe\n",
    "if not os.path.exists(caminho_completo_excel_origem):\n",
    "    logging.error(f\"!!! ERRO FATAL: Arquivo Excel de ORIGEM não encontrado: '{caminho_completo_excel_origem}'.\")\n",
    "    exit()\n",
    "\n",
    "# 3. Lê o arquivo Excel de origem\n",
    "df_source = None\n",
    "try:\n",
    "    logging.info(f\"Lendo arquivo Excel: {nome_arquivo_excel_origem}...\")\n",
    "    df_source = pd.read_excel(caminho_completo_excel_origem, dtype=str)\n",
    "    if df_source is None or df_source.empty: raise ValueError(\"Planilha Excel vazia.\")\n",
    "    logging.info(f\"Leitura inicial completa: {len(df_source)} linhas.\")\n",
    "\n",
    "    # 4. Filtra APENAS as colunas desejadas (incluindo a que será dividida)\n",
    "    logging.info(f\"Selecionando colunas: {colunas_selecionadas}...\")\n",
    "    colunas_encontradas = [col for col in colunas_selecionadas if col in df_source.columns]\n",
    "    colunas_faltantes = [col for col in colunas_selecionadas if col not in df_source.columns]\n",
    "    if not colunas_encontradas: raise ValueError(\"Nenhuma coluna desejada foi encontrada.\")\n",
    "    if colunas_faltantes: logging.warning(f\"Colunas NÃO encontradas: {colunas_faltantes}\")\n",
    "    if coluna_chave not in colunas_encontradas: raise ValueError(f\"Coluna chave '{coluna_chave}' não encontrada/selecionada.\")\n",
    "    if coluna_para_dividir not in colunas_encontradas: logging.warning(f\"Coluna '{coluna_para_dividir}' para divisão não encontrada/selecionada.\")\n",
    "    if coluna_distancia not in colunas_encontradas: logging.warning(f\"Coluna '{coluna_distancia}' para limpeza não encontrada/selecionada.\")\n",
    "\n",
    "    df_processed = df_source[colunas_encontradas].copy()\n",
    "    logging.info(f\"Colunas mantidas inicialmente: {list(df_processed.columns)}\")\n",
    "\n",
    "    # 5. Limpeza de Dados (N/A, Espaços, Chave Vazia)\n",
    "    logging.info(\"Realizando limpeza básica (espaços, linhas vazias, N/A)...\")\n",
    "    df_processed.dropna(how='all', inplace=True)\n",
    "    for col in df_processed.columns:\n",
    "        if df_processed[col].dtype == 'object': df_processed[col] = df_processed[col].str.strip()\n",
    "    logging.info(\"Substituindo 'N/A' por vazio...\")\n",
    "    df_processed.replace('N/A', '', inplace=True)\n",
    "    df_processed.dropna(subset=[coluna_chave], inplace=True)\n",
    "    df_processed = df_processed[df_processed[coluna_chave].astype(str).str.strip() != '']\n",
    "    if df_processed.empty: raise ValueError(f\"Nenhuma linha válida com '{coluna_chave}' após limpeza.\")\n",
    "    logging.info(f\"Linhas após limpeza: {len(df_processed)}\")\n",
    "\n",
    "    # ****** ETAPA DE PROCESSAMENTO ESPECÍFICO ******\n",
    "    # 5a. Dividir Coluna de Quantidade (ex: \"31 / 31\" -> colunas 'Qtd Prevista', 'Qtd Realizada')\n",
    "    if coluna_para_dividir in df_processed.columns:\n",
    "        logging.info(f\"Dividindo coluna '{coluna_para_dividir}'...\")\n",
    "        # Divide pela barra '/', cria novas colunas temporárias\n",
    "        split_cols = df_processed[coluna_para_dividir].astype(str).str.split('/', n=1, expand=True)\n",
    "        # Atribui às novas colunas no dataframe principal, limpando espaços\n",
    "        df_processed[nova_coluna_prevista] = split_cols[0].str.strip()\n",
    "        df_processed[nova_coluna_realizada] = split_cols[1].str.strip() if split_cols.shape[1] > 1 else ''\n",
    "        # Tenta converter para número\n",
    "        df_processed[nova_coluna_prevista] = pd.to_numeric(df_processed[nova_coluna_prevista], errors='coerce')\n",
    "        df_processed[nova_coluna_realizada] = pd.to_numeric(df_processed[nova_coluna_realizada], errors='coerce')\n",
    "        # Remove a coluna original\n",
    "        df_processed.drop(columns=[coluna_para_dividir], inplace=True)\n",
    "        logging.info(f\" -> Colunas '{nova_coluna_prevista}' e '{nova_coluna_realizada}' criadas.\")\n",
    "    else:\n",
    "        logging.warning(f\"Coluna '{coluna_para_dividir}' não encontrada para divisão.\")\n",
    "\n",
    "    # 5b. Limpar Coluna de Distância (ex: \"82,9 Km\" -> 82.9)\n",
    "    if coluna_distancia in df_processed.columns:\n",
    "        logging.info(f\"Limpando e convertendo coluna '{coluna_distancia}'...\")\n",
    "        # Remove \"km\" (case-insensitive) e espaços, troca vírgula por ponto, converte para número\n",
    "        df_processed[coluna_distancia] = pd.to_numeric(\n",
    "            df_processed[coluna_distancia].astype(str).str.replace(r'(?i)\\s*km', '', regex=True).str.replace(',', '.', regex=False).str.strip(),\n",
    "            errors='coerce'\n",
    "        )\n",
    "        dist_nulos = df_processed[coluna_distancia].isna().sum()\n",
    "        if dist_nulos > 0: logging.warning(f\" -> {dist_nulos} valores em '{coluna_distancia}' não convertidos para número.\")\n",
    "        logging.info(f\" -> Coluna '{coluna_distancia}' processada.\")\n",
    "    else:\n",
    "        logging.warning(f\"Coluna '{coluna_distancia}' não encontrada para limpeza.\")\n",
    "    # ***************************************************\n",
    "\n",
    "    # 6. Remove Duplicatas baseado na chave\n",
    "    logging.info(f\"Removendo rotas duplicadas baseado em '{coluna_chave}' (mantendo a última)...\")\n",
    "    df_processed.drop_duplicates(subset=[coluna_chave], keep='last', inplace=True)\n",
    "    logging.info(f\"Linhas após remoção de duplicatas: {len(df_processed)}\")\n",
    "\n",
    "    # 7. Formata as Colunas de Data\n",
    "    logging.info(\"Formatando colunas de data...\")\n",
    "    df_processed = format_date_columns(df_processed, colunas_datas_simples)\n",
    "\n",
    "    # 8. Limpa Nomes das Colunas ANTES de Salvar\n",
    "    logging.info(\"Limpando nomes das colunas para o CSV final...\")\n",
    "    df_processed.columns = [limpar_nome_coluna(col) for col in df_processed.columns]\n",
    "    # Pega o nome limpo da coluna chave para referência (opcional)\n",
    "    chave_limpa = limpar_nome_coluna(coluna_chave)\n",
    "    logging.info(f\"Nomes das colunas limpos (ex: '{coluna_chave}' virou '{chave_limpa}').\")\n",
    "\n",
    "    # 9. Salva o Resultado Final como CSV\n",
    "    if not df_processed.empty:\n",
    "        logging.info(f\"Salvando dados processados em CSV: '{caminho_completo_csv_destino}'...\")\n",
    "        df_processed.fillna('', inplace=True) # Garante que nulos/NaN virem strings vazias\n",
    "        df_processed.to_csv(caminho_completo_csv_destino, sep=',', encoding='utf-8-sig', index=False, quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        logging.info(f\"✅ Arquivo CSV '{nome_arquivo_csv_destino}' criado/atualizado com sucesso em '{caminho_pasta_destino_csv}'.\")\n",
    "    else:\n",
    "         logging.warning(\"DataFrame final está vazio após processamento. Nenhum dado foi salvo.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"!!! ERRO FATAL: Arquivo Excel não encontrado em '{caminho_completo_excel_origem}'.\")\n",
    "except ValueError as ve:\n",
    "    logging.error(f\"!!! ERRO FATAL: Problema com os dados ou colunas: {ve}\")\n",
    "except PermissionError:\n",
    "    logging.error(f\"!!! ERRO DE PERMISSÃO ao salvar CSV em '{caminho_completo_csv_destino}'. Verifique se o arquivo está aberto ou se há permissão.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"!!! ERRO GERAL INESPERADO: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "logging.info(\"--- FIM DO SCRIPT ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a160594a",
   "metadata": {},
   "source": [
    "# Separação\n",
    "Verificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08213d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Arquivo Acompanhamento de separação Ciclo 01-25.xlsm processado com datas únicas!\n",
      "✔️ Arquivo Acompanhamento de separação Ciclo 02-25.xlsm processado com datas únicas!\n",
      "✔️ Arquivo Acompanhamento de separação Ciclo 03-25.xlsm processado com datas únicas!\n",
      "✔️ Arquivo Acompanhamento de separação Ciclo 04-25.xlsm processado com datas únicas!\n",
      "✔️ Arquivo Acompanhamento de separação Ciclo 05-25.xlsm processado com datas únicas!\n",
      "✔️ Arquivo Acompanhamento de separação Ciclo 06-25.xlsm processado com datas únicas!\n",
      "✔️ Arquivo Acompanhamento de separação Ciclo 07-25.xlsm processado com datas únicas!\n",
      "✔️ Arquivo Acompanhamento de separação Ciclo 08-25.xlsm processado com datas únicas!\n",
      "⚠️ Arquivo Acompanhamento de separação Ciclo 09-25.xlsm não encontrado.\n",
      "⚠️ Arquivo Acompanhamento de separação Ciclo 10-25.xlsm não encontrado.\n",
      "⚠️ Arquivo Acompanhamento de separação Ciclo 11-25.xlsm não encontrado.\n",
      "⚠️ Arquivo Acompanhamento de separação Ciclo 12-25.xlsm não encontrado.\n",
      "🔄 DataFrames concatenados! Total de linhas: 1562\n",
      "✅ Arquivo final salvo em: G:\\Meu Drive\\Relatorio Oficial 2025\\Drive\\Pedidos Separados\\Acompanhamento_separação_completo.xlsx\n"
     ]
    }
   ],
   "source": [
    "# prompt: ta errado ainda, faz pela data mesmo, do dia 28 de dez 2024, até today(), a planilha tem q vim com essas datas, mas n pode se repetir, se não, vai fica os mesmo dados.\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 🗂 Caminho onde deseja salvar o arquivo final (MODIFIQUE CONFORME SUA MÁQUINA)\n",
    "caminho_pasta_local = r\"G:\\Meu Drive\\Relatorio Oficial 2025\\Drive\\Pedidos Separados\"\n",
    "\n",
    "# 🛠 Criar a pasta caso não exista\n",
    "if not os.path.exists(caminho_pasta_local):\n",
    "    os.makedirs(caminho_pasta_local)\n",
    "    print(f\"Pasta criada: {caminho_pasta_local}\")\n",
    "\n",
    "# 📂 Pasta onde os arquivos XLSM estão no Google Drive\n",
    "pasta_drive = r\"G:.shortcut-targets-by-id\\1NpLOQC_eCJ4vFpBiP54a5AhOorhBC0kZ\\Logística\\Acompanhamento separação\"\n",
    "\n",
    "# 📊 Lista para armazenar os DataFrames de cada arquivo\n",
    "dfs = []\n",
    "datas_vistas = set()  # cria um conjunto, que é uma estrutura de dados que armazena valores únicos e não ordenados.\n",
    "# Isso significa que ele não permite elementos duplicados e pode ser útil para rastrear valores distintos.\n",
    "\n",
    "\n",
    "# 🔄 Loop para processar os arquivos\n",
    "data_inicio = datetime(2024, 12, 28)\n",
    "data_fim = datetime.today()\n",
    "\n",
    "for i in range(1, 13):  # Ciclos de 1 a 12\n",
    "    nome_arquivo = f'Acompanhamento de separação Ciclo {i:02}-25.xlsm'\n",
    "    caminho_arquivo = os.path.join(pasta_drive, nome_arquivo)\n",
    "\n",
    "    # Verifica se o arquivo existe antes de tentar abrir\n",
    "    if os.path.exists(caminho_arquivo):\n",
    "        try:\n",
    "            df = pd.read_excel(caminho_arquivo, sheet_name=\"Base de dados\", engine='openpyxl')\n",
    "\n",
    "            # Converte a coluna de data para datetime, ignorando erros\n",
    "            if 'Data' in df.columns:\n",
    "                df['Data'] = pd.to_datetime(df['Data'], errors='coerce')\n",
    "\n",
    "                # 🔍 Remove registros que possuem uma data já vista anteriormente\n",
    "                df_sem_repetidos = df[~df['Data'].isin(datas_vistas)]\n",
    "\n",
    "                # Atualiza as datas já vistas\n",
    "                datas_vistas.update(df_sem_repetidos['Data'].unique())\n",
    "\n",
    "                # Filtra a data no range definido\n",
    "                df_filtrado = df_sem_repetidos[(df_sem_repetidos['Data'] >= data_inicio) & (df_sem_repetidos['Data'] <= data_fim)]\n",
    "\n",
    "                dfs.append(df_filtrado)\n",
    "                print(f\"✔️ Arquivo {nome_arquivo} processado com datas únicas!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro ao ler o arquivo {nome_arquivo}: {e}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Arquivo {nome_arquivo} não encontrado.\")\n",
    "\n",
    "# 🏁 Concatenando os DataFrames\n",
    "if dfs:\n",
    "    df_final = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"🔄 DataFrames concatenados! Total de linhas: {df_final.shape[0]}\")\n",
    "\n",
    "    # 📥 Caminho do arquivo final na sua máquina\n",
    "    caminho_arquivo_local = os.path.join(caminho_pasta_local, \"Acompanhamento_separação_completo.xlsx\")\n",
    "\n",
    "    # 💾 Salvando na sua máquina\n",
    "    df_final.to_excel(caminho_arquivo_local, index=False, engine='openpyxl')\n",
    "\n",
    "    print(f\"✅ Arquivo final salvo em: {caminho_arquivo_local}\")\n",
    "else:\n",
    "    print(\"⚠️ Nenhum arquivo foi processado. Verifique os caminhos dos arquivos.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9617a0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 10:33:50,770 - INFO - Período de filtro: 2024-12-28 a 2025-06-06\n",
      "2025-06-06 10:33:50,795 - INFO - Pasta de destino verificada/criada: G:\\Meu Drive\\Relatorio Oficial 2025\\Drive\\Pedidos Separados\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 10:33:50,799 - INFO - Iniciando processamento dos arquivos de ciclo...\n",
      "2025-06-06 10:33:50,803 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 01-25.xlsm...\n",
      "2025-06-06 10:33:50,811 - INFO -   -> Lendo aba 'Base de dados'...\n",
      "2025-06-06 10:33:52,045 - INFO -   -> Leitura OK: 470 linhas.\n",
      "2025-06-06 10:33:52,059 - WARNING -   -> 206 linhas removidas por terem data inválida ou vazia.\n",
      "2025-06-06 10:33:52,059 - INFO -   -> Padronizando coluna 'Responsável' no arquivo Acompanhamento de separação Ciclo 01-25.xlsm...\n",
      "2025-06-06 10:33:52,071 - INFO -   -> Coluna 'Responsável' padronizada.\n",
      "2025-06-06 10:33:52,079 - INFO -   -> ✔️ 264 linhas adicionadas de Acompanhamento de separação Ciclo 01-25.xlsm (datas únicas no período).\n",
      "2025-06-06 10:33:52,079 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 02-25.xlsm...\n",
      "2025-06-06 10:33:52,088 - INFO -   -> Lendo aba 'Base de dados'...\n",
      "2025-06-06 10:33:54,374 - INFO -   -> Leitura OK: 362 linhas.\n",
      "2025-06-06 10:33:54,398 - WARNING -   -> 15 linhas removidas por terem data inválida ou vazia.\n",
      "2025-06-06 10:33:54,408 - INFO -   -> Padronizando coluna 'Responsável' no arquivo Acompanhamento de separação Ciclo 02-25.xlsm...\n",
      "2025-06-06 10:33:54,415 - INFO -   -> Coluna 'Responsável' padronizada.\n",
      "2025-06-06 10:33:54,415 - INFO -   -> ✔️ 286 linhas adicionadas de Acompanhamento de separação Ciclo 02-25.xlsm (datas únicas no período).\n",
      "2025-06-06 10:33:54,415 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 03-25.xlsm...\n",
      "2025-06-06 10:33:54,427 - INFO -   -> Lendo aba 'Base de dados'...\n",
      "2025-06-06 10:33:56,171 - INFO -   -> Leitura OK: 532 linhas.\n",
      "2025-06-06 10:33:56,188 - INFO -   -> Padronizando coluna 'Responsável' no arquivo Acompanhamento de separação Ciclo 03-25.xlsm...\n",
      "2025-06-06 10:33:56,197 - INFO -   -> Coluna 'Responsável' padronizada.\n",
      "2025-06-06 10:33:56,201 - INFO -   -> ✔️ 182 linhas adicionadas de Acompanhamento de separação Ciclo 03-25.xlsm (datas únicas no período).\n",
      "2025-06-06 10:33:56,201 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 04-25.xlsm...\n",
      "2025-06-06 10:33:56,210 - INFO -   -> Lendo aba 'Base de dados'...\n",
      "2025-06-06 10:33:57,686 - INFO -   -> Leitura OK: 716 linhas.\n",
      "2025-06-06 10:33:57,702 - WARNING -   -> 2 linhas removidas por terem data inválida ou vazia.\n",
      "2025-06-06 10:33:57,712 - INFO -   -> Padronizando coluna 'Responsável' no arquivo Acompanhamento de separação Ciclo 04-25.xlsm...\n",
      "2025-06-06 10:33:57,716 - INFO -   -> Coluna 'Responsável' padronizada.\n",
      "2025-06-06 10:33:57,719 - INFO -   -> ✔️ 187 linhas adicionadas de Acompanhamento de separação Ciclo 04-25.xlsm (datas únicas no período).\n",
      "2025-06-06 10:33:57,721 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 05-25.xlsm...\n",
      "2025-06-06 10:33:57,727 - INFO -   -> Lendo aba 'Base de dados'...\n",
      "2025-06-06 10:34:10,897 - INFO -   -> Leitura OK: 1048519 linhas.\n",
      "2025-06-06 10:34:11,447 - WARNING -   -> 1048032 linhas removidas por terem data inválida ou vazia.\n",
      "2025-06-06 10:34:11,454 - INFO -   -> Padronizando coluna 'Responsável' no arquivo Acompanhamento de separação Ciclo 05-25.xlsm...\n",
      "2025-06-06 10:34:11,467 - INFO -   -> Coluna 'Responsável' padronizada.\n",
      "2025-06-06 10:34:11,471 - INFO -   -> ✔️ 450 linhas adicionadas de Acompanhamento de separação Ciclo 05-25.xlsm (datas únicas no período).\n",
      "2025-06-06 10:34:11,471 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 06-25.xlsm...\n",
      "2025-06-06 10:34:11,479 - INFO -   -> Lendo aba 'Base de dados'...\n",
      "2025-06-06 10:34:25,196 - INFO -   -> Leitura OK: 1048324 linhas.\n",
      "2025-06-06 10:34:25,709 - WARNING -   -> 1048032 linhas removidas por terem data inválida ou vazia.\n",
      "2025-06-06 10:34:25,717 - INFO -   -> Todas as datas válidas de Acompanhamento de separação Ciclo 06-25.xlsm já foram vistas em arquivos anteriores.\n",
      "2025-06-06 10:34:25,717 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 07-25.xlsm...\n",
      "2025-06-06 10:34:25,725 - INFO -   -> Lendo aba 'Base de dados'...\n",
      "2025-06-06 10:34:38,509 - INFO -   -> Leitura OK: 1047968 linhas.\n",
      "2025-06-06 10:34:38,894 - WARNING -   -> 1047794 linhas removidas por terem data inválida ou vazia.\n",
      "2025-06-06 10:34:38,902 - INFO -   -> Padronizando coluna 'Responsável' no arquivo Acompanhamento de separação Ciclo 07-25.xlsm...\n",
      "2025-06-06 10:34:38,905 - INFO -   -> Coluna 'Responsável' padronizada.\n",
      "2025-06-06 10:34:38,909 - INFO -   -> ✔️ 133 linhas adicionadas de Acompanhamento de separação Ciclo 07-25.xlsm (datas únicas no período).\n",
      "2025-06-06 10:34:38,911 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 08-25.xlsm...\n",
      "2025-06-06 10:34:38,914 - INFO -   -> Lendo aba 'Base de dados'...\n",
      "2025-06-06 10:34:51,316 - INFO -   -> Leitura OK: 1047792 linhas.\n",
      "2025-06-06 10:34:51,688 - WARNING -   -> 1047675 linhas removidas por terem data inválida ou vazia.\n",
      "2025-06-06 10:34:51,695 - INFO -   -> Padronizando coluna 'Responsável' no arquivo Acompanhamento de separação Ciclo 08-25.xlsm...\n",
      "2025-06-06 10:34:51,700 - INFO -   -> Coluna 'Responsável' padronizada.\n",
      "2025-06-06 10:34:51,703 - INFO -   -> ✔️ 69 linhas adicionadas de Acompanhamento de separação Ciclo 08-25.xlsm (datas únicas no período).\n",
      "2025-06-06 10:34:51,705 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 09-25.xlsm...\n",
      "2025-06-06 10:34:51,706 - WARNING -   -> ⚠️ Arquivo não encontrado: G:.shortcut-targets-by-id\\1NpLOQC_eCJ4vFpBiP54a5AhOorhBC0kZ\\Logística\\Acompanhamento separação\\Acompanhamento de separação Ciclo 09-25.xlsm\n",
      "2025-06-06 10:34:51,710 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 10-25.xlsm...\n",
      "2025-06-06 10:34:51,714 - WARNING -   -> ⚠️ Arquivo não encontrado: G:.shortcut-targets-by-id\\1NpLOQC_eCJ4vFpBiP54a5AhOorhBC0kZ\\Logística\\Acompanhamento separação\\Acompanhamento de separação Ciclo 10-25.xlsm\n",
      "2025-06-06 10:34:51,714 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 11-25.xlsm...\n",
      "2025-06-06 10:34:51,718 - WARNING -   -> ⚠️ Arquivo não encontrado: G:.shortcut-targets-by-id\\1NpLOQC_eCJ4vFpBiP54a5AhOorhBC0kZ\\Logística\\Acompanhamento separação\\Acompanhamento de separação Ciclo 11-25.xlsm\n",
      "2025-06-06 10:34:51,720 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 12-25.xlsm...\n",
      "2025-06-06 10:34:51,724 - WARNING -   -> ⚠️ Arquivo não encontrado: G:.shortcut-targets-by-id\\1NpLOQC_eCJ4vFpBiP54a5AhOorhBC0kZ\\Logística\\Acompanhamento separação\\Acompanhamento de separação Ciclo 12-25.xlsm\n",
      "2025-06-06 10:34:51,726 - INFO - Processamento dos arquivos de ciclo concluído.\n",
      "2025-06-06 10:34:51,727 - INFO - Concatenando resultados...\n",
      "2025-06-06 10:34:51,847 - INFO - 🔄 DataFrames concatenados! Total de linhas final: 1571\n",
      "2025-06-06 10:34:51,851 - INFO - Formatando coluna 'Data' para AAAA-MM-DD no DataFrame final...\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\966714565.py:135: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_final['Data'].fillna('', inplace=True)\n",
      "2025-06-06 10:34:51,860 - INFO - Coluna 'Data' formatada com sucesso.\n",
      "2025-06-06 10:34:51,863 - INFO - Salvando arquivo final em: G:\\Meu Drive\\Relatorio Oficial 2025\\Drive\\Pedidos Separados\\Acompanhamento_separação_completo_formatado.xlsx...\n",
      "2025-06-06 10:34:53,450 - INFO - ✅ Arquivo final salvo com sucesso!\n",
      "2025-06-06 10:34:53,457 - INFO - --- FIM DO SCRIPT ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import re # Importar a biblioteca re\n",
    "\n",
    "# --- Configurações ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 🗂 Caminho onde deseja salvar o arquivo final\n",
    "caminho_pasta_local = r\"G:\\Meu Drive\\Relatorio Oficial 2025\\Drive\\Pedidos Separados\"\n",
    "nome_arquivo_final = \"Acompanhamento_separação_completo_formatado.xlsx\" # Nome do arquivo de saída\n",
    "caminho_arquivo_local = os.path.join(caminho_pasta_local, nome_arquivo_final)\n",
    "\n",
    "# 📂 Pasta onde os arquivos XLSM de origem estão\n",
    "pasta_drive = r\"G:.shortcut-targets-by-id\\1NpLOQC_eCJ4vFpBiP54a5AhOorhBC0kZ\\Logística\\Acompanhamento separação\"\n",
    "\n",
    "# 📅 Período de datas para filtrar\n",
    "data_inicio = datetime(2024, 12, 28)\n",
    "data_fim = datetime.today().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "logging.info(f\"Período de filtro: {data_inicio.strftime('%Y-%m-%d')} a {data_fim.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "def padronizar_responsavel(nome_responsavel):\n",
    "    \"\"\"\n",
    "    Padroniza o nome do responsável:\n",
    "    - Corrige nomes específicos ('deigo' -> 'Diego', 'Miqueias' -> 'Miquieias').\n",
    "    - Remove espaços extras no início/fim.\n",
    "    - Converte para minúsculas.\n",
    "    - Capitaliza a primeira letra de cada palavra.\n",
    "    \"\"\"\n",
    "    if pd.isna(nome_responsavel) or not isinstance(nome_responsavel, str) or nome_responsavel.strip() == '':\n",
    "        return '' # Retorna string vazia para nulos, não-strings ou strings vazias\n",
    "\n",
    "    nome_processado = str(nome_responsavel).strip()\n",
    "\n",
    "    if 'deigo' in nome_processado.lower():\n",
    "        nome_processado = re.sub(r'deigo', 'Diego', nome_processado, flags=re.IGNORECASE)\n",
    "\n",
    "    if 'miqueias' in nome_processado.lower():\n",
    "        nome_processado = re.sub(r'miqueias', 'Miquieias', nome_processado, flags=re.IGNORECASE)\n",
    "\n",
    "    nome_padronizado = ' '.join(word.capitalize() for word in nome_processado.lower().split())\n",
    "\n",
    "    if 'Diego' in nome_padronizado and 'deigo' in str(nome_responsavel).strip().lower():\n",
    "        nome_padronizado = re.sub(r'Deigo', 'Diego', nome_padronizado, flags=re.IGNORECASE)\n",
    "\n",
    "    if 'Miquieias' in nome_padronizado and 'miqueias' in str(nome_responsavel).strip().lower():\n",
    "        nome_padronizado = re.sub(r'Miqueias', 'Miquieias', nome_padronizado, flags=re.IGNORECASE)\n",
    "\n",
    "    return nome_padronizado\n",
    "\n",
    "# --- Processamento ---\n",
    "try:\n",
    "    os.makedirs(caminho_pasta_local, exist_ok=True)\n",
    "    logging.info(f\"Pasta de destino verificada/criada: {caminho_pasta_local}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erro ao verificar/criar pasta de destino: {e}\")\n",
    "    exit()\n",
    "\n",
    "dfs = []\n",
    "datas_vistas = set()\n",
    "\n",
    "logging.info(\"Iniciando processamento dos arquivos de ciclo...\")\n",
    "for i in range(1, 13):\n",
    "    nome_arquivo = f'Acompanhamento de separação Ciclo {i:02}-25.xlsm'\n",
    "    caminho_arquivo = os.path.join(pasta_drive, nome_arquivo)\n",
    "    logging.info(f\"Verificando arquivo: {nome_arquivo}...\")\n",
    "\n",
    "    if os.path.exists(caminho_arquivo):\n",
    "        try:\n",
    "            logging.info(f\"  -> Lendo aba 'Base de dados'...\")\n",
    "            df = pd.read_excel(caminho_arquivo, sheet_name=\"Base de dados\", engine='openpyxl')\n",
    "            logging.info(f\"  -> Leitura OK: {len(df)} linhas.\")\n",
    "\n",
    "            if 'Data' in df.columns:\n",
    "                df['Data'] = pd.to_datetime(df['Data'].astype(str), errors='coerce')\n",
    "                linhas_antes_nan = len(df)\n",
    "                df.dropna(subset=['Data'], inplace=True)\n",
    "                linhas_depois_nan = len(df)\n",
    "                if linhas_antes_nan > linhas_depois_nan:\n",
    "                    logging.warning(f\"  -> {linhas_antes_nan - linhas_depois_nan} linhas removidas por terem data inválida ou vazia.\")\n",
    "\n",
    "                if not df.empty:\n",
    "                    df_copia = df.copy()\n",
    "                    df_sem_repetidos = df_copia[~df_copia['Data'].isin(datas_vistas)]\n",
    "\n",
    "                    if not df_sem_repetidos.empty:\n",
    "                        datas_vistas.update(df_sem_repetidos['Data'].unique())\n",
    "                        logging.debug(f\"  -> Filtrando datas entre {data_inicio.date()} e {data_fim.date()}...\")\n",
    "                        df_filtrado = df_sem_repetidos[\n",
    "                            (df_sem_repetidos['Data'] >= data_inicio) &\n",
    "                            (df_sem_repetidos['Data'] <= data_fim)\n",
    "                        ].copy() # .copy() aqui é uma boa prática\n",
    "\n",
    "                        if not df_filtrado.empty:\n",
    "                            # -------> LOCAL CORRETO PARA PADRONIZAR A COLUNA 'Responsável' <-------\n",
    "                            nome_coluna_responsavel = 'Responsável' # Defina o nome exato da sua coluna aqui\n",
    "                            if nome_coluna_responsavel in df_filtrado.columns:\n",
    "                                logging.info(f\"  -> Padronizando coluna '{nome_coluna_responsavel}' no arquivo {nome_arquivo}...\")\n",
    "                                # Criar uma cópia para evitar SettingWithCopyWarning ao aplicar a função\n",
    "                                df_filtrado_para_modificar = df_filtrado.copy()\n",
    "                                df_filtrado_para_modificar[nome_coluna_responsavel] = df_filtrado_para_modificar[nome_coluna_responsavel].apply(padronizar_responsavel)\n",
    "                                df_filtrado = df_filtrado_para_modificar # Atribui o DataFrame modificado de volta\n",
    "                                logging.info(f\"  -> Coluna '{nome_coluna_responsavel}' padronizada.\")\n",
    "                            else:\n",
    "                                logging.warning(f\"  -> Coluna '{nome_coluna_responsavel}' não encontrada no arquivo {nome_arquivo}. Não será padronizada.\")\n",
    "                            # ---------------------------------------------------------------------\n",
    "\n",
    "                            dfs.append(df_filtrado)\n",
    "                            logging.info(f\"  -> ✔️ {len(df_filtrado)} linhas adicionadas de {nome_arquivo} (datas únicas no período).\")\n",
    "                        else:\n",
    "                            logging.info(f\"  -> Nenhuma linha de {nome_arquivo} está dentro do período após deduplicação.\")\n",
    "                    else:\n",
    "                        logging.info(f\"  -> Todas as datas válidas de {nome_arquivo} já foram vistas em arquivos anteriores.\")\n",
    "            else:\n",
    "                logging.warning(f\"  -> Coluna 'Data' não encontrada no arquivo {nome_arquivo}. Arquivo ignorado.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"  -> ❌ Erro ao ler ou processar o arquivo {nome_arquivo}: {e}\")\n",
    "            # import traceback # Descomente para depuração detalhada\n",
    "            # traceback.print_exc() # Descomente para depuração detalhada\n",
    "    else:\n",
    "        logging.warning(f\"  -> ⚠️ Arquivo não encontrado: {caminho_arquivo}\")\n",
    "\n",
    "logging.info(\"Processamento dos arquivos de ciclo concluído.\")\n",
    "if dfs:\n",
    "    logging.info(\"Concatenando resultados...\")\n",
    "    df_final = pd.concat(dfs, ignore_index=True)\n",
    "    logging.info(f\"🔄 DataFrames concatenados! Total de linhas final: {df_final.shape[0]}\")\n",
    "\n",
    "    if 'Data' in df_final.columns:\n",
    "        try:\n",
    "            logging.info(\"Formatando coluna 'Data' para AAAA-MM-DD no DataFrame final...\")\n",
    "            df_final['Data'] = pd.to_datetime(df_final['Data'], errors='coerce')\n",
    "            df_final['Data'] = df_final['Data'].dt.strftime('%Y-%m-%d')\n",
    "            df_final['Data'].fillna('', inplace=True)\n",
    "            logging.info(\"Coluna 'Data' formatada com sucesso.\")\n",
    "        except Exception as format_e:\n",
    "            logging.error(f\"Erro ao formatar a coluna 'Data' final: {format_e}\")\n",
    "            logging.warning(\"A coluna 'Data' pode conter a hora no arquivo salvo.\")\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Salvando arquivo final em: {caminho_arquivo_local}...\")\n",
    "        df_final.to_excel(caminho_arquivo_local, index=False, engine='openpyxl')\n",
    "        logging.info(f\"✅ Arquivo final salvo com sucesso!\")\n",
    "    except PermissionError:\n",
    "        logging.error(f\"!!! ERRO DE PERMISSÃO ao salvar Excel em '{caminho_arquivo_local}'. Verifique se o arquivo está aberto ou se há permissão de escrita.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"!!! ERRO ao salvar o arquivo Excel final: {e}\")\n",
    "else:\n",
    "    logging.warning(\"⚠️ Nenhum dado foi processado ou filtrado dos arquivos de ciclo. Nenhum arquivo final foi gerado.\")\n",
    "\n",
    "logging.info(\"--- FIM DO SCRIPT ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c17746f",
   "metadata": {},
   "source": [
    "### Enviar para o bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a968e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 10:34:53,569 - INFO - Período de filtro: 2024-12-28 a 2025-06-06\n",
      "2025-06-06 10:34:53,582 - INFO - Pasta de destino verificada/criada: G:\\Meu Drive\\Relatorio Oficial 2025\\Drive\\Pedidos Separados\n",
      "2025-06-06 10:34:53,587 - INFO - Iniciando processamento dos arquivos de ciclo...\n",
      "2025-06-06 10:34:53,590 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 01-25.xlsm...\n",
      "2025-06-06 10:34:53,597 - INFO -   -> Lendo aba 'Base de dados'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 10:34:54,687 - INFO -   -> Leitura OK: 470 linhas.\n",
      "2025-06-06 10:34:54,692 - INFO -   -> Nomes das colunas com codificação corrigida para o arquivo Acompanhamento de separação Ciclo 01-25.xlsm.\n",
      "2025-06-06 10:34:54,692 - INFO -   -> Nomes das colunas higienizados para BigQuery no arquivo Acompanhamento de separação Ciclo 01-25.xlsm.\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\2840626952.py:144: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  df[nome_coluna_data_bq] = pd.to_datetime(df[nome_coluna_data_bq].astype(str), errors='coerce', dayfirst=True)\n",
      "2025-06-06 10:34:54,710 - WARNING -   -> 206 linhas removidas por terem data inválida ou vazia na coluna 'data'.\n",
      "2025-06-06 10:34:54,717 - INFO -   -> Padronizando coluna 'responsavel' no arquivo Acompanhamento de separação Ciclo 01-25.xlsm...\n",
      "2025-06-06 10:34:54,724 - INFO -   -> Coluna 'responsavel' padronizada.\n",
      "2025-06-06 10:34:54,727 - INFO -   -> ✔️ 264 linhas adicionadas de Acompanhamento de separação Ciclo 01-25.xlsm (datas únicas no período).\n",
      "2025-06-06 10:34:54,727 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 02-25.xlsm...\n",
      "2025-06-06 10:34:54,736 - INFO -   -> Lendo aba 'Base de dados'...\n",
      "2025-06-06 10:34:57,268 - INFO -   -> Leitura OK: 362 linhas.\n",
      "2025-06-06 10:34:57,270 - INFO -   -> Nomes das colunas com codificação corrigida para o arquivo Acompanhamento de separação Ciclo 02-25.xlsm.\n",
      "2025-06-06 10:34:57,274 - INFO -   -> Nomes das colunas higienizados para BigQuery no arquivo Acompanhamento de separação Ciclo 02-25.xlsm.\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\2840626952.py:144: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  df[nome_coluna_data_bq] = pd.to_datetime(df[nome_coluna_data_bq].astype(str), errors='coerce', dayfirst=True)\n",
      "2025-06-06 10:34:57,291 - WARNING -   -> 15 linhas removidas por terem data inválida ou vazia na coluna 'data'.\n",
      "2025-06-06 10:34:57,299 - INFO -   -> Padronizando coluna 'responsavel' no arquivo Acompanhamento de separação Ciclo 02-25.xlsm...\n",
      "2025-06-06 10:34:57,306 - INFO -   -> Coluna 'responsavel' padronizada.\n",
      "2025-06-06 10:34:57,307 - INFO -   -> ✔️ 286 linhas adicionadas de Acompanhamento de separação Ciclo 02-25.xlsm (datas únicas no período).\n",
      "2025-06-06 10:34:57,309 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 03-25.xlsm...\n",
      "2025-06-06 10:34:57,315 - INFO -   -> Lendo aba 'Base de dados'...\n",
      "2025-06-06 10:34:58,841 - INFO -   -> Leitura OK: 532 linhas.\n",
      "2025-06-06 10:34:58,845 - INFO -   -> Nomes das colunas com codificação corrigida para o arquivo Acompanhamento de separação Ciclo 03-25.xlsm.\n",
      "2025-06-06 10:34:58,847 - INFO -   -> Nomes das colunas higienizados para BigQuery no arquivo Acompanhamento de separação Ciclo 03-25.xlsm.\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\2840626952.py:144: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  df[nome_coluna_data_bq] = pd.to_datetime(df[nome_coluna_data_bq].astype(str), errors='coerce', dayfirst=True)\n",
      "2025-06-06 10:34:58,865 - INFO -   -> Padronizando coluna 'responsavel' no arquivo Acompanhamento de separação Ciclo 03-25.xlsm...\n",
      "2025-06-06 10:34:58,869 - INFO -   -> Coluna 'responsavel' padronizada.\n",
      "2025-06-06 10:34:58,871 - INFO -   -> ✔️ 182 linhas adicionadas de Acompanhamento de separação Ciclo 03-25.xlsm (datas únicas no período).\n",
      "2025-06-06 10:34:58,873 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 04-25.xlsm...\n",
      "2025-06-06 10:34:58,877 - INFO -   -> Lendo aba 'Base de dados'...\n",
      "2025-06-06 10:35:00,584 - INFO -   -> Leitura OK: 716 linhas.\n",
      "2025-06-06 10:35:00,587 - INFO -   -> Nomes das colunas com codificação corrigida para o arquivo Acompanhamento de separação Ciclo 04-25.xlsm.\n",
      "2025-06-06 10:35:00,587 - INFO -   -> Nomes das colunas higienizados para BigQuery no arquivo Acompanhamento de separação Ciclo 04-25.xlsm.\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\2840626952.py:144: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  df[nome_coluna_data_bq] = pd.to_datetime(df[nome_coluna_data_bq].astype(str), errors='coerce', dayfirst=True)\n",
      "2025-06-06 10:35:00,603 - WARNING -   -> 2 linhas removidas por terem data inválida ou vazia na coluna 'data'.\n",
      "2025-06-06 10:35:00,611 - INFO -   -> Padronizando coluna 'responsavel' no arquivo Acompanhamento de separação Ciclo 04-25.xlsm...\n",
      "2025-06-06 10:35:00,617 - INFO -   -> Coluna 'responsavel' padronizada.\n",
      "2025-06-06 10:35:00,619 - INFO -   -> ✔️ 187 linhas adicionadas de Acompanhamento de separação Ciclo 04-25.xlsm (datas únicas no período).\n",
      "2025-06-06 10:35:00,621 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 05-25.xlsm...\n",
      "2025-06-06 10:35:00,629 - INFO -   -> Lendo aba 'Base de dados'...\n",
      "2025-06-06 10:35:13,827 - INFO -   -> Leitura OK: 1048519 linhas.\n",
      "2025-06-06 10:35:13,827 - INFO -   -> Nomes das colunas com codificação corrigida para o arquivo Acompanhamento de separação Ciclo 05-25.xlsm.\n",
      "2025-06-06 10:35:13,834 - INFO -   -> Nomes das colunas higienizados para BigQuery no arquivo Acompanhamento de separação Ciclo 05-25.xlsm.\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\2840626952.py:144: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  df[nome_coluna_data_bq] = pd.to_datetime(df[nome_coluna_data_bq].astype(str), errors='coerce', dayfirst=True)\n",
      "2025-06-06 10:35:14,321 - WARNING -   -> 1048032 linhas removidas por terem data inválida ou vazia na coluna 'data'.\n",
      "2025-06-06 10:35:14,334 - INFO -   -> Padronizando coluna 'responsavel' no arquivo Acompanhamento de separação Ciclo 05-25.xlsm...\n",
      "2025-06-06 10:35:14,340 - INFO -   -> Coluna 'responsavel' padronizada.\n",
      "2025-06-06 10:35:14,344 - INFO -   -> ✔️ 450 linhas adicionadas de Acompanhamento de separação Ciclo 05-25.xlsm (datas únicas no período).\n",
      "2025-06-06 10:35:14,347 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 06-25.xlsm...\n",
      "2025-06-06 10:35:14,352 - INFO -   -> Lendo aba 'Base de dados'...\n",
      "2025-06-06 10:35:27,907 - INFO -   -> Leitura OK: 1048324 linhas.\n",
      "2025-06-06 10:35:27,912 - INFO -   -> Nomes das colunas com codificação corrigida para o arquivo Acompanhamento de separação Ciclo 06-25.xlsm.\n",
      "2025-06-06 10:35:27,915 - INFO -   -> Nomes das colunas higienizados para BigQuery no arquivo Acompanhamento de separação Ciclo 06-25.xlsm.\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\2840626952.py:144: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  df[nome_coluna_data_bq] = pd.to_datetime(df[nome_coluna_data_bq].astype(str), errors='coerce', dayfirst=True)\n",
      "2025-06-06 10:35:28,414 - WARNING -   -> 1048032 linhas removidas por terem data inválida ou vazia na coluna 'data'.\n",
      "2025-06-06 10:35:28,419 - INFO -   -> Todas as datas válidas de Acompanhamento de separação Ciclo 06-25.xlsm (coluna 'data') já foram vistas em arquivos anteriores.\n",
      "2025-06-06 10:35:28,424 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 07-25.xlsm...\n",
      "2025-06-06 10:35:28,428 - INFO -   -> Lendo aba 'Base de dados'...\n",
      "2025-06-06 10:35:39,903 - INFO -   -> Leitura OK: 1047968 linhas.\n",
      "2025-06-06 10:35:39,904 - INFO -   -> Nomes das colunas com codificação corrigida para o arquivo Acompanhamento de separação Ciclo 07-25.xlsm.\n",
      "2025-06-06 10:35:39,908 - INFO -   -> Nomes das colunas higienizados para BigQuery no arquivo Acompanhamento de separação Ciclo 07-25.xlsm.\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\2840626952.py:144: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  df[nome_coluna_data_bq] = pd.to_datetime(df[nome_coluna_data_bq].astype(str), errors='coerce', dayfirst=True)\n",
      "2025-06-06 10:35:40,262 - WARNING -   -> 1047794 linhas removidas por terem data inválida ou vazia na coluna 'data'.\n",
      "2025-06-06 10:35:40,274 - INFO -   -> Padronizando coluna 'responsavel' no arquivo Acompanhamento de separação Ciclo 07-25.xlsm...\n",
      "2025-06-06 10:35:40,278 - INFO -   -> Coluna 'responsavel' padronizada.\n",
      "2025-06-06 10:35:40,278 - INFO -   -> ✔️ 133 linhas adicionadas de Acompanhamento de separação Ciclo 07-25.xlsm (datas únicas no período).\n",
      "2025-06-06 10:35:40,282 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 08-25.xlsm...\n",
      "2025-06-06 10:35:40,284 - INFO -   -> Lendo aba 'Base de dados'...\n",
      "2025-06-06 10:35:53,025 - INFO -   -> Leitura OK: 1047792 linhas.\n",
      "2025-06-06 10:35:53,028 - INFO -   -> Nomes das colunas com codificação corrigida para o arquivo Acompanhamento de separação Ciclo 08-25.xlsm.\n",
      "2025-06-06 10:35:53,032 - INFO -   -> Nomes das colunas higienizados para BigQuery no arquivo Acompanhamento de separação Ciclo 08-25.xlsm.\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\2840626952.py:144: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  df[nome_coluna_data_bq] = pd.to_datetime(df[nome_coluna_data_bq].astype(str), errors='coerce', dayfirst=True)\n",
      "2025-06-06 10:35:53,453 - WARNING -   -> 1047675 linhas removidas por terem data inválida ou vazia na coluna 'data'.\n",
      "2025-06-06 10:35:53,464 - INFO -   -> Padronizando coluna 'responsavel' no arquivo Acompanhamento de separação Ciclo 08-25.xlsm...\n",
      "2025-06-06 10:35:53,467 - INFO -   -> Coluna 'responsavel' padronizada.\n",
      "2025-06-06 10:35:53,467 - INFO -   -> ✔️ 69 linhas adicionadas de Acompanhamento de separação Ciclo 08-25.xlsm (datas únicas no período).\n",
      "2025-06-06 10:35:53,472 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 09-25.xlsm...\n",
      "2025-06-06 10:35:53,477 - WARNING -   -> ⚠️ Arquivo não encontrado: G:.shortcut-targets-by-id\\1NpLOQC_eCJ4vFpBiP54a5AhOorhBC0kZ\\Logística\\Acompanhamento separação\\Acompanhamento de separação Ciclo 09-25.xlsm\n",
      "2025-06-06 10:35:53,477 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 10-25.xlsm...\n",
      "2025-06-06 10:35:53,484 - WARNING -   -> ⚠️ Arquivo não encontrado: G:.shortcut-targets-by-id\\1NpLOQC_eCJ4vFpBiP54a5AhOorhBC0kZ\\Logística\\Acompanhamento separação\\Acompanhamento de separação Ciclo 10-25.xlsm\n",
      "2025-06-06 10:35:53,486 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 11-25.xlsm...\n",
      "2025-06-06 10:35:53,488 - WARNING -   -> ⚠️ Arquivo não encontrado: G:.shortcut-targets-by-id\\1NpLOQC_eCJ4vFpBiP54a5AhOorhBC0kZ\\Logística\\Acompanhamento separação\\Acompanhamento de separação Ciclo 11-25.xlsm\n",
      "2025-06-06 10:35:53,490 - INFO - Verificando arquivo: Acompanhamento de separação Ciclo 12-25.xlsm...\n",
      "2025-06-06 10:35:53,494 - WARNING -   -> ⚠️ Arquivo não encontrado: G:.shortcut-targets-by-id\\1NpLOQC_eCJ4vFpBiP54a5AhOorhBC0kZ\\Logística\\Acompanhamento separação\\Acompanhamento de separação Ciclo 12-25.xlsm\n",
      "2025-06-06 10:35:53,496 - INFO - Processamento dos arquivos de ciclo concluído.\n",
      "2025-06-06 10:35:53,499 - INFO - Concatenando resultados...\n",
      "2025-06-06 10:35:53,618 - INFO - 🔄 DataFrames concatenados! Total de linhas final: 1571\n",
      "2025-06-06 10:35:53,618 - INFO - Formatando coluna 'data' para YYYY-MM-DD no DataFrame final...\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_15748\\2840626952.py:200: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_final[nome_coluna_data_bq].fillna('', inplace=True)\n",
      "2025-06-06 10:35:53,636 - INFO - Coluna 'data' formatada com sucesso para YYYY-MM-DD.\n",
      "2025-06-06 10:35:53,636 - INFO - Salvando arquivo final em formato CSV: G:\\Meu Drive\\Relatorio Oficial 2025\\Drive\\Pedidos Separados\\Acompanhamento_separação_completo_formatado.csv...\n",
      "2025-06-06 10:35:53,740 - INFO - ✅ Arquivo CSV final salvo com sucesso! Nomes de colunas estão higienizados para BigQuery.\n",
      "2025-06-06 10:35:53,742 - INFO - --- FIM DO SCRIPT ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import re\n",
    "import unicodedata # Importado para remover acentos\n",
    "\n",
    "# --- Configurações ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "caminho_pasta_local = r\"G:\\Meu Drive\\Relatorio Oficial 2025\\Drive\\Pedidos Separados\"\n",
    "nome_arquivo_final = \"Acompanhamento_separação_completo_formatado.csv\"\n",
    "caminho_arquivo_local = os.path.join(caminho_pasta_local, nome_arquivo_final)\n",
    "\n",
    "pasta_drive = r\"G:.shortcut-targets-by-id\\1NpLOQC_eCJ4vFpBiP54a5AhOorhBC0kZ\\Logística\\Acompanhamento separação\"\n",
    "\n",
    "data_inicio = datetime(2024, 12, 28)\n",
    "data_fim = datetime.today().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "logging.info(f\"Período de filtro: {data_inicio.strftime('%Y-%m-%d')} a {data_fim.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "def padronizar_responsavel(nome_responsavel):\n",
    "    if pd.isna(nome_responsavel) or not isinstance(nome_responsavel, str) or nome_responsavel.strip() == '':\n",
    "        return ''\n",
    "    nome_processado = str(nome_responsavel).strip()\n",
    "    if 'deigo' in nome_processado.lower():\n",
    "        nome_processado = re.sub(r'deigo', 'Diego', nome_processado, flags=re.IGNORECASE)\n",
    "    if 'miqueias' in nome_processado.lower():\n",
    "        nome_processado = re.sub(r'miqueias', 'Miquieias', nome_processado, flags=re.IGNORECASE)\n",
    "    nome_padronizado = ' '.join(word.capitalize() for word in nome_processado.lower().split())\n",
    "    if 'Diego' in nome_padronizado and 'deigo' in str(nome_responsavel).strip().lower():\n",
    "        nome_padronizado = re.sub(r'Deigo', 'Diego', nome_padronizado, flags=re.IGNORECASE)\n",
    "    if 'Miquieias' in nome_padronizado and 'miqueias' in str(nome_responsavel).strip().lower():\n",
    "        nome_padronizado = re.sub(r'Miqueias', 'Miquieias', nome_padronizado, flags=re.IGNORECASE)\n",
    "    return nome_padronizado\n",
    "\n",
    "def corrigir_nome_coluna(nome_coluna):\n",
    "    mapa_nomes_completos = {\n",
    "        'ResponsÃ¡vel': 'Responsável', 'RegiÃ£o': 'Região',\n",
    "        'NÂ° de Pedidos': 'N° de Pedidos', 'NÂ° de Itens': 'N° de Itens',\n",
    "        'InÃ­cio Picking': 'Início Picking', 'Fim Picking': 'Fim Picking',\n",
    "        'PrevisÃ£o de tÃ©rmino': 'Previsão de término',\n",
    "        'Tempo mÃ©dio por item': 'Tempo médio por item',\n",
    "        'Tempo mÃ©dio por Pedido': 'Tempo médio por Pedido',\n",
    "        'HorÃ¡rio de almoÃ§o': 'Horário de almoço'\n",
    "    }\n",
    "    if nome_coluna in mapa_nomes_completos:\n",
    "        return mapa_nomes_completos[nome_coluna]\n",
    "    nome_corrigido = nome_coluna\n",
    "    substituicoes_caracteres = {\n",
    "        'NÂ°': 'N°', 'Ã¡': 'á', 'Ã©': 'é', 'Ã­': 'í', 'Ã³': 'ó', 'Ãº': 'ú',\n",
    "        'Ã‚': 'Â', 'ÃŠ': 'Ê', 'Ã”': 'Ô', 'Ã¢': 'â', 'Ãª': 'ê', 'Ã´': 'ô',\n",
    "        'Ã£': 'ã', 'Ãµ': 'õ', 'Ã§': 'ç', 'Â°': '°', 'Âº': 'º', 'Âª': 'ª',\n",
    "        'Ãü': 'ü', 'Ã': 'Á', 'Ã‰': 'É', 'Ã': 'Í', 'Ã“': 'Ó', 'Ãš': 'Ú',\n",
    "        'Ãƒ': 'Ã', 'Ã•': 'Õ', 'Ã‡': 'Ç', 'Ãœ': 'Ü'\n",
    "    }\n",
    "    for problematica, correta in substituicoes_caracteres.items():\n",
    "        nome_corrigido = nome_corrigido.replace(problematica, correta)\n",
    "    return nome_corrigido\n",
    "\n",
    "# NOVA FUNÇÃO para remover acentos\n",
    "def remover_acentos(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        return texto\n",
    "    nfkd_form = unicodedata.normalize('NFKD', texto)\n",
    "    return \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "# NOVA FUNÇÃO para higienizar nomes para o BigQuery\n",
    "def higienizar_nome_para_bigquery(nome_coluna):\n",
    "    if not isinstance(nome_coluna, str) or not nome_coluna.strip():\n",
    "        # Para nomes de coluna inválidos ou vazios, cria um placeholder\n",
    "        return f\"coluna_invalida_{abs(hash(str(nome_coluna))) % 10000}\"\n",
    "\n",
    "    # 1. Remove acentos (ex: \"Região\" -> \"Regiao\")\n",
    "    nome_sem_acentos = remover_acentos(nome_coluna)\n",
    "    \n",
    "    processado = nome_sem_acentos\n",
    "    # 2. Substituições específicas (ex: \"N° de\" -> \"Num_de\", \"N°\" -> \"No\")\n",
    "    processado = processado.replace('N° de', 'Num_de') \n",
    "    processado = processado.replace('N°', 'No')      \n",
    "    processado = processado.replace('°', 'o') # Para qualquer '°' restante\n",
    "\n",
    "    # 3. Substitui qualquer sequência de caracteres que NÃO seja letra ou número por um único underscore\n",
    "    processado = re.sub(r'[^a-zA-Z0-9_]+', '_', processado) # Underscore já é permitido, então incluímos na negação\n",
    "\n",
    "    # 4. Remove underscores no início ou no fim\n",
    "    processado = processado.strip('_')\n",
    "    \n",
    "    # 5. Se o nome ficou vazio após as transformações (ex: nome original era \"---\")\n",
    "    if not processado:\n",
    "        nome_base_original = re.sub(r'[^a-zA-Z0-9]', '', nome_coluna) # Usa o nome original da coluna\n",
    "        if nome_base_original:\n",
    "            processado = 'col_' + nome_base_original[:20].lower() # Cria um nome a partir do original\n",
    "        else: # Se o original também não tinha letras/números\n",
    "            processado = f'col_gen_{abs(hash(nome_coluna)) % 10000}'\n",
    "\n",
    "    # 6. Garante que não comece com um número (BigQuery exige começar com letra ou underscore)\n",
    "    if processado and processado[0].isdigit(): # Verifica se processado não é vazio antes de indexar\n",
    "        processado = '_' + processado\n",
    "        \n",
    "    return processado.lower() # Padroniza para minúsculas para consistência\n",
    "\n",
    "# --- Processamento ---\n",
    "try:\n",
    "    os.makedirs(caminho_pasta_local, exist_ok=True)\n",
    "    logging.info(f\"Pasta de destino verificada/criada: {caminho_pasta_local}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erro ao verificar/criar pasta de destino: {e}\")\n",
    "    exit()\n",
    "\n",
    "dfs = []\n",
    "datas_vistas = set()\n",
    "\n",
    "# Nome original da coluna 'Responsável' (após correção de codificação, antes da higienização BQ)\n",
    "nome_coluna_responsavel_corrigido = 'Responsável'\n",
    "# Obter o nome higienizado para usar nas buscas e manipulações\n",
    "nome_coluna_responsavel_bq = higienizar_nome_para_bigquery(nome_coluna_responsavel_corrigido)\n",
    "\n",
    "# Nome original da coluna de Data (geralmente não muda com higienização se for só 'Data')\n",
    "nome_coluna_data_corrigido = 'Data'\n",
    "nome_coluna_data_bq = higienizar_nome_para_bigquery(nome_coluna_data_corrigido)\n",
    "\n",
    "\n",
    "logging.info(\"Iniciando processamento dos arquivos de ciclo...\")\n",
    "for i in range(1, 13):\n",
    "    nome_arquivo = f'Acompanhamento de separação Ciclo {i:02}-25.xlsm'\n",
    "    caminho_arquivo = os.path.join(pasta_drive, nome_arquivo)\n",
    "    logging.info(f\"Verificando arquivo: {nome_arquivo}...\")\n",
    "\n",
    "    if os.path.exists(caminho_arquivo):\n",
    "        try:\n",
    "            logging.info(f\"  -> Lendo aba 'Base de dados'...\")\n",
    "            df = pd.read_excel(caminho_arquivo, sheet_name=\"Base de dados\", engine='openpyxl')\n",
    "            logging.info(f\"  -> Leitura OK: {len(df)} linhas.\")\n",
    "\n",
    "            # Etapa 1: Corrigir problemas de codificação nos nomes das colunas\n",
    "            df.columns = [corrigir_nome_coluna(col) for col in df.columns]\n",
    "            logging.info(f\"  -> Nomes das colunas com codificação corrigida para o arquivo {nome_arquivo}.\")\n",
    "            \n",
    "            # Etapa 2: Higienizar nomes das colunas para compatibilidade com BigQuery\n",
    "            df.columns = [higienizar_nome_para_bigquery(col) for col in df.columns]\n",
    "            logging.info(f\"  -> Nomes das colunas higienizados para BigQuery no arquivo {nome_arquivo}.\")\n",
    "\n",
    "            if nome_coluna_data_bq in df.columns:\n",
    "                df[nome_coluna_data_bq] = pd.to_datetime(df[nome_coluna_data_bq].astype(str), errors='coerce', dayfirst=True)\n",
    "                linhas_antes_nan = len(df)\n",
    "                df.dropna(subset=[nome_coluna_data_bq], inplace=True)\n",
    "                linhas_depois_nan = len(df)\n",
    "                if linhas_antes_nan > linhas_depois_nan:\n",
    "                    logging.warning(f\"  -> {linhas_antes_nan - linhas_depois_nan} linhas removidas por terem data inválida ou vazia na coluna '{nome_coluna_data_bq}'.\")\n",
    "\n",
    "                if not df.empty:\n",
    "                    df_copia = df.copy()\n",
    "                    # Usar o nome da coluna de data higienizado para a deduplicação\n",
    "                    df_sem_repetidos = df_copia[~df_copia[nome_coluna_data_bq].isin(datas_vistas)]\n",
    "\n",
    "                    if not df_sem_repetidos.empty:\n",
    "                        datas_vistas.update(df_sem_repetidos[nome_coluna_data_bq].unique())\n",
    "                        df_filtrado = df_sem_repetidos[\n",
    "                            (df_sem_repetidos[nome_coluna_data_bq] >= data_inicio) &\n",
    "                            (df_sem_repetidos[nome_coluna_data_bq] <= data_fim)\n",
    "                        ].copy()\n",
    "\n",
    "                        if not df_filtrado.empty:\n",
    "                            if nome_coluna_responsavel_bq in df_filtrado.columns:\n",
    "                                logging.info(f\"  -> Padronizando coluna '{nome_coluna_responsavel_bq}' no arquivo {nome_arquivo}...\")\n",
    "                                df_filtrado_para_modificar = df_filtrado.copy() # Evitar SettingWithCopyWarning\n",
    "                                df_filtrado_para_modificar[nome_coluna_responsavel_bq] = df_filtrado_para_modificar[nome_coluna_responsavel_bq].apply(padronizar_responsavel)\n",
    "                                df_filtrado = df_filtrado_para_modificar\n",
    "                                logging.info(f\"  -> Coluna '{nome_coluna_responsavel_bq}' padronizada.\")\n",
    "                            else:\n",
    "                                logging.warning(f\"  -> Coluna '{nome_coluna_responsavel_bq}' não encontrada no arquivo {nome_arquivo} após higienização. Não será padronizada.\")\n",
    "                            \n",
    "                            dfs.append(df_filtrado)\n",
    "                            logging.info(f\"  -> ✔️ {len(df_filtrado)} linhas adicionadas de {nome_arquivo} (datas únicas no período).\")\n",
    "                        else:\n",
    "                            logging.info(f\"  -> Nenhuma linha de {nome_arquivo} está dentro do período após deduplicação.\")\n",
    "                    else:\n",
    "                        logging.info(f\"  -> Todas as datas válidas de {nome_arquivo} (coluna '{nome_coluna_data_bq}') já foram vistas em arquivos anteriores.\")\n",
    "            else:\n",
    "                logging.warning(f\"  -> Coluna de data '{nome_coluna_data_bq}' não encontrada no arquivo {nome_arquivo} (após higienização). Arquivo ignorado.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"  -> ❌ Erro ao ler ou processar o arquivo {nome_arquivo}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc() # Para depuração mais detalhada do erro\n",
    "    else:\n",
    "        logging.warning(f\"  -> ⚠️ Arquivo não encontrado: {caminho_arquivo}\")\n",
    "\n",
    "logging.info(\"Processamento dos arquivos de ciclo concluído.\")\n",
    "if dfs:\n",
    "    logging.info(\"Concatenando resultados...\")\n",
    "    df_final = pd.concat(dfs, ignore_index=True)\n",
    "    logging.info(f\"🔄 DataFrames concatenados! Total de linhas final: {df_final.shape[0]}\")\n",
    "\n",
    "    # Usar o nome da coluna de data higienizado para a formatação final\n",
    "    if nome_coluna_data_bq in df_final.columns:\n",
    "        try:\n",
    "            logging.info(f\"Formatando coluna '{nome_coluna_data_bq}' para YYYY-MM-DD no DataFrame final...\")\n",
    "            df_final[nome_coluna_data_bq] = pd.to_datetime(df_final[nome_coluna_data_bq], errors='coerce')\n",
    "            df_final[nome_coluna_data_bq] = df_final[nome_coluna_data_bq].dt.strftime('%Y-%m-%d')\n",
    "            df_final[nome_coluna_data_bq].fillna('', inplace=True)\n",
    "            logging.info(f\"Coluna '{nome_coluna_data_bq}' formatada com sucesso para YYYY-MM-DD.\")\n",
    "        except Exception as format_e:\n",
    "            logging.error(f\"Erro ao formatar a coluna '{nome_coluna_data_bq}' final: {format_e}\")\n",
    "            logging.warning(f\"A coluna '{nome_coluna_data_bq}' pode não estar no formato YYYY-MM-DD no arquivo CSV salvo.\")\n",
    "    else:\n",
    "        logging.warning(f\"Coluna de data '{nome_coluna_data_bq}' não encontrada no DataFrame final para formatação.\")\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Salvando arquivo final em formato CSV: {caminho_arquivo_local}...\")\n",
    "        df_final.to_csv(caminho_arquivo_local, index=False, sep=',', encoding='utf-8')\n",
    "        logging.info(f\"✅ Arquivo CSV final salvo com sucesso! Nomes de colunas estão higienizados para BigQuery.\")\n",
    "    except PermissionError:\n",
    "        logging.error(f\"!!! ERRO DE PERMISSÃO ao salvar CSV em '{caminho_arquivo_local}'. Verifique se o arquivo está aberto ou se há permissão de escrita.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"!!! ERRO ao salvar o arquivo CSV final: {e}\")\n",
    "else:\n",
    "    logging.warning(\"⚠️ Nenhum dado foi processado ou filtrado dos arquivos de ciclo. Nenhum arquivo final foi gerado.\")\n",
    "\n",
    "logging.info(\"--- FIM DO SCRIPT ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90835811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 10:35:53,809 - INFO - -----------------------------------------------------\n",
      "2025-06-06 10:35:53,811 - INFO - Iniciando envio do CSV 'Acompanhamento_separação_completo_formatado.csv' para BigQuery 'VDHUB.separacao'.\n",
      "2025-06-06 10:35:53,816 - INFO - Coluna de data 'data' será enviada como STRING (como está no CSV).\n",
      "2025-06-06 10:35:53,817 - INFO - Colunas ['num_de_pedidos', 'num_de_itens'] serão convertidas para numérico.\n",
      "2025-06-06 10:35:53,821 - INFO - Outras colunas serão enviadas como STRING.\n",
      "2025-06-06 10:35:53,821 - INFO - A tabela no BigQuery será SUBSTITUÍDA.\n",
      "2025-06-06 10:35:53,825 - INFO - -----------------------------------------------------\n",
      "2025-06-06 10:35:53,967 - INFO - [OK] Credenciais carregadas: C:\\Users\\Florestas\\Desktop\\grupo-florestas-429613-080e980a456d.json\n",
      "2025-06-06 10:35:53,975 - INFO - Lendo arquivo CSV: Acompanhamento_separação_completo_formatado.csv...\n",
      "2025-06-06 10:35:54,035 - INFO - Leitura do CSV OK: 1571 linhas.\n",
      "2025-06-06 10:35:54,044 - INFO - Limpando nomes das colunas para o BigQuery...\n",
      "2025-06-06 10:35:54,044 - INFO - Nomes das colunas limpos (ex: ['data', 'responsavel', 'regiao', 'tipo', 'num_de_pedidos']...).\n",
      "2025-06-06 10:35:54,050 - INFO - Coluna de data principal 'data' será mantida como STRING (como lida do CSV).\n",
      "2025-06-06 10:35:54,051 - INFO - Convertendo colunas ['num_de_pedidos', 'num_de_itens'] para tipo numérico...\n",
      "2025-06-06 10:35:54,058 - WARNING - Coluna 'num_de_pedidos': 5 valores não convertidos para número e serão NULOS.\n",
      "2025-06-06 10:35:54,060 - INFO - Coluna 'num_de_pedidos' convertida para numérico.\n",
      "2025-06-06 10:35:54,068 - WARNING - Coluna 'num_de_itens': 5 valores não convertidos para número e serão NULOS.\n",
      "2025-06-06 10:35:54,072 - INFO - Coluna 'num_de_itens' convertida para numérico.\n",
      "2025-06-06 10:35:54,074 - INFO - Schema definido para o BigQuery (coluna de data principal como STRING): [{'name': 'data', 'type': 'STRING'}, {'name': 'responsavel', 'type': 'STRING'}, {'name': 'regiao', 'type': 'STRING'}, {'name': 'tipo', 'type': 'STRING'}, {'name': 'num_de_pedidos', 'type': 'INTEGER'}, {'name': 'num_de_itens', 'type': 'INTEGER'}, {'name': 'inicio_picking', 'type': 'STRING'}, {'name': 'fim_picking', 'type': 'STRING'}, {'name': 'previsao_de_termino', 'type': 'STRING'}, {'name': 'tempo_medio_por_item', 'type': 'STRING'}, {'name': 'tempo_medio_por_pedido', 'type': 'STRING'}, {'name': 'meta_picking', 'type': 'STRING'}, {'name': 'tempo_real_de_picking', 'type': 'STRING'}, {'name': 'velocidade', 'type': 'STRING'}, {'name': 'resultado', 'type': 'STRING'}, {'name': 'horario_de_almoco', 'type': 'STRING'}, {'name': 'unnamed_16', 'type': 'STRING'}]\n",
      "2025-06-06 10:35:54,076 - INFO - Preparação do DataFrame e schema para envio ao BigQuery concluída.\n",
      "2025-06-06 10:35:54,078 - INFO - Enviando 1571 linhas para BigQuery: 'grupo-florestas-429613:VDHUB.separacao' (Substituindo)...\n",
      "1571 out of 1571 rows loaded., ?it/s]2025-06-06 10:35:57,779 - INFO - \n",
      "100%|██████████| 1/1 [00:00<00:00, 556.64it/s]\n",
      "2025-06-06 10:35:57,785 - INFO - ✅ SUCESSO! Dados enviados e tabela 'VDHUB.separacao' foi substituída no BigQuery.\n",
      "2025-06-06 10:35:57,785 - INFO - Verifique o esquema da tabela no BigQuery: 'data' deve ser STRING, ['num_de_pedidos', 'num_de_itens'] devem ser INTEGER.\n",
      "2025-06-06 10:35:57,787 - INFO - --- FIM DO SCRIPT DE ENVIO PARA BIGQUERY (VDHUB.separacao) ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata # Para a função limpar_nome_coluna_bq\n",
    "import re          # Para a função limpar_nome_coluna_bq\n",
    "import logging\n",
    "from google.oauth2 import service_account # Para carregar as credenciais do BigQuery\n",
    "import pandas_gbq # Para usar pandas_gbq.to_gbq()\n",
    "\n",
    "# --- Configuração do Logging ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Caminhos e Nomes ---\n",
    "caminho_pasta_dados_separacao = r\"G:\\Meu Drive\\Relatorio Oficial 2025\\Drive\\Pedidos Separados\"\n",
    "nome_arquivo_csv_separacao = \"Acompanhamento_separação_completo_formatado.csv\"\n",
    "caminho_completo_csv_separacao = os.path.join(caminho_pasta_dados_separacao, nome_arquivo_csv_separacao)\n",
    "\n",
    "# --- CONFIGURAÇÕES DO BIGQUERY ---\n",
    "PROJECT_ID = 'grupo-florestas-429613'\n",
    "DATASET_ID = 'VDHUB'\n",
    "TABLE_NAME_SEPARACAO = 'separacao'\n",
    "DESTINATION_TABLE_SEPARACAO = f'{DATASET_ID}.{TABLE_NAME_SEPARACAO}'\n",
    "ARQUIVO_CREDENCIAL_JSON = r'C:\\Users\\Florestas\\Desktop\\grupo-florestas-429613-080e980a456d.json'\n",
    "\n",
    "# --- Nomes das Colunas para Tratamento Específico (APÓS LIMPEZA DE NOMES) ---\n",
    "# Seu script de preparação higieniza 'Data' para 'data'.\n",
    "NOME_COLUNA_DATA_PRINCIPAL = 'data'\n",
    "COLUNAS_NUMERICAS = ['num_de_pedidos', 'num_de_itens'] # Estas serão convertidas para número\n",
    "\n",
    "# --- Funções Auxiliares ---\n",
    "def limpar_nome_coluna_bq_final_check(col_name):\n",
    "    if not isinstance(col_name, str): col_name = str(col_name)\n",
    "    nfkd_form = unicodedata.normalize('NFKD', col_name)\n",
    "    ascii_name = \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "    lower_name = ascii_name.lower()\n",
    "    cleaned_name = re.sub(r'\\s+', '_', lower_name)\n",
    "    cleaned_name = re.sub(r'[^\\w_]+', '', cleaned_name)\n",
    "    if cleaned_name and cleaned_name[0].isdigit():\n",
    "        cleaned_name = '_' + cleaned_name\n",
    "    if not cleaned_name:\n",
    "        cleaned_name = f'col_gerada_{abs(hash(col_name))%10000}'\n",
    "    return cleaned_name\n",
    "\n",
    "# --- Execução Principal ---\n",
    "logging.info(\"-----------------------------------------------------\")\n",
    "logging.info(f\"Iniciando envio do CSV '{nome_arquivo_csv_separacao}' para BigQuery '{DESTINATION_TABLE_SEPARACAO}'.\")\n",
    "logging.info(f\"Coluna de data '{NOME_COLUNA_DATA_PRINCIPAL}' será enviada como STRING (como está no CSV).\")\n",
    "logging.info(f\"Colunas {COLUNAS_NUMERICAS} serão convertidas para numérico.\")\n",
    "logging.info(\"Outras colunas serão enviadas como STRING.\")\n",
    "logging.info(\"A tabela no BigQuery será SUBSTITUÍDA.\")\n",
    "logging.info(\"-----------------------------------------------------\")\n",
    "\n",
    "# 1. Autenticar\n",
    "if not os.path.exists(ARQUIVO_CREDENCIAL_JSON):\n",
    "    logging.error(f\"!!! ERRO FATAL: Arquivo de Credenciais JSON não encontrado: '{ARQUIVO_CREDENCIAL_JSON}'.\")\n",
    "    exit()\n",
    "try:\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        ARQUIVO_CREDENCIAL_JSON,\n",
    "        scopes=[\"https://www.googleapis.com/auth/bigquery\"]\n",
    "    )\n",
    "    logging.info(f\"[OK] Credenciais carregadas: {ARQUIVO_CREDENCIAL_JSON}\")\n",
    "except Exception as auth_err:\n",
    "    logging.error(f\"!!! ERRO FATAL: Falha ao carregar credenciais: {auth_err}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Verifica se o arquivo CSV de origem existe\n",
    "if not os.path.exists(caminho_completo_csv_separacao):\n",
    "    logging.error(f\"!!! ERRO FATAL: Arquivo CSV de origem não encontrado: '{caminho_completo_csv_separacao}'.\")\n",
    "    exit()\n",
    "\n",
    "# 3. Lê o CSV e Prepara o DataFrame\n",
    "df_bq_separacao = None\n",
    "try:\n",
    "    logging.info(f\"Lendo arquivo CSV: {nome_arquivo_csv_separacao}...\")\n",
    "    df_bq_separacao = pd.read_csv(caminho_completo_csv_separacao, dtype=str, keep_default_na=False, na_values=[''])\n",
    "\n",
    "    if df_bq_separacao.empty:\n",
    "        raise ValueError(\"O arquivo CSV está vazio.\")\n",
    "    logging.info(f\"Leitura do CSV OK: {len(df_bq_separacao)} linhas.\")\n",
    "\n",
    "    logging.info(\"Limpando nomes das colunas para o BigQuery...\")\n",
    "    df_bq_separacao.columns = [limpar_nome_coluna_bq_final_check(col) for col in df_bq_separacao.columns]\n",
    "    logging.info(f\"Nomes das colunas limpos (ex: {list(df_bq_separacao.columns[:5])}...).\")\n",
    "\n",
    "    # NÃO convertemos a coluna de data principal para datetime, ela permanecerá como string.\n",
    "    if NOME_COLUNA_DATA_PRINCIPAL in df_bq_separacao.columns:\n",
    "        logging.info(f\"Coluna de data principal '{NOME_COLUNA_DATA_PRINCIPAL}' será mantida como STRING (como lida do CSV).\")\n",
    "    else:\n",
    "        logging.warning(f\"Coluna de data principal '{NOME_COLUNA_DATA_PRINCIPAL}' não encontrada no DataFrame.\")\n",
    "\n",
    "    # Converte as colunas numéricas especificadas\n",
    "    logging.info(f\"Convertendo colunas {COLUNAS_NUMERICAS} para tipo numérico...\")\n",
    "    for col_num in COLUNAS_NUMERICAS:\n",
    "        if col_num in df_bq_separacao.columns:\n",
    "            if df_bq_separacao[col_num].dtype == 'object': # Se for string\n",
    "                 df_bq_separacao[col_num] = df_bq_separacao[col_num].str.replace(',', '.', regex=False)\n",
    "            df_bq_separacao[col_num] = pd.to_numeric(df_bq_separacao[col_num], errors='coerce')\n",
    "            num_nulos_num = df_bq_separacao[col_num].isna().sum()\n",
    "            if num_nulos_num > 0:\n",
    "                logging.warning(f\"Coluna '{col_num}': {num_nulos_num} valores não convertidos para número e serão NULOS.\")\n",
    "            logging.info(f\"Coluna '{col_num}' convertida para numérico.\")\n",
    "        else:\n",
    "            logging.warning(f\"Coluna numérica '{col_num}' não encontrada no DataFrame.\")\n",
    "\n",
    "    # Define o esquema da tabela para o BigQuery\n",
    "    table_schema = []\n",
    "    for col_name in df_bq_separacao.columns:\n",
    "        if col_name in COLUNAS_NUMERICAS:\n",
    "            # Vamos assumir INTEGER. Se precisar de decimais, use 'FLOAT' ou 'NUMERIC'.\n",
    "            table_schema.append({'name': col_name, 'type': 'INTEGER'})\n",
    "        # A coluna de data principal e todas as outras não listadas como numéricas serão STRING\n",
    "        else:\n",
    "            table_schema.append({'name': col_name, 'type': 'STRING'})\n",
    "    logging.info(f\"Schema definido para o BigQuery (coluna de data principal como STRING): {table_schema}\")\n",
    "\n",
    "    logging.info(\"Preparação do DataFrame e schema para envio ao BigQuery concluída.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"!!! ERRO FATAL durante leitura ou preparação do CSV: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    exit()\n",
    "\n",
    "# 4. Envia o DataFrame para o BigQuery\n",
    "if df_bq_separacao is not None and not df_bq_separacao.empty:\n",
    "    logging.info(f\"Enviando {len(df_bq_separacao)} linhas para BigQuery: '{PROJECT_ID}:{DESTINATION_TABLE_SEPARACAO}' (Substituindo)...\")\n",
    "    try:\n",
    "        pandas_gbq.to_gbq(\n",
    "            df_bq_separacao,\n",
    "            destination_table=DESTINATION_TABLE_SEPARACAO,\n",
    "            project_id=PROJECT_ID,\n",
    "            credentials=credentials,\n",
    "            if_exists='replace',\n",
    "            table_schema=table_schema, # Fornecendo o schema para garantir os tipos\n",
    "            progress_bar=True\n",
    "        )\n",
    "        logging.info(f\"✅ SUCESSO! Dados enviados e tabela '{DESTINATION_TABLE_SEPARACAO}' foi substituída no BigQuery.\")\n",
    "        logging.info(f\"Verifique o esquema da tabela no BigQuery: '{NOME_COLUNA_DATA_PRINCIPAL}' deve ser STRING, {COLUNAS_NUMERICAS} devem ser INTEGER.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"!!! ERRO AO ENVIAR PARA O BIGQUERY: {e}\")\n",
    "        logging.error(\"Verifique: Permissões no IAM, PROJECT_ID, DESTINATION_TABLE, API Ativa.\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    logging.error(\"DataFrame está vazio ou não foi definido. Nenhum dado enviado.\")\n",
    "\n",
    "logging.info(f\"--- FIM DO SCRIPT DE ENVIO PARA BIGQUERY ({DESTINATION_TABLE_SEPARACAO}) ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812680c8",
   "metadata": {},
   "source": [
    "# Devoluções e Cancelamentos\n",
    "Salvo em csv para subir para bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92564ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 08:35:17,115 - INFO - Iniciando conversão de 'Monitoramento de Risco (Inicio Abril_24).xlsx' para CSV...\n",
      "2025-06-05 08:35:17,117 - INFO - Limpando Nomes de Coluna e Quebras de Linha internas.\n",
      "2025-06-05 08:35:17,117 - INFO - [OK] Pasta de destino verificada/criada: C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Devoluções e Cancelamentos\n",
      "2025-06-05 08:35:17,117 - INFO - Lendo arquivo Excel: 'Monitoramento de Risco (Inicio Abril_24).xlsx'...\n",
      "c:\\Users\\Florestas\\anaconda3\\Lib\\site-packages\\openpyxl\\worksheet\\_read_only.py:85: UserWarning: Cell H163 is marked as a date but the serial value 6703797 is outside the limits for dates. The cell will be treated as an error.\n",
      "  for idx, row in parser.parse():\n",
      "c:\\Users\\Florestas\\anaconda3\\Lib\\site-packages\\openpyxl\\worksheet\\_read_only.py:85: UserWarning: Cell H1431 is marked as a date but the serial value 6183717 is outside the limits for dates. The cell will be treated as an error.\n",
      "  for idx, row in parser.parse():\n",
      "2025-06-05 08:35:17,447 - INFO - Leitura concluída: 1900 linhas.\n",
      "2025-06-05 08:35:17,447 - INFO - Realizando limpeza (espaços, NAs, quebras de linha internas)...\n",
      "2025-06-05 08:35:17,513 - INFO - Limpeza de dados concluída.\n",
      "2025-06-05 08:35:17,513 - INFO - Limpando nomes das colunas...\n",
      "2025-06-05 08:35:17,513 - INFO - Nomes das colunas limpos (ex: ['solicitantes', 'data', 'carteira', 'cod_re', 'ciclo']).\n",
      "2025-06-05 08:35:17,513 - INFO - Salvando como CSV em: 'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Devoluções e Cancelamentos\\Monitoramento de Risco (Inicio Abril_24)_limpo.csv'...\n",
      "2025-06-05 08:35:17,529 - INFO - --- FIM DO SCRIPT ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Arquivo CSV salvo com sucesso em: C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Devoluções e Cancelamentos\\Monitoramento de Risco (Inicio Abril_24)_limpo.csv\n"
     ]
    }
   ],
   "source": [
    "# Configuração básica de logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Caminhos ---\n",
    "excel_folder_path = r\"C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Excel - Pedidos\"\n",
    "excel_file_name = \"Monitoramento de Risco (Inicio Abril_24).xlsx\"\n",
    "excel_file_path = os.path.join(excel_folder_path, excel_file_name)\n",
    "\n",
    "csv_output_folder = r\"C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Devoluções e Cancelamentos\"\n",
    "csv_file_name = \"Monitoramento de Risco (Inicio Abril_24)_limpo.csv\" # Adicionei '_limpo' ao nome\n",
    "csv_file_path = os.path.join(csv_output_folder, csv_file_name)\n",
    "\n",
    "# --- Função para Limpar Nomes de Colunas ---\n",
    "def limpar_nome_coluna(col_name):\n",
    "    if not isinstance(col_name, str): col_name = str(col_name)\n",
    "    nfkd_form = unicodedata.normalize('NFKD', col_name)\n",
    "    ascii_name = nfkd_form.encode('ASCII', 'ignore').decode('ASCII')\n",
    "    lower_name = ascii_name.lower()\n",
    "    cleaned_name = re.sub(r'\\s+', '_', lower_name)\n",
    "    cleaned_name = re.sub(r'[^\\w_]+', '', cleaned_name)\n",
    "    cleaned_name = re.sub(r'_+', '_', cleaned_name)\n",
    "    cleaned_name = cleaned_name.strip('_')\n",
    "    if cleaned_name and cleaned_name[0].isdigit(): cleaned_name = '_' + cleaned_name\n",
    "    if not cleaned_name: cleaned_name = 'coluna_vazia'\n",
    "    return cleaned_name\n",
    "\n",
    "# --- Processo ---\n",
    "logging.info(f\"Iniciando conversão de '{excel_file_name}' para CSV...\")\n",
    "logging.info(\"Limpando Nomes de Coluna e Quebras de Linha internas.\")\n",
    "\n",
    "# 1. Garante que a pasta de destino exista\n",
    "try:\n",
    "    os.makedirs(csv_output_folder, exist_ok=True)\n",
    "    logging.info(f\"[OK] Pasta de destino verificada/criada: {csv_output_folder}\")\n",
    "except Exception as e:\n",
    "     logging.error(f\"⚠️ Erro ao criar pasta de destino '{csv_output_folder}': {e}\")\n",
    "\n",
    "# 2. Verifica se o arquivo de origem existe\n",
    "if not os.path.exists(excel_file_path):\n",
    "    logging.error(f\"❌ ERRO FATAL: Arquivo Excel não encontrado em: {excel_file_path}\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    # 3. Ler o arquivo Excel\n",
    "    logging.info(f\"Lendo arquivo Excel: '{excel_file_name}'...\")\n",
    "    df = pd.read_excel(excel_file_path, dtype=str)\n",
    "    logging.info(f\"Leitura concluída: {len(df)} linhas.\")\n",
    "\n",
    "    if df.empty:\n",
    "        logging.warning(\"Planilha Excel está vazia. Arquivo CSV será gerado vazio.\")\n",
    "    else:\n",
    "        # 4. Limpeza Geral e Específica\n",
    "        logging.info(\"Realizando limpeza (espaços, NAs, quebras de linha internas)...\")\n",
    "        df.dropna(how='all', inplace=True) # Remove linhas totalmente vazias\n",
    "\n",
    "        for col in df.columns:\n",
    "            # Aplica apenas a colunas que são de texto/objeto\n",
    "            if pd.api.types.is_object_dtype(df[col]):\n",
    "                logging.debug(f\"Limpando coluna: {col}\")\n",
    "                # Remove espaços inicio/fim\n",
    "                df[col] = df[col].str.strip()\n",
    "                # !!! IMPORTANTE: Substitui quebras de linha (\\n ou \\r\\n) por espaço !!!\n",
    "                df[col] = df[col].astype(str).str.replace('\\r\\n', ' ', regex=False).str.replace('\\n', ' ', regex=False)\n",
    "\n",
    "        # Substitui 'N/A' (string) por vazio '' (se necessário)\n",
    "        # df.replace('N/A', '', inplace=True)\n",
    "        # Preenche nulos restantes (NaN) com vazio ''\n",
    "        df.fillna('', inplace=True)\n",
    "        logging.info(\"Limpeza de dados concluída.\")\n",
    "\n",
    "        # 5. Limpar Nomes das Colunas (ANTES DE SALVAR)\n",
    "        logging.info(\"Limpando nomes das colunas...\")\n",
    "        df.columns = [limpar_nome_coluna(col) for col in df.columns]\n",
    "        logging.info(f\"Nomes das colunas limpos (ex: {list(df.columns[:5])}).\")\n",
    "\n",
    "    # 6. Salvar o DataFrame como CSV\n",
    "    logging.info(f\"Salvando como CSV em: '{csv_file_path}'...\")\n",
    "    df.to_csv(\n",
    "        csv_file_path,\n",
    "        index=False,\n",
    "        sep=',',            # Separador vírgula\n",
    "        encoding='utf-8',   # UTF-8 padrão\n",
    "        quoting=csv.QUOTE_ALL, # <<< MANTIDO: Coloca aspas em TODOS os campos\n",
    "        quotechar='\"',      # Aspas duplas\n",
    "        doublequote=True    # Escapa aspas internas como \"\"\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Arquivo CSV salvo com sucesso em: {csv_file_path}\") # Sua mensagem original\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Arquivo Excel não encontrado em: {excel_file_path}\") # Sua mensagem original\n",
    "except ImportError:\n",
    "    print(\"Erro: Biblioteca necessária não instalada. Tente 'pip install pandas openpyxl'\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro inesperado ao processar o arquivo: {e}\") # Sua mensagem original\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "logging.info(\"--- FIM DO SCRIPT ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b049a7f3",
   "metadata": {},
   "source": [
    "# Cortar Captação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b254cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 14:48:58,452 - INFO - -----------------------------------------------------\n",
      "2025-06-07 14:48:58,453 - INFO - Criando DataFrame interno para gerar 'CorteCaptacao.csv'.\n",
      "2025-06-07 14:48:58,453 - INFO - Formatando datas para AAAA-MM-DD e salvando como CSV.\n",
      "2025-06-07 14:48:58,454 - INFO - -----------------------------------------------------\n",
      "2025-06-07 14:48:58,456 - INFO - [OK] Pasta de destino CSV verificada/criada: 'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Excel - Pedidos\\Pedidos'\n",
      "2025-06-07 14:48:58,458 - INFO - DataFrame criado com 8 linhas.\n",
      "2025-06-07 14:48:58,459 - INFO - Formatando coluna 'mês' para AAAA-MM-DD...\n",
      "2025-06-07 14:48:58,467 - INFO - Coluna 'mês' formatada.\n",
      "2025-06-07 14:48:58,468 - INFO - Verificando/Convertendo coluna 'Qtd' para numérico...\n",
      "C:\\Users\\Florestas\\AppData\\Local\\Temp\\ipykernel_1308\\2663631126.py:97: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_processed[coluna_quantidade].fillna(0, inplace=True)\n",
      "2025-06-07 14:48:58,469 - INFO - Salvando dados processados em CSV: 'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Excel - Pedidos\\Pedidos\\CorteCaptacao.csv'...\n",
      "2025-06-07 14:48:58,473 - INFO - ✅ Arquivo CSV 'CorteCaptacao.csv' criado com sucesso em 'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Excel - Pedidos\\Pedidos'.\n",
      "2025-06-07 14:48:58,474 - INFO - --- FIM DO SCRIPT ---\n"
     ]
    }
   ],
   "source": [
    "# Script para CRIAR dados (baseado na imagem), formatar data AAAA-MM-DD\n",
    "# e salvar como CSV na pasta especificada.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "# --- Configurações ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Caminho de Destino LOCAL para o CSV ---\n",
    "caminho_pasta_destino_csv = r'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Excel - Pedidos\\Pedidos'\n",
    "# Nome do arquivo CSV de saída\n",
    "nome_arquivo_csv_destino = 'CorteCaptacao.csv' # Nome sugestivo\n",
    "caminho_completo_csv_destino = os.path.join(caminho_pasta_destino_csv, nome_arquivo_csv_destino)\n",
    "\n",
    "# --- Dados (Baseados na imagem que você forneceu) ---\n",
    "dados = {\n",
    "    'mês': [ # Usando o nome original da coluna da imagem\n",
    "        '01/01/2025',\n",
    "        '01/02/2025',\n",
    "        '01/03/2025',\n",
    "        '01/04/2025',\n",
    "        '01/10/2024',\n",
    "        '01/11/2024',\n",
    "        '01/12/2024',\n",
    "        '01/05/2025',\n",
    "\n",
    "    ],\n",
    "    'Qtd': [ # Usando o nome original da coluna da imagem\n",
    "        249,\n",
    "        122,\n",
    "        148,\n",
    "        269,\n",
    "        220,\n",
    "        210,\n",
    "        281,\n",
    "        104\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Nome exato da coluna que contém as datas a serem formatadas\n",
    "coluna_data_para_formatar = 'mês'\n",
    "\n",
    "# --- Execução Principal ---\n",
    "logging.info(\"-----------------------------------------------------\")\n",
    "logging.info(f\"Criando DataFrame interno para gerar '{nome_arquivo_csv_destino}'.\")\n",
    "logging.info(\"Formatando datas para AAAA-MM-DD e salvando como CSV.\")\n",
    "logging.info(\"-----------------------------------------------------\")\n",
    "\n",
    "# 1. Garante que a pasta de destino do CSV exista\n",
    "try:\n",
    "    # Cria a pasta (e subpastas) se não existirem\n",
    "    os.makedirs(caminho_pasta_destino_csv, exist_ok=True)\n",
    "    logging.info(f\"[OK] Pasta de destino CSV verificada/criada: '{caminho_pasta_destino_csv}'\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"!!! ERRO FATAL: Falha ao verificar/criar pasta de destino: {e}\")\n",
    "    exit() # Interrompe se não conseguir criar a pasta\n",
    "\n",
    "# 2. Cria o DataFrame a partir dos dados definidos\n",
    "try:\n",
    "    df_processed = pd.DataFrame(dados)\n",
    "    logging.info(f\"DataFrame criado com {len(df_processed)} linhas.\")\n",
    "\n",
    "    # 3. Formata a Coluna de Data ('mês') para AAAA-MM-DD\n",
    "    if coluna_data_para_formatar in df_processed.columns:\n",
    "        logging.info(f\"Formatando coluna '{coluna_data_para_formatar}' para AAAA-MM-DD...\")\n",
    "        try:\n",
    "            # Converte a coluna para datetime, especificando o formato de origem DD/MM/YYYY\n",
    "            # errors='coerce' transforma erros de conversão em NaT (Not a Time)\n",
    "            datas_convertidas = pd.to_datetime(df_processed[coluna_data_para_formatar], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "            # Formata as datas convertidas com sucesso para AAAA-MM-DD como string\n",
    "            df_processed[coluna_data_para_formatar] = datas_convertidas.dt.strftime('%Y-%m-%d')\n",
    "\n",
    "            # Verifica se houve erros na conversão original\n",
    "            erros_conversao = datas_convertidas.isna().sum()\n",
    "            if erros_conversao > 0:\n",
    "                logging.warning(f\"Coluna '{coluna_data_para_formatar}': {erros_conversao} valores não eram datas válidas no formato DD/MM/YYYY e ficarão vazios.\")\n",
    "                # Preenche os erros (que viraram NaN no strftime) com vazio ''\n",
    "                df_processed[coluna_data_para_formatar].fillna('', inplace=True)\n",
    "\n",
    "            logging.info(f\"Coluna '{coluna_data_para_formatar}' formatada.\")\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Erro ao formatar coluna de data '{coluna_data_para_formatar}': {e}\")\n",
    "             df_processed[coluna_data_para_formatar] = '' # Define como vazio em caso de erro geral\n",
    "    else:\n",
    "        logging.warning(f\"Coluna de data '{coluna_data_para_formatar}' não encontrada no DataFrame criado.\")\n",
    "\n",
    "    # 4. Opcional: Garante que a coluna 'Qtd' seja numérica (se necessário)\n",
    "    coluna_quantidade = 'Qtd'\n",
    "    if coluna_quantidade in df_processed.columns:\n",
    "        logging.info(f\"Verificando/Convertendo coluna '{coluna_quantidade}' para numérico...\")\n",
    "        df_processed[coluna_quantidade] = pd.to_numeric(df_processed[coluna_quantidade], errors='coerce')\n",
    "        # Preenche erros de conversão numérica com 0 (ou pode usar pd.NA)\n",
    "        df_processed[coluna_quantidade].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "    # 5. Salva como CSV\n",
    "    logging.info(f\"Salvando dados processados em CSV: '{caminho_completo_csv_destino}'...\")\n",
    "    # Garante que outros nulos (se houver) virem strings vazias\n",
    "    df_processed.fillna('', inplace=True)\n",
    "    df_processed.to_csv(caminho_completo_csv_destino,\n",
    "                        sep=',',                 # Separador vírgula\n",
    "                        encoding='utf-8-sig',    # Encoding bom para Excel ler acentos\n",
    "                        index=False,             # Não salva índice do pandas\n",
    "                        quotechar='\"',\n",
    "                        quoting=csv.QUOTE_MINIMAL) # Aspas só quando necessário\n",
    "    logging.info(f\"✅ Arquivo CSV '{nome_arquivo_csv_destino}' criado com sucesso em '{caminho_pasta_destino_csv}'.\")\n",
    "\n",
    "\n",
    "except ValueError as ve:\n",
    "     logging.error(f\"!!! ERRO ao criar DataFrame: {ve}.\")\n",
    "except PermissionError:\n",
    "    logging.error(f\"!!! ERRO DE PERMISSÃO ao salvar CSV em '{caminho_completo_csv_destino}'. Verifique se o arquivo está aberto ou se há permissão de escrita na pasta.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"!!! ERRO GERAL INESPERADO: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "logging.info(\"--- FIM DO SCRIPT ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea259166",
   "metadata": {},
   "source": [
    "# Divergancia na Separação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da7e841f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 13:04:44,562 - INFO - -----------------------------------------------------\n",
      "2025-05-28 13:04:44,562 - INFO - Processando ABA 'Divergencia por Colaborador' de 'Faturado Não Enviado (HUB).xlsx'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 13:04:44,562 - INFO - Para gerar o arquivo CSV 'Faturado_Nao_Enviado_Divergencia.csv'.\n",
      "2025-05-28 13:04:44,562 - INFO - -----------------------------------------------------\n",
      "2025-05-28 13:04:44,570 - INFO - [OK] Pasta de destino CSV verificada/criada: 'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Excel - Pedidos\\Pedidos'\n",
      "2025-05-28 13:04:44,571 - INFO - Lendo ABA 'Divergencia por Colaborador' do arquivo Excel: Faturado Não Enviado (HUB).xlsx...\n",
      "2025-05-28 13:04:44,872 - INFO - Leitura da aba OK: 219 linhas.\n",
      "2025-05-28 13:04:44,872 - INFO - Limpando espaços extras...\n",
      "2025-05-28 13:04:44,879 - INFO - Limpando nomes das colunas...\n",
      "2025-05-28 13:04:44,881 - INFO - Nomes das colunas limpos (ex: ['pedido', 'loja_pedido', 'nome']...).\n",
      "2025-05-28 13:04:44,881 - INFO - Salvando dados processados em CSV: 'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Excel - Pedidos\\Pedidos\\Faturado_Nao_Enviado_Divergencia.csv'...\n",
      "2025-05-28 13:04:44,881 - INFO - ✅ Arquivo CSV 'Faturado_Nao_Enviado_Divergencia.csv' criado/atualizado com sucesso em 'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Excel - Pedidos\\Pedidos'.\n",
      "2025-05-28 13:04:44,887 - INFO - --- FIM DO SCRIPT ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import csv\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# --- Configurações ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Caminhos e Nomes de Arquivo ---\n",
    "# Pasta onde está o Excel de origem\n",
    "caminho_pasta_origem_excel = r'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Excel - Pedidos'\n",
    "# !!! NOVO ARQUIVO DE ORIGEM !!!\n",
    "nome_arquivo_excel_origem = 'Faturado Não Enviado (HUB).xlsx'\n",
    "caminho_completo_excel_origem = os.path.join(caminho_pasta_origem_excel, nome_arquivo_excel_origem)\n",
    "\n",
    "# !!! NOME EXATO DA ABA/PLANILHA DENTRO DO ARQUIVO EXCEL !!!\n",
    "nome_da_aba_excel = 'Divergencia por Colaborador'\n",
    "\n",
    "# ====> DEFINA O DESTINO E NOME DO ARQUIVO FINAL <====\n",
    "# Onde salvar o CSV? Qual nome dar a ele?\n",
    "# Substitua os valores abaixo pelos corretos para este arquivo.\n",
    "\n",
    "# Exemplo 1: Pasta Local 'Pedidos'\n",
    "caminho_pasta_destino_csv = r'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Excel - Pedidos\\Pedidos'\n",
    "nome_arquivo_csv_destino = 'Faturado_Nao_Enviado_Divergencia.csv' # Nome sugerido\n",
    "\n",
    "# Exemplo 2: Pasta Google Drive 'Custos Transportadora'\n",
    "# caminho_pasta_destino_csv = r'G:\\Meu Drive\\Relatorio Oficial 2025\\Drive\\Custos Transportadora'\n",
    "# nome_arquivo_csv_destino = 'Faturado_Nao_Enviado_Divergencia.csv' # Nome sugerido\n",
    "\n",
    "# Verifique se o caminho e nome abaixo estão corretos para este arquivo!\n",
    "caminho_completo_csv_destino = os.path.join(caminho_pasta_destino_csv, nome_arquivo_csv_destino)\n",
    "# ======================================================\n",
    "\n",
    "# --- Funções Auxiliares ---\n",
    "def limpar_nome_coluna(col_name):\n",
    "    # (Mesma função para limpar nomes de colunas)\n",
    "    if not isinstance(col_name, str): col_name = str(col_name)\n",
    "    nfkd_form = unicodedata.normalize('NFKD', col_name)\n",
    "    ascii_name = nfkd_form.encode('ASCII', 'ignore').decode('ASCII')\n",
    "    lower_name = ascii_name.lower()\n",
    "    cleaned_name = re.sub(r'\\s+', '_', lower_name)\n",
    "    cleaned_name = re.sub(r'[^\\w_]+', '', cleaned_name)\n",
    "    cleaned_name = re.sub(r'_+', '_', cleaned_name)\n",
    "    cleaned_name = cleaned_name.strip('_')\n",
    "    if cleaned_name and cleaned_name[0].isdigit(): cleaned_name = '_' + cleaned_name\n",
    "    if not cleaned_name: cleaned_name = 'coluna_vazia'\n",
    "    return cleaned_name\n",
    "\n",
    "# --- Execução Principal ---\n",
    "logging.info(\"-----------------------------------------------------\")\n",
    "logging.info(f\"Processando ABA '{nome_da_aba_excel}' de '{nome_arquivo_excel_origem}'\")\n",
    "logging.info(f\"Para gerar o arquivo CSV '{nome_arquivo_csv_destino}'.\")\n",
    "logging.info(\"-----------------------------------------------------\")\n",
    "\n",
    "# 1. Garante que a pasta de destino do CSV exista\n",
    "try:\n",
    "    os.makedirs(caminho_pasta_destino_csv, exist_ok=True)\n",
    "    logging.info(f\"[OK] Pasta de destino CSV verificada/criada: '{caminho_pasta_destino_csv}'\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"!!! ERRO FATAL: Falha ao verificar/criar pasta de destino: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Verifica se o arquivo Excel de origem existe\n",
    "if not os.path.exists(caminho_completo_excel_origem):\n",
    "    logging.error(f\"!!! ERRO FATAL: Arquivo Excel de ORIGEM não encontrado: '{caminho_completo_excel_origem}'.\")\n",
    "    exit()\n",
    "\n",
    "# 3. Lê a ABA ESPECÍFICA do arquivo Excel\n",
    "df_processed = None\n",
    "try:\n",
    "    logging.info(f\"Lendo ABA '{nome_da_aba_excel}' do arquivo Excel: {nome_arquivo_excel_origem}...\")\n",
    "    # Usa sheet_name para especificar qual aba ler\n",
    "    df_processed = pd.read_excel(caminho_completo_excel_origem, sheet_name=nome_da_aba_excel, dtype=str)\n",
    "    if df_processed is None or df_processed.empty: raise ValueError(f\"Aba '{nome_da_aba_excel}' está vazia ou falha na leitura.\")\n",
    "    logging.info(f\"Leitura da aba OK: {len(df_processed)} linhas.\")\n",
    "\n",
    "    # --- !!! ADICIONE AQUI OS PASSOS DE PROCESSAMENTO NECESSÁRIOS !!! ---\n",
    "    #      (Limpar N/A? Formatar Datas? Remover Duplicatas? Selecionar Colunas?)\n",
    "\n",
    "    # Exemplo: Limpeza básica de espaços (geralmente útil)\n",
    "    logging.info(\"Limpando espaços extras...\")\n",
    "    for col in df_processed.columns:\n",
    "        if pd.api.types.is_object_dtype(df_processed[col]):\n",
    "            df_processed[col] = df_processed[col].str.strip()\n",
    "\n",
    "    # Exemplo: Limpar Nomes das Colunas (geralmente útil)\n",
    "    logging.info(\"Limpando nomes das colunas...\")\n",
    "    df_processed.columns = [limpar_nome_coluna(col) for col in df_processed.columns]\n",
    "    logging.info(f\"Nomes das colunas limpos (ex: {list(df_processed.columns[:5])}...).\")\n",
    "    # -------------------------------------------------------------------\n",
    "\n",
    "    # 4. Salva como CSV (Substituindo o CSV anterior no destino)\n",
    "    if not df_processed.empty:\n",
    "        logging.info(f\"Salvando dados processados em CSV: '{caminho_completo_csv_destino}'...\")\n",
    "        df_processed.fillna('', inplace=True) # Garante que nulos virem strings vazias\n",
    "        df_processed.to_csv(caminho_completo_csv_destino, sep=',', encoding='utf-8-sig', index=False, quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        logging.info(f\"✅ Arquivo CSV '{nome_arquivo_csv_destino}' criado/atualizado com sucesso em '{caminho_pasta_destino_csv}'.\")\n",
    "    else:\n",
    "         logging.warning(\"DataFrame ficou vazio após leitura/processamento. Nenhum CSV salvo.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"!!! ERRO: Arquivo Excel não encontrado em '{caminho_completo_excel_origem}'.\")\n",
    "except ValueError as ve:\n",
    "     # Erro pode acontecer se a ABA não existir\n",
    "     logging.error(f\"!!! ERRO: {ve}. Verifique se a aba '{nome_da_aba_excel}' existe no arquivo Excel.\")\n",
    "except PermissionError:\n",
    "    logging.error(f\"!!! ERRO DE PERMISSÃO ao salvar CSV em '{caminho_completo_csv_destino}'.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"!!! ERRO GERAL INESPERADO: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "logging.info(\"--- FIM DO SCRIPT ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388b387c",
   "metadata": {},
   "source": [
    "## Sac - faturado e n enviado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96fef85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 13:04:44,918 - INFO - --- INICIANDO PROCESSAMENTO PARA SALVAR COMO .XLSX ---\n",
      "2025-05-28 13:04:44,920 - INFO - Pasta de destino configurada para: C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Excel - Pedidos\\Pedidos\n",
      "2025-05-28 13:04:44,920 - INFO - Procurando arquivos 'Atendimen*.xls' em 'C:\\Users\\Florestas\\Downloads'...\n",
      "2025-05-28 13:04:44,930 - INFO - Arquivo Excel de origem: 'C:\\Users\\Florestas\\Downloads\\Atendimento_2d01d30b-f93e-4441-968d-2c4e8373e53c.xls'\n",
      "2025-05-28 13:04:44,942 - INFO - Lidas 100 linhas e 18 colunas do Excel com engine='xlrd'.\n",
      "2025-05-28 13:04:44,942 - INFO - Nomes de colunas após renomeação e limpeza final: ['cod_atendimento', 'cod_pedido', 'cod_externo_pedido', 'data_abertura', 'cod_pessoa', 'nome_pessoa', 'origem', 'tipo', 'situacao', 'regra_de_notificacao', 'nivel_atual', 'nota_fiscal', 'data_situacao', 'cod_usuario', 'nome_usuario', 'cod_usuario_criacao', 'nome_usuario_criacao', 'outras_informacoes']\n",
      "2025-05-28 13:04:44,942 - INFO - Tentando salvar arquivo Excel em: 'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Excel - Pedidos\\Pedidos\\sac_faturado_nao_enviado_processado.xlsx'\n",
      "2025-05-28 13:04:45,000 - INFO - ✅ Arquivo Excel (.xlsx) salvo com sucesso em: 'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Excel - Pedidos\\Pedidos\\sac_faturado_nao_enviado_processado.xlsx'\n",
      "2025-05-28 13:04:45,002 - INFO - --- PROCESSAMENTO CONCLUÍDO ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import unicodedata\n",
    "import logging\n",
    "\n",
    "# --- Configurações Iniciais ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Constantes e Configurações ---\n",
    "PASTA_DOWNLOADS = os.path.join(os.path.expanduser('~'), 'Downloads')\n",
    "\n",
    "# --- DESTINO CORRIGIDO PARA A PASTA 'Pedidos' ---\n",
    "PASTA_DESTINO_EXCEL = r'C:\\Users\\Florestas\\Desktop\\Relatorio Oficial 2025\\Excel - Pedidos\\Pedidos' # << CORRIGIDO AQUI\n",
    "# ------------------------------------------------\n",
    "\n",
    "# Mantendo o nome do arquivo de destino como .xlsx\n",
    "NOME_ARQUIVO_EXCEL_DESTINO = 'sac_faturado_nao_enviado_processado.xlsx'\n",
    "CAMINHO_COMPLETO_EXCEL_DESTINO = os.path.join(PASTA_DESTINO_EXCEL, NOME_ARQUIVO_EXCEL_DESTINO)\n",
    "\n",
    "# Nomes das colunas de data no Excel ORIGINAL e como queremos que fiquem no arquivo final\n",
    "MAPEAMENTO_COLUNAS_DATAS = {\n",
    "    'Data/Hora criação': 'data_abertura',\n",
    "    'Data/Hora finalização': 'data_fechamento'\n",
    "}\n",
    "\n",
    "# Outras colunas para renomear e limpar\n",
    "MAPEAMENTO_OUTRAS_COLUNAS = {\n",
    "    'Situação': 'situacao', 'Cód Atendimento': 'cod_atendimento', 'Nome Pessoa': 'nome_pessoa',\n",
    "    'Cód Pedido': 'cod_pedido', 'Cód Externo Pedido': 'cod_externo_pedido', 'cod_pessoa': 'cod_pessoa',\n",
    "    'CPF': 'cpf', 'Sexo': 'sexo', 'Idade': 'idade', 'RuaResidencial': 'rua_residencial',\n",
    "    'ComplementoResidencial': 'complemento_residencial', 'BairroResidencial': 'bairro_residencial',\n",
    "    'CidadeResidencial': 'cidade_residencial', 'EstadoResidencial': 'estado_residencial',\n",
    "    'CEPResidencial': 'cep_residencial', 'E-mail principal': 'email_principal',\n",
    "    'Cód Atendente Criação': 'cod_atendente_criacao', 'Nome do Atendente Criação': 'nome_atendente_criacao',\n",
    "    'Cód Atendente Finalização': 'cod_atendente_finalizacao', 'Nome do atendente Finalização': 'nome_atendente_finalizacao',\n",
    "    'Origem': 'origem', 'Tipo': 'tipo', 'Cód Produto': 'cod_produto', 'Nome do Produto': 'nome_produto',\n",
    "    'Número do lote': 'numero_lote', 'Cód Transportadora': 'cod_transportadora',\n",
    "    'Nome Transportadora': 'nome_transportadora', 'Pergunta': 'pergunta', 'Resposta': 'resposta'\n",
    "}\n",
    "\n",
    "def limpar_nome_coluna(nome_coluna):\n",
    "    \"\"\"Limpa um nome de coluna: minúsculas, sem acentos, espaços/especiais para underscore.\"\"\"\n",
    "    if not isinstance(nome_coluna, str): nome_coluna = str(nome_coluna)\n",
    "    nfkd_form = unicodedata.normalize('NFKD', nome_coluna)\n",
    "    nome_sem_acentos = \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "    nome_minusculo = nome_sem_acentos.lower()\n",
    "    nome_limpo = re.sub(r'[^\\w_]+', '_', nome_minusculo)\n",
    "    nome_limpo = re.sub(r'_+', '_', nome_limpo)\n",
    "    nome_limpo = nome_limpo.strip('_')\n",
    "    if nome_limpo and nome_limpo[0].isdigit(): nome_limpo = '_' + nome_limpo\n",
    "    return nome_limpo if nome_limpo else 'coluna_indefinida'\n",
    "\n",
    "def processar_arquivo_excel_para_xlsx():\n",
    "    \"\"\"\n",
    "    Encontra o arquivo Excel (.xls) mais recente, lê, processa e salva como um novo arquivo .xlsx.\n",
    "    \"\"\"\n",
    "    logging.info(\"--- INICIANDO PROCESSAMENTO PARA SALVAR COMO .XLSX ---\")\n",
    "    logging.info(f\"Pasta de destino configurada para: {PASTA_DESTINO_EXCEL}\") # Log mostrará o caminho correto\n",
    "\n",
    "    # 1. Encontrar o arquivo Excel de origem\n",
    "    logging.info(f\"Procurando arquivos 'Atendimen*.xls' em '{PASTA_DOWNLOADS}'...\")\n",
    "    arquivos_encontrados = glob.glob(os.path.join(PASTA_DOWNLOADS, \"Atendimen*.xls\"))\n",
    "    if not arquivos_encontrados:\n",
    "        logging.error(\"Nenhum arquivo Excel 'Atendimen*.xls' encontrado.\")\n",
    "        return\n",
    "    arquivo_excel_origem = max(arquivos_encontrados, key=os.path.getmtime)\n",
    "    logging.info(f\"Arquivo Excel de origem: '{arquivo_excel_origem}'\")\n",
    "\n",
    "    # 2. Ler o arquivo Excel de origem\n",
    "    df = None\n",
    "    try:\n",
    "        df = pd.read_excel(arquivo_excel_origem, sheet_name=0, dtype=str, engine='xlrd')\n",
    "        logging.info(f\"Lidas {len(df)} linhas e {len(df.columns)} colunas do Excel com engine='xlrd'.\")\n",
    "    except Exception as e_xlrd:\n",
    "        logging.warning(f\"Falha ao ler com xlrd: {e_xlrd}\")\n",
    "        if \"Excel xlsx file; not supported\" in str(e_xlrd) or \"Unsupported format, or corrupt file\" in str(e_xlrd) :\n",
    "            logging.info(\"Tentando ler como .xlsx (engine=openpyxl)...\")\n",
    "            try:\n",
    "                df = pd.read_excel(arquivo_excel_origem, sheet_name=0, dtype=str, engine='openpyxl')\n",
    "                logging.info(f\"Lidas {len(df)} linhas e {len(df.columns)} colunas com engine='openpyxl'.\")\n",
    "            except Exception as e_openpyxl: logging.error(f\"Erro ao ler como .xlsx também: {e_openpyxl}\"); return\n",
    "        else: logging.error(f\"Erro não relacionado a formato ao ler com xlrd: {e_xlrd}\"); return\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        logging.warning(\"O DataFrame está vazio após as tentativas de leitura.\"); return\n",
    "\n",
    "    # 3. Renomear e Limpar Colunas\n",
    "    novos_nomes_colunas = {}\n",
    "    colunas_datas_processadas = []\n",
    "    for nome_col_original_excel in df.columns:\n",
    "        nome_original_strip = str(nome_col_original_excel).strip()\n",
    "        novo_nome_final = None\n",
    "        if nome_original_strip in MAPEAMENTO_COLUNAS_DATAS:\n",
    "            novo_nome_final = MAPEAMENTO_COLUNAS_DATAS[nome_original_strip]\n",
    "            colunas_datas_processadas.append(novo_nome_final)\n",
    "        elif nome_original_strip in MAPEAMENTO_OUTRAS_COLUNAS:\n",
    "            novo_nome_final = MAPEAMENTO_OUTRAS_COLUNAS[nome_original_strip]\n",
    "        else: novo_nome_final = limpar_nome_coluna(nome_original_strip)\n",
    "        novos_nomes_colunas[nome_col_original_excel] = novo_nome_final\n",
    "    df.rename(columns=novos_nomes_colunas, inplace=True)\n",
    "    df.columns = [limpar_nome_coluna(col) for col in df.columns]\n",
    "    colunas_datas_processadas = [limpar_nome_coluna(col) for col in colunas_datas_processadas]\n",
    "    logging.info(f\"Nomes de colunas após renomeação e limpeza final: {list(df.columns)}\")\n",
    "\n",
    "    # 4. Converter Colunas de Data\n",
    "    for nome_col_data_limpo in colunas_datas_processadas:\n",
    "        if nome_col_data_limpo in df.columns:\n",
    "            logging.info(f\"Processando coluna de data: '{nome_col_data_limpo}'\")\n",
    "            try:\n",
    "                df[nome_col_data_limpo] = pd.to_datetime(df[nome_col_data_limpo], errors='coerce')\n",
    "                if not df[nome_col_data_limpo].isna().all():\n",
    "                     logging.info(f\"Formatando '{nome_col_data_limpo}' para YYYY-MM-DD.\")\n",
    "                     df[nome_col_data_limpo] = df[nome_col_data_limpo].dt.strftime('%Y-%m-%d')\n",
    "                     df[nome_col_data_limpo].replace('NaT', '', inplace=True)\n",
    "                else:\n",
    "                    logging.warning(f\"Conversão para datetime falhou para a coluna '{nome_col_data_limpo}'. Preenchendo com vazio.\")\n",
    "                    df[nome_col_data_limpo] = ''\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Erro ao converter data da coluna '{nome_col_data_limpo}': {e}\")\n",
    "                df[nome_col_data_limpo] = ''\n",
    "        else:\n",
    "            logging.warning(f\"Coluna de data '{nome_col_data_limpo}' não encontrada para conversão.\")\n",
    "\n",
    "    # 5. Garantir que a pasta de destino exista e Salvar como .XLSX\n",
    "    try:\n",
    "        os.makedirs(PASTA_DESTINO_EXCEL, exist_ok=True) # Usa o caminho corrigido\n",
    "        logging.info(f\"Tentando salvar arquivo Excel em: '{CAMINHO_COMPLETO_EXCEL_DESTINO}'\") # Usa o caminho corrigido\n",
    "        df.fillna('', inplace=True)\n",
    "        df.to_excel(CAMINHO_COMPLETO_EXCEL_DESTINO, index=False, engine='openpyxl') # Usa o caminho corrigido\n",
    "        logging.info(f\"✅ Arquivo Excel (.xlsx) salvo com sucesso em: '{CAMINHO_COMPLETO_EXCEL_DESTINO}'\") # Usa o caminho corrigido\n",
    "\n",
    "    except PermissionError:\n",
    "        logging.error(f\"ERRO DE PERMISSÃO ao salvar em '{CAMINHO_COMPLETO_EXCEL_DESTINO}'.\")\n",
    "        logging.error(\"Verifique se o arquivo está aberto ou se você tem permissão para escrever nesta pasta.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro desconhecido ao criar pasta de destino ou salvar o arquivo Excel: {e}\")\n",
    "\n",
    "    logging.info(\"--- PROCESSAMENTO CONCLUÍDO ---\")\n",
    "\n",
    "# --- Executar o Processamento ---\n",
    "if __name__ == \"__main__\":\n",
    "    processar_arquivo_excel_para_xlsx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1506647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffde757a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692aa258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
